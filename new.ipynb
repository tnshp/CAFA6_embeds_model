{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b0488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe014139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_with_random_terms(terms, max_terms, all_terms_array, existing_terms_set=None):\n",
    "    \"\"\"Optimized padding with random GO terms using rejection sampling.\"\"\"\n",
    "    if len(terms) >= max_terms:\n",
    "        return terms[:max_terms]\n",
    "    \n",
    "    # Calculate how many more terms we need\n",
    "    num_needed = max_terms - len(terms)\n",
    "    \n",
    "    # Use pre-computed set if available, otherwise create it\n",
    "    if existing_terms_set is None:\n",
    "        existing_terms_set = set(terms)\n",
    "    \n",
    "    # For small num_needed relative to available terms, use rejection sampling\n",
    "    # This avoids creating the full boolean mask\n",
    "    if num_needed < len(all_terms_array) * 0.1:  # If we need < 10% of total terms\n",
    "        random_terms = []\n",
    "        max_attempts = num_needed * 10  # Prevent infinite loop\n",
    "        attempts = 0\n",
    "        \n",
    "        while len(random_terms) < num_needed and attempts < max_attempts:\n",
    "            # Sample with replacement first (fast)\n",
    "            batch_size = min(num_needed * 2, 1000)  # Sample in batches\n",
    "            candidates = np.random.choice(all_terms_array, size=batch_size, replace=True)\n",
    "            \n",
    "            # Filter out existing terms\n",
    "            for candidate in candidates:\n",
    "                if candidate not in existing_terms_set:\n",
    "                    random_terms.append(candidate)\n",
    "                    existing_terms_set.add(candidate)\n",
    "                    if len(random_terms) >= num_needed:\n",
    "                        break\n",
    "            \n",
    "            attempts += batch_size\n",
    "        \n",
    "        # If rejection sampling didn't get enough, fall back to full filtering\n",
    "        if len(random_terms) < num_needed:\n",
    "            available_mask = np.isin(all_terms_array, list(existing_terms_set), invert=True)\n",
    "            available_terms = all_terms_array[available_mask]\n",
    "            remaining_needed = num_needed - len(random_terms)\n",
    "            \n",
    "            if len(available_terms) >= remaining_needed:\n",
    "                additional = np.random.choice(available_terms, size=remaining_needed, replace=False).tolist()\n",
    "            else:\n",
    "                additional = np.random.choice(available_terms, size=remaining_needed, replace=True).tolist()\n",
    "            \n",
    "            random_terms.extend(additional)\n",
    "    else:\n",
    "        # For large num_needed, use vectorized filtering (more efficient for bulk)\n",
    "        available_mask = np.isin(all_terms_array, list(existing_terms_set), invert=True)\n",
    "        available_terms = all_terms_array[available_mask]\n",
    "        \n",
    "        # Randomly sample\n",
    "        if len(available_terms) >= num_needed:\n",
    "            random_terms = np.random.choice(available_terms, size=num_needed, replace=False).tolist()\n",
    "        else:\n",
    "            # If not enough unique terms, sample with replacement\n",
    "            random_terms = np.random.choice(available_terms, size=num_needed, replace=True).tolist()\n",
    "    \n",
    "    return terms + random_terms[:num_needed]\n",
    "\n",
    "def load_data(data_paths, max_terms=256, aspect=None):\n",
    "    \n",
    "    seq_2_terms_df = data_paths['seq_2_terms_df']\n",
    "    train_terms_df = data_paths['train_terms_df']\n",
    "    plm_features_path = data_paths['plm_features_path']\n",
    "    go_embeds_paths = data_paths['go_embeds_paths']\n",
    "\n",
    "    seq_2_terms = pd.read_parquet(seq_2_terms_df, engine='fastparquet')\n",
    "    train_terms = pd.read_csv(train_terms_df, sep='\\t')\n",
    "\n",
    "    term_to_aspect = train_terms.groupby('term')['aspect'].first().to_dict()\n",
    "        \n",
    "    with open(go_embeds_paths, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        embeddings_dict = data['embeddings']\n",
    "        go_ids = data['go_ids']\n",
    "\n",
    "    # Filter to keep only terms from a specific aspect if aspect is provided\n",
    "    print('filtering by aspect:', aspect)\n",
    "    if aspect is not None:\n",
    "        seq_2_terms['terms_predicted'] = seq_2_terms['terms_predicted'].apply(\n",
    "            lambda terms: [t for t in terms if term_to_aspect.get(t) == aspect]\n",
    "        )\n",
    "        seq_2_terms['terms_true'] = seq_2_terms['terms_true'].apply(\n",
    "            lambda terms: [t for t in terms if term_to_aspect.get(t) == aspect]\n",
    "        )\n",
    "        # Remove rows where terms_predicted or terms_true is now empty\n",
    "        seq_2_terms = seq_2_terms[seq_2_terms['terms_predicted'].apply(len) > 0]\n",
    "        seq_2_terms = seq_2_terms[seq_2_terms['terms_true'].apply(len) > 0]\n",
    "\n",
    "        # Pad terms with random terms from the same aspect\n",
    "        print(f\"Padding terms_predicted with random terms from aspect {aspect}...\")\n",
    "        # Get all terms from this aspect that have embeddings\n",
    "        aspect_terms = [term for term, asp in term_to_aspect.items() if asp == aspect and term in embeddings_dict]\n",
    "        all_aspect_terms = np.array(aspect_terms)\n",
    "        \n",
    "        tqdm.pandas(desc=f\"Padding with random {aspect} terms\")\n",
    "        seq_2_terms['terms_predicted'] = seq_2_terms['terms_predicted'].progress_apply(\n",
    "            lambda terms: pad_with_random_terms(terms, max_terms, all_aspect_terms)\n",
    "        )\n",
    "        \n",
    "        # Verify padding\n",
    "        term_lengths_after = seq_2_terms['terms_predicted'].apply(len)\n",
    "        print(f\"After padding - Min: {term_lengths_after.min()}, Max: {term_lengths_after.max()}, Mean: {term_lengths_after.mean():.2f}\")\n",
    "\n",
    "    \n",
    "\n",
    "    plm_embeds_dict = np.load(plm_features_path, allow_pickle=True).item()\n",
    "\n",
    "    # term_lengths = seq_2_terms['terms_predicted'].apply(len)\n",
    "\n",
    "    print(\"filtering sequences by term lengths\")\n",
    "   #currently only using sequences with 256 terms, need to change later \n",
    "    # seq_2_terms = seq_2_terms[term_lengths == max_terms]\n",
    "\n",
    "    features_ids = list(plm_embeds_dict.keys())\n",
    "    train_ids =  pd.DataFrame(features_ids, columns=['qseqid'])\n",
    "    seq_2_terms = seq_2_terms.merge(train_ids, on='qseqid', how='inner')    \n",
    "\n",
    "    prot_2_pmid_path = data_paths['prot_2_pmid_path']\n",
    "    pmid_2_embed_path = data_paths['pmid_2_embed_path']\n",
    "    prot_2_pmid = np.load(prot_2_pmid_path, allow_pickle=True).item()\n",
    "    pmid_2_embed = np.load(pmid_2_embed_path, allow_pickle=True).item()\n",
    "\n",
    "    out = {'seq_2_terms': seq_2_terms,\n",
    "           'plm_embeds': plm_embeds_dict,\n",
    "           'prot_2_pmid': prot_2_pmid,\n",
    "           'pmid_2_embed': pmid_2_embed,\n",
    "           'go_embeds': embeddings_dict,\n",
    "           }\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9666876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering by aspect: F\n",
      "Padding terms_predicted with random terms from aspect F...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5963a332ba14b95a659d9c61059d91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Padding with random F terms:   0%|          | 0/57960 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After padding - Min: 64, Max: 64, Mean: 64.00\n",
      "filtering sequences by term lengths\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(57960, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_paths = {\n",
    "    'seq_2_terms_df':           '/mnt/d/ML/Kaggle/CAFA6-new/data_packet1/seq_2_terms.parquet',\n",
    "    'train_terms_df':           '/mnt/d/ML/Kaggle/CAFA6/cafa-6-protein-function-prediction/Train/train_terms.tsv',\n",
    "    \"plm_features_path\":        '/mnt/d/ML/Kaggle/CAFA6-new/data_packet1/plm_features.npy',\n",
    "    'prot_2_pmid_path':         '/mnt/d/ML/Kaggle/CAFA6-new/data_packet1/prot_2_pmid.npy',\n",
    "    \"pmid_2_embed_path\":        '/mnt/d/ML/Kaggle/CAFA6-new/data_packet1/pmid_2_embed.npy',\n",
    "    'go_embeds_paths':          '/mnt/d/ML/Kaggle/CAFA6-new/data_packet1/go_embeddings.pkl'\n",
    "}   \n",
    "\n",
    "data = load_data(data_paths, max_terms=64, aspect='F')\n",
    "data['seq_2_terms'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7476d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min terms: 64\n",
      "Max terms: 64\n",
      "Mean terms: 64.00\n",
      "Median terms: 64.00\n",
      "No. empty predicted terms: 0\n",
      "\n",
      "Sample row with padded terms:\n",
      "Number of predicted terms: 64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of terms_predicted lengths after padding\n",
    "term_lengths = data['seq_2_terms']['terms_predicted'].apply(len)\n",
    "print(f\"Min terms: {term_lengths.min()}\")\n",
    "print(f\"Max terms: {term_lengths.max()}\")\n",
    "print(f\"Mean terms: {term_lengths.mean():.2f}\")\n",
    "print(f\"Median terms: {term_lengths.median():.2f}\")\n",
    "print(f\"No. empty predicted terms: {(term_lengths == 0).sum()}\")\n",
    "print(f\"\\nSample row with padded terms:\")\n",
    "print(f\"Number of predicted terms: {len(data['seq_2_terms'].iloc[0]['terms_predicted'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf35af7",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b610e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsDataset(Dataset):\n",
    "    \"\"\"Dataset that yields raw embeddings; tokenization is done in collate_fn for batching.\"\"\"\n",
    "    def __init__(self, \n",
    "                 data, \n",
    "                 max_go_embeds = 256,  \n",
    "                 oversample_indices=None\n",
    "                ):\n",
    "        \n",
    "        self.data = data\n",
    "        self.max_go_embeds = max_go_embeds\n",
    "        self.oversample_indices = oversample_indices if oversample_indices is not None else list(range(len(self.data['seq_2_terms'])))\n",
    "        self.mask_embed = np.zeros(next(iter(self.data['go_embeds'].values())).shape, dtype=np.float32)\n",
    "        self.plm_dim  = self.data['plm_embeds'][next(iter(self.data['plm_embeds']))].shape[0]\n",
    "        self.blm_dim  = self.data['pmid_2_embed'][next(iter(self.data['pmid_2_embed']))].shape[0]\n",
    "\n",
    "        print(f\"PLM dim: {self.plm_dim}, BLM dim: {self.blm_dim}\")\n",
    "        #ensure len of predicted go terms is less than max_go_embeds\n",
    "        #self.data['seq_2_terms'] = self.data['seq_2_terms'][self\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.oversample_indices)         \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx = self.oversample_indices[idx]\n",
    "\n",
    "        row = self.data['seq_2_terms'].iloc[sample_idx]\n",
    "        qseqid = row['qseqid']\n",
    "\n",
    "        plm_embed = self.data['plm_embeds'][qseqid]\n",
    "\n",
    "        true_terms_set = set(row['terms_true'])\n",
    "        predicted_terms = row['terms_predicted']\n",
    "        \n",
    "        # Filter terms that have embeddings (should be all of them after padding)\n",
    "        # valid_terms = [term for term in predicted_terms if term in self.data['go_embeds']]\n",
    "        valid_terms = predicted_terms\n",
    "        # Vectorized operations using list comprehensions\n",
    "        go_embeds = np.array([self.data['go_embeds'].get(term, self.mask_embed) for term in valid_terms])\n",
    "        label = np.array([term in true_terms_set for term in valid_terms], dtype=np.float32)\n",
    "        \n",
    "        pmid_list = list(self.data['prot_2_pmid'].get(qseqid, []))\n",
    "\n",
    "        #skip None embeddings and collect valid ones\n",
    "        valid_blm_embeds = [self.data['pmid_2_embed'].get(pmid) for pmid in pmid_list if self.data['pmid_2_embed'].get(pmid) is not None]\n",
    "        \n",
    "        # Create blm_embeds array - if no valid embeddings, create empty array with correct shape\n",
    "        if len(valid_blm_embeds) > 0:\n",
    "            blm_embeds = np.vstack(valid_blm_embeds)\n",
    "        else:\n",
    "            # Create empty array with shape [0, blm_dim]\n",
    "            blm_embeds = np.zeros((0, self.blm_dim), dtype=np.float32)\n",
    "        \n",
    "\n",
    "        return {\n",
    "            'entryID'   :       qseqid,\n",
    "            'plm_embed' :       plm_embed,\n",
    "            'blm_embeds':       blm_embeds,\n",
    "            'go_embed'  :       go_embeds,\n",
    "            'label'     :       label,\n",
    "            'predicted_terms':  valid_terms,\n",
    "            'true_terms':       row['terms_true']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "472cb4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLM dim: 1280, BLM dim: 768\n"
     ]
    }
   ],
   "source": [
    "dataset = EmbeddingsDataset(data, max_go_embeds=64)\n",
    "sample = dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e3d8d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_with_blm_projection(batch, blm_projection_layer, tokenizer, device=None, dtype=torch.float32, num_plm_tokens=32, num_blm_tokens=32):\n",
    "    \"\"\"\n",
    "    Custom collate function that tokenizes PLM features to 32 tokens and \n",
    "    projects BLM features to 32 tokens, stacking them to get 64 total tokens.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of samples from the dataset\n",
    "        blm_projection_layer: nn.Linear layer to project BLM features from their dim to model_dim (512)\n",
    "        tokenizer: Tokenizer to apply to PLM features (projects to 32 tokens) - REQUIRED\n",
    "        device: Device to move tensors to (cuda or cpu)\n",
    "        dtype: Target dtype for tensors (torch.float32, torch.float16, or torch.bfloat16)\n",
    "        num_plm_tokens: Number of PLM tokens (default: 32)\n",
    "        num_blm_tokens: Number of BLM tokens (default: 32)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "            - entryID: List of entry IDs\n",
    "            - features: Stacked PLM + BLM tokens [batch, 64, model_dim]\n",
    "            - mask: Attention mask [batch, 64]\n",
    "            - go_embed: GO embeddings [batch, num_terms, go_embed_dim]\n",
    "            - label: Labels [batch, num_terms]\n",
    "            - predicted_terms: List of predicted terms\n",
    "            - true_terms: List of true terms\n",
    "    \"\"\"\n",
    "    batch_size = len(batch)\n",
    "    model_dim = blm_projection_layer.out_features\n",
    "    \n",
    "    # Process PLM embeddings - stack and convert to tensors\n",
    "    plm_embeds = torch.stack([torch.from_numpy(item['plm_embed']) for item in batch])\n",
    "    plm_embeds = plm_embeds.to(dtype=dtype)\n",
    "    \n",
    "    if device is not None:\n",
    "        plm_embeds = plm_embeds.to(device)\n",
    "    \n",
    "    # Tokenize PLM embeddings to fixed number of tokens (32)\n",
    "    plm_tokens = tokenizer(plm_embeds)  # [batch, 32, model_dim]\n",
    "    \n",
    "    # Process BLM embeddings - project to model_dim, cap at 32, and pad to 32\n",
    "    blm_embeds_padded = torch.zeros(batch_size, num_blm_tokens, model_dim, dtype=dtype)\n",
    "    blm_attention_mask = torch.zeros(batch_size, num_blm_tokens, dtype=torch.bool)\n",
    "    \n",
    "    if device is not None:\n",
    "        blm_embeds_padded = blm_embeds_padded.to(device)\n",
    "        blm_attention_mask = blm_attention_mask.to(device)\n",
    "    \n",
    "    for i, item in enumerate(batch):\n",
    "        blm = torch.from_numpy(item['blm_embeds']).to(dtype=dtype)  # [num_tokens, blm_dim]\n",
    "        \n",
    "        # Cap at num_blm_tokens (32)\n",
    "        actual_tokens = min(blm.shape[0], num_blm_tokens)\n",
    "        if actual_tokens > 0:\n",
    "            blm = blm[:actual_tokens]\n",
    "            \n",
    "            if device is not None:\n",
    "                blm = blm.to(device)\n",
    "            \n",
    "            # Project to model_dim (512)\n",
    "            with torch.no_grad():\n",
    "                blm_projected = blm_projection_layer(blm)  # [actual_tokens, model_dim]\n",
    "            \n",
    "            # Place in padded tensor\n",
    "            blm_embeds_padded[i, :actual_tokens] = blm_projected\n",
    "            blm_attention_mask[i, :actual_tokens] = True\n",
    "    \n",
    "    # Stack PLM tokens (32) and BLM tokens (32) to get 64 tokens total\n",
    "    features = torch.cat([plm_tokens, blm_embeds_padded], dim=1)  # [batch, 64, model_dim]\n",
    "    \n",
    "    # Create combined attention mask (PLM tokens are always valid, BLM may be padded)\n",
    "    plm_attention_mask = torch.ones(batch_size, num_plm_tokens, dtype=torch.bool)\n",
    "    if device is not None:\n",
    "        plm_attention_mask = plm_attention_mask.to(device)\n",
    "    \n",
    "    mask = torch.cat([plm_attention_mask, blm_attention_mask], dim=1)  # [batch, 64]\n",
    "    \n",
    "    # Process GO embeddings and labels\n",
    "    go_embed = torch.stack([torch.from_numpy(item['go_embed']) for item in batch])\n",
    "    go_embed = go_embed.to(dtype=dtype)\n",
    "    \n",
    "    label = torch.stack([torch.from_numpy(item['label']) for item in batch])\n",
    "    label = label.to(dtype=dtype)\n",
    "    \n",
    "    if device is not None:\n",
    "        go_embed = go_embed.to(device)\n",
    "        label = label.to(device)\n",
    "    \n",
    "    return {\n",
    "        'entryID': [item['entryID'] for item in batch],\n",
    "        'features': features,\n",
    "        'mask': mask,\n",
    "        'go_embed': go_embed,\n",
    "        'label': label,\n",
    "        'predicted_terms': [item['predicted_terms'] for item in batch],\n",
    "        'true_terms': [item['true_terms'] for item in batch]\n",
    "    }\n",
    "\n",
    "\n",
    "class PrefetchLoaderWithBLM:\n",
    "    \"\"\"\n",
    "    Prefetch loader that loads batches asynchronously to GPU for faster training.\n",
    "    Handles PLM tokenization (32 tokens), BLM projection (32 tokens), stacking to 64 tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader, device, blm_projection_layer, tokenizer, num_plm_tokens=32, num_blm_tokens=32, max_prefetch=1):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        self.blm_projection_layer = blm_projection_layer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_plm_tokens = num_plm_tokens\n",
    "        self.num_blm_tokens = num_blm_tokens\n",
    "        self.max_prefetch = max_prefetch\n",
    "        self.stream = torch.cuda.Stream() if device.type == 'cuda' else None\n",
    "        \n",
    "        # Move projection layer to device\n",
    "        self.blm_projection_layer = self.blm_projection_layer.to(device)\n",
    "        \n",
    "        # Move tokenizer to device\n",
    "        self.tokenizer = self.tokenizer.to(device)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.stream is not None:\n",
    "            return self._cuda_iter()\n",
    "        else:\n",
    "            return self._cpu_iter()\n",
    "    \n",
    "    def _cpu_iter(self):\n",
    "        \"\"\"Iterator without prefetching for CPU.\"\"\"\n",
    "        for batch in self.dataloader:\n",
    "            # Apply collate with projection and tokenization\n",
    "            batch = collate_with_blm_projection(\n",
    "                batch, \n",
    "                self.blm_projection_layer,\n",
    "                tokenizer=self.tokenizer,\n",
    "                device=self.device, \n",
    "                dtype=next(self.blm_projection_layer.parameters()).dtype,\n",
    "                num_plm_tokens=self.num_plm_tokens,\n",
    "                num_blm_tokens=self.num_blm_tokens\n",
    "            )\n",
    "            yield batch\n",
    "    \n",
    "    def _cuda_iter(self):\n",
    "        \"\"\"Iterator with CUDA stream prefetching.\"\"\"\n",
    "        loader_iter = iter(self.dataloader)\n",
    "        \n",
    "        # Preload first batch\n",
    "        try:\n",
    "            with torch.cuda.stream(self.stream):\n",
    "                next_batch = next(loader_iter)\n",
    "                next_batch = self._process_batch(next_batch)\n",
    "        except StopIteration:\n",
    "            return\n",
    "        \n",
    "        while True:\n",
    "            # Wait for the prefetch stream to finish\n",
    "            torch.cuda.current_stream().wait_stream(self.stream)\n",
    "            batch = next_batch\n",
    "            \n",
    "            # Record stream for tensors\n",
    "            if isinstance(batch, dict):\n",
    "                for k, v in batch.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        v.record_stream(torch.cuda.current_stream())\n",
    "            \n",
    "            # Start loading next batch in background\n",
    "            try:\n",
    "                with torch.cuda.stream(self.stream):\n",
    "                    next_batch = next(loader_iter)\n",
    "                    next_batch = self._process_batch(next_batch)\n",
    "            except StopIteration:\n",
    "                yield batch\n",
    "                del batch\n",
    "                break\n",
    "                \n",
    "            yield batch\n",
    "            del batch\n",
    "    \n",
    "    def _process_batch(self, batch):\n",
    "        \"\"\"Process batch with PLM tokenization, BLM projection, and move to device.\"\"\"\n",
    "        return collate_with_blm_projection(\n",
    "            batch,\n",
    "            self.blm_projection_layer,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device=self.device,\n",
    "            dtype=next(self.blm_projection_layer.parameters()).dtype,\n",
    "            num_plm_tokens=self.num_plm_tokens,\n",
    "            num_blm_tokens=self.num_blm_tokens\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3cedae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.tokenizer import EmbedTokenizer\n",
    "\n",
    "tokenizer = EmbedTokenizer(1280, 512, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2190d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Initialize the BLM projection layer and tokenizer\n",
    "model_dim = 512  # Target dimension\n",
    "blm_dim = dataset.blm_dim  # Get from dataset\n",
    "plm_dim = dataset.plm_dim  # Get from dataset\n",
    "\n",
    "# Create projection layer to project BLM features to model_dim\n",
    "blm_projection = torch.nn.Linear(blm_dim, model_dim)\n",
    "\n",
    "# Create tokenizer for PLM features (must tokenize to 32 tokens)\n",
    "# tokenizer should already be defined above (e.g., EmbedTokenizer(1280, 512, 32))\n",
    "# tokenizer = EmbedTokenizer(plm_dim, model_dim, 32)\n",
    "\n",
    "# Simple collate function that just returns the batch as-is (no stacking)\n",
    "def simple_collate(batch):\n",
    "    return batch\n",
    "\n",
    "# Create dataloader with simple collate (we'll handle the actual collation in PrefetchLoader)\n",
    "train_dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Use 0 for single process, or multi-worker compatible\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    collate_fn=simple_collate  # Don't stack yet - let PrefetchLoader handle it\n",
    ")\n",
    "\n",
    "# Wrap with PrefetchLoader that handles PLM tokenization (32) + BLM projection (32) = 64 tokens\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader = PrefetchLoaderWithBLM(\n",
    "    train_dataloader,\n",
    "    device=device,\n",
    "    blm_projection_layer=blm_projection,\n",
    "    tokenizer=tokenizer,  # REQUIRED: tokenizer for PLM features (32 tokens)\n",
    "    num_plm_tokens=32,    # Fixed: 32 PLM tokens\n",
    "    num_blm_tokens=32     # Fixed: 32 BLM tokens (padded if less)\n",
    ")\n",
    "\n",
    "# Now you can iterate through the loader\n",
    "# for batch in train_loader:\n",
    "#     features = batch['features']  # [batch, 64, 512] - 32 PLM + 32 BLM tokens\n",
    "#     mask = batch['mask']          # [batch, 64] - combined attention mask\n",
    "#     go_embed = batch['go_embed']  # [batch, num_terms, go_dim]\n",
    "#     label = batch['label']        # [batch, num_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "deff289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3992b276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64, 512]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([32, 64, 512]),\n",
       " torch.Size([32, 64]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['features'].shape, batch['mask'].shape, batch['go_embed'].shape, batch['label'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f20719",
   "metadata": {},
   "source": [
    "## -------Analysis----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "745a7931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proteins with PMID annotations: 82404\n",
      "\n",
      "PMIDs per protein statistics:\n",
      "Mean: 6.97\n",
      "Min: 0\n",
      "Max: 224\n",
      "Median: 5.00\n",
      "Std: 7.50\n",
      "\n",
      "Distribution:\n",
      "Proteins with 0 PMIDs: 589\n",
      "Proteins with 1-5 PMIDs: 45223\n",
      "Proteins with 6-10 PMIDs: 22675\n",
      "Proteins with 10 -64 PMIDs: 13756\n",
      "Proteins with >64 PMIDs: 161\n"
     ]
    }
   ],
   "source": [
    "# Get all PMID lists from the dictionary\n",
    "pmid_lists = [pmids for pmids in data['prot_2_pmid'].values()]\n",
    "\n",
    "# Calculate statistics\n",
    "num_pmids_per_protein = [len(pmids) for pmids in pmid_lists]\n",
    "\n",
    "print(f\"Number of proteins with PMID annotations: {len(pmid_lists)}\")\n",
    "print(f\"\\nPMIDs per protein statistics:\")\n",
    "print(f\"Mean: {np.mean(num_pmids_per_protein):.2f}\")\n",
    "print(f\"Min: {np.min(num_pmids_per_protein)}\")\n",
    "print(f\"Max: {np.max(num_pmids_per_protein)}\")\n",
    "print(f\"Median: {np.median(num_pmids_per_protein):.2f}\")\n",
    "print(f\"Std: {np.std(num_pmids_per_protein):.2f}\")\n",
    "\n",
    "# Additional distribution info\n",
    "print(f\"\\nDistribution:\")\n",
    "print(f\"Proteins with 0 PMIDs: {sum(1 for n in num_pmids_per_protein if n == 0)}\")\n",
    "print(f\"Proteins with 1-5 PMIDs: {sum(1 for n in num_pmids_per_protein if 1 <= n <= 5)}\")\n",
    "print(f\"Proteins with 6-10 PMIDs: {sum(1 for n in num_pmids_per_protein if 6 <= n <= 10)}\")\n",
    "print(f\"Proteins with 10 -56 PMIDs: {sum(1 for n in num_pmids_per_protein if 10 < n <= 64)}\")\n",
    "print(f\"Proteins with >56 PMIDs: {sum(1 for n in num_pmids_per_protein if n > 64)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe93d346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proteins with >32 PMIDs: 1106\n"
     ]
    }
   ],
   "source": [
    "print(f\"Proteins with >32 PMIDs: {sum(1 for n in num_pmids_per_protein if n > 32)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab1a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d42111fc",
   "metadata": {},
   "source": [
    "## Integration Complete ✅\n",
    "\n",
    "Successfully integrated the new dataset and dataloader with BLM embeddings into the training pipeline:\n",
    "\n",
    "### Changes Made:\n",
    "\n",
    "1. **EmbeddingsDataset.py**:\n",
    "   - Updated `EmbeddingsDataset` to return both `plm_embed` and `blm_embeds`\n",
    "   - Added `simple_collate` function to avoid premature stacking\n",
    "   - Added `collate_with_blm_projection` for PLM tokenization + BLM projection\n",
    "   - Replaced `PrefetchLoader` with `PrefetchLoaderWithBLM` (32 PLM + 32 BLM = 64 tokens)\n",
    "\n",
    "2. **train.py**:\n",
    "   - Updated imports to use new loader classes\n",
    "   - Created BLM projection layer (`blm_projection`)\n",
    "   - Modified tokenizer to only handle PLM (32 tokens instead of 64)\n",
    "   - Updated dataloaders to use `simple_collate` and `PrefetchLoaderWithBLM`\n",
    "\n",
    "3. **configs_new.json**:\n",
    "   - Changed `features_embeds_path` → `plm_features_path`\n",
    "   - Added `prot_2_pmid_path` and `pmid_2_embed_path` for BLM data\n",
    "   - Removed `features_ids_path` (not needed with .npy dict format)\n",
    "\n",
    "4. **Dataset/Utils.py**:\n",
    "   - Updated `load_data` to load PLM, BLM mappings, and PMID embeddings\n",
    "   - Changed output keys: `features_embeds` → `plm_embeds`\n",
    "   - Added `prot_2_pmid` and `pmid_2_embed` to output\n",
    "\n",
    "### Output Format:\n",
    "- **features**: `[batch, 64, 512]` - 32 PLM tokens + 32 BLM tokens\n",
    "- **mask**: `[batch, 64]` - Combined attention mask\n",
    "- **go_embed**: `[batch, num_terms, go_dim]`\n",
    "- **label**: `[batch, num_terms]`\n",
    "\n",
    "Ready to train with: `python train.py --config configs_new.json`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cafa6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
