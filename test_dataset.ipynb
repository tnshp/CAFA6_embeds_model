{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27d7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import obonet\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30fa9f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def print_size(data):\n",
    "    print(f'{sys.getsizeof(data) / (1024 * 1024):.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c63dffe",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 141\u001b[39m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m    133\u001b[39m data_paths = {\n\u001b[32m    134\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mseq_2_terms_df\u001b[39m\u001b[33m'\u001b[39m:       \u001b[33m'\u001b[39m\u001b[33m/mnt/d/ML/Kaggle/CAFA6-new/seq_2_terms.parquet\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    135\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtrain_terms_df\u001b[39m\u001b[33m'\u001b[39m:       \u001b[33m'\u001b[39m\u001b[33m/mnt/d/ML/Kaggle/CAFA6/cafa-6-protein-function-prediction/Train/train_terms.tsv\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    138\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mgo_embeds_paths\u001b[39m\u001b[33m'\u001b[39m:      \u001b[33m'\u001b[39m\u001b[33m/mnt/d/ML/Kaggle/CAFA6-new/uniprot/go_embeddings.pkl\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    139\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m data = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_terms\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maspect\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mF\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m data[\u001b[33m'\u001b[39m\u001b[33mseq_2_terms\u001b[39m\u001b[33m'\u001b[39m].shape\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m(data_paths, max_terms, aspect)\u001b[39m\n\u001b[32m     66\u001b[39m features_ids_path = data_paths[\u001b[33m'\u001b[39m\u001b[33mfeatures_ids_path\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     68\u001b[39m go_embeds_paths = data_paths[\u001b[33m'\u001b[39m\u001b[33mgo_embeds_paths\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m seq_2_terms = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_2_terms_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfastparquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m train_terms = pd.read_csv(train_terms_df, sep=\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     73\u001b[39m term_to_aspect = train_terms.groupby(\u001b[33m'\u001b[39m\u001b[33mterm\u001b[39m\u001b[33m'\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33maspect\u001b[39m\u001b[33m'\u001b[39m].first().to_dict()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cafa6/lib/python3.12/site-packages/pandas/io/parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cafa6/lib/python3.12/site-packages/pandas/io/parquet.py:405\u001b[39m, in \u001b[36mFastParquetImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    404\u001b[39m     parquet_file = \u001b[38;5;28mself\u001b[39m.api.ParquetFile(path, **parquet_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparquet_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cafa6/lib/python3.12/site-packages/fastparquet/api.py:790\u001b[39m, in \u001b[36mParquetFile.to_pandas\u001b[39m\u001b[34m(self, columns, categories, filters, index, row_filter, dtypes)\u001b[39m\n\u001b[32m    786\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    787\u001b[39m     parts = {name: (v \u001b[38;5;28;01mif\u001b[39;00m name.endswith(\u001b[33m'\u001b[39m\u001b[33m-catdef\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    788\u001b[39m                     \u001b[38;5;28;01melse\u001b[39;00m v[start:start + thislen])\n\u001b[32m    789\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m (name, v) \u001b[38;5;129;01min\u001b[39;00m views.items()}\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_row_group_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m                             \u001b[49m\u001b[43massign\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition_meta\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartition_meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mrow_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43msel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    793\u001b[39m     start += thislen\n\u001b[32m    794\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cafa6/lib/python3.12/site-packages/fastparquet/api.py:388\u001b[39m, in \u001b[36mParquetFile.read_row_group_file\u001b[39m\u001b[34m(self, rg, columns, categories, index, assign, partition_meta, row_filter, infile)\u001b[39m\n\u001b[32m    385\u001b[39m     ret = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    386\u001b[39m f = infile \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.open(fn, mode=\u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m \u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_row_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mselfmade\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mselfmade\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43massign\u001b[49m\u001b[43m=\u001b[49m\u001b[43massign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheme\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_scheme\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition_meta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrow_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow_filter\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[32m    395\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cafa6/lib/python3.12/site-packages/fastparquet/core.py:644\u001b[39m, in \u001b[36mread_row_group\u001b[39m\u001b[34m(file, rg, columns, categories, schema_helper, cats, selfmade, index, assign, scheme, partition_meta, row_filter)\u001b[39m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m assign \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    643\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mGoing with pre-allocation!\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m \u001b[43mread_row_group_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_helper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mcats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselfmade\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign\u001b[49m\u001b[43m=\u001b[49m\u001b[43massign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow_filter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cat \u001b[38;5;129;01min\u001b[39;00m cats:\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cat \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m assign:\n\u001b[32m    649\u001b[39m         \u001b[38;5;66;03m# do no need to have partition columns in output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cafa6/lib/python3.12/site-packages/fastparquet/core.py:615\u001b[39m, in \u001b[36mread_row_group_arrays\u001b[39m\u001b[34m(file, rg, columns, categories, schema_helper, cats, selfmade, assign, row_filter)\u001b[39m\n\u001b[32m    612\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    613\u001b[39m remains.discard(name)\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m \u001b[43mread_col\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_helper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m+\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-catdef\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m         \u001b[49m\u001b[43mselfmade\u001b[49m\u001b[43m=\u001b[49m\u001b[43mselfmade\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m         \u001b[49m\u001b[43mcatdef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m+\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-catdef\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m         \u001b[49m\u001b[43mrow_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow_filter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_map_like(schema_helper, column):\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# TODO: could be done in fast loop in _assemble_objects?\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m maps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cafa6/lib/python3.12/site-packages/fastparquet/core.py:584\u001b[39m, in \u001b[36mread_col\u001b[39m\u001b[34m(column, schema_helper, infile, use_cat, selfmade, assign, catdef, row_filter)\u001b[39m\n\u001b[32m    582\u001b[39m     piece[:] = dic[val]\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_cat:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     piece[:] = \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43massign\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    586\u001b[39m     piece[:] = val\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cafa6/lib/python3.12/site-packages/fastparquet/converted_types.py:230\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(data, se, timestamp96, dtype)\u001b[39m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# TODO: unnecessary list - loop would save memory, and can cythonize\u001b[39;00m\n\u001b[32m    229\u001b[39m     decoder = json_decoder()\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     out[:] = [\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m ctype == parquet_thrift.ConvertedType.BSON:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cafa6/lib/python3.12/site-packages/fastparquet/json.py:81\u001b[39m, in \u001b[36mJsonImpl.loads\u001b[39m\u001b[34m(self, s)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloads\u001b[39m(\u001b[38;5;28mself\u001b[39m, s):\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cafa6/lib/python3.12/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cafa6/lib/python3.12/json/decoder.py:338\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    334\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    336\u001b[39m \n\u001b[32m    337\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     end = _w(s, end).end()\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cafa6/lib/python3.12/json/decoder.py:354\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    345\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[33;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[32m    347\u001b[39m \u001b[33;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    351\u001b[39m \n\u001b[32m    352\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    356\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def pad_with_random_terms(terms, max_terms, all_terms_array, existing_terms_set=None):\n",
    "    \"\"\"Optimized padding with random GO terms using rejection sampling.\"\"\"\n",
    "    if len(terms) >= max_terms:\n",
    "        return terms[:max_terms]\n",
    "    \n",
    "    # Calculate how many more terms we need\n",
    "    num_needed = max_terms - len(terms)\n",
    "    \n",
    "    # Use pre-computed set if available, otherwise create it\n",
    "    if existing_terms_set is None:\n",
    "        existing_terms_set = set(terms)\n",
    "    \n",
    "    # For small num_needed relative to available terms, use rejection sampling\n",
    "    # This avoids creating the full boolean mask\n",
    "    if num_needed < len(all_terms_array) * 0.1:  # If we need < 10% of total terms\n",
    "        random_terms = []\n",
    "        max_attempts = num_needed * 10  # Prevent infinite loop\n",
    "        attempts = 0\n",
    "        \n",
    "        while len(random_terms) < num_needed and attempts < max_attempts:\n",
    "            # Sample with replacement first (fast)\n",
    "            batch_size = min(num_needed * 2, 1000)  # Sample in batches\n",
    "            candidates = np.random.choice(all_terms_array, size=batch_size, replace=True)\n",
    "            \n",
    "            # Filter out existing terms\n",
    "            for candidate in candidates:\n",
    "                if candidate not in existing_terms_set:\n",
    "                    random_terms.append(candidate)\n",
    "                    existing_terms_set.add(candidate)\n",
    "                    if len(random_terms) >= num_needed:\n",
    "                        break\n",
    "            \n",
    "            attempts += batch_size\n",
    "        \n",
    "        # If rejection sampling didn't get enough, fall back to full filtering\n",
    "        if len(random_terms) < num_needed:\n",
    "            available_mask = np.isin(all_terms_array, list(existing_terms_set), invert=True)\n",
    "            available_terms = all_terms_array[available_mask]\n",
    "            remaining_needed = num_needed - len(random_terms)\n",
    "            \n",
    "            if len(available_terms) >= remaining_needed:\n",
    "                additional = np.random.choice(available_terms, size=remaining_needed, replace=False).tolist()\n",
    "            else:\n",
    "                additional = np.random.choice(available_terms, size=remaining_needed, replace=True).tolist()\n",
    "            \n",
    "            random_terms.extend(additional)\n",
    "    else:\n",
    "        # For large num_needed, use vectorized filtering (more efficient for bulk)\n",
    "        available_mask = np.isin(all_terms_array, list(existing_terms_set), invert=True)\n",
    "        available_terms = all_terms_array[available_mask]\n",
    "        \n",
    "        # Randomly sample\n",
    "        if len(available_terms) >= num_needed:\n",
    "            random_terms = np.random.choice(available_terms, size=num_needed, replace=False).tolist()\n",
    "        else:\n",
    "            # If not enough unique terms, sample with replacement\n",
    "            random_terms = np.random.choice(available_terms, size=num_needed, replace=True).tolist()\n",
    "    \n",
    "    return terms + random_terms[:num_needed]\n",
    "\n",
    "def load_data(data_paths, max_terms=256, aspect=None):\n",
    "    \n",
    "    seq_2_terms_df = data_paths['seq_2_terms_df']\n",
    "    train_terms_df = data_paths['train_terms_df']\n",
    "    features_embeds_path = data_paths['features_embeds_path']\n",
    "    features_ids_path = data_paths['features_ids_path']\n",
    "\n",
    "    go_embeds_paths = data_paths['go_embeds_paths']\n",
    "\n",
    "    seq_2_terms = pd.read_parquet(seq_2_terms_df, engine='fastparquet')\n",
    "    train_terms = pd.read_csv(train_terms_df, sep='\\t')\n",
    "\n",
    "    term_to_aspect = train_terms.groupby('term')['aspect'].first().to_dict()\n",
    "        \n",
    "    with open(go_embeds_paths, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        embeddings_dict = data['embeddings']\n",
    "        go_ids = data['go_ids']\n",
    "\n",
    "    # Filter to keep only terms from a specific aspect if aspect is provided\n",
    "    print('filtering by aspect:', aspect)\n",
    "    if aspect is not None:\n",
    "        seq_2_terms['terms_predicted'] = seq_2_terms['terms_predicted'].apply(\n",
    "            lambda terms: [t for t in terms if term_to_aspect.get(t) == aspect]\n",
    "        )\n",
    "        seq_2_terms['terms_true'] = seq_2_terms['terms_true'].apply(\n",
    "            lambda terms: [t for t in terms if term_to_aspect.get(t) == aspect]\n",
    "        )\n",
    "        # Remove rows where terms_predicted or terms_true is now empty\n",
    "        seq_2_terms = seq_2_terms[seq_2_terms['terms_predicted'].apply(len) > 0]\n",
    "        seq_2_terms = seq_2_terms[seq_2_terms['terms_true'].apply(len) > 0]\n",
    "\n",
    "        # Pad terms with random terms from the same aspect\n",
    "        print(f\"Padding terms_predicted with random terms from aspect {aspect}...\")\n",
    "        # Get all terms from this aspect that have embeddings\n",
    "        aspect_terms = [term for term, asp in term_to_aspect.items() if asp == aspect and term in embeddings_dict]\n",
    "        all_aspect_terms = np.array(aspect_terms)\n",
    "        \n",
    "        tqdm.pandas(desc=f\"Padding with random {aspect} terms\")\n",
    "        seq_2_terms['terms_predicted'] = seq_2_terms['terms_predicted'].progress_apply(\n",
    "            lambda terms: pad_with_random_terms(terms, max_terms, all_aspect_terms)\n",
    "        )\n",
    "        \n",
    "        # Verify padding\n",
    "        term_lengths_after = seq_2_terms['terms_predicted'].apply(len)\n",
    "        print(f\"After padding - Min: {term_lengths_after.min()}, Max: {term_lengths_after.max()}, Mean: {term_lengths_after.mean():.2f}\")\n",
    "\n",
    "    \n",
    "    print(\"loading features embeddings and ids\")\n",
    "    features_embeds = np.load(features_embeds_path, allow_pickle=True)\n",
    "    features_ids = np.load(features_ids_path, allow_pickle=True)\n",
    "    \n",
    "    print(\"creating features embeddings dict\")\n",
    "    features_embeds_dict = {feat_id: embed for feat_id, embed in zip(features_ids, features_embeds)}\n",
    "\n",
    "    term_lengths = seq_2_terms['terms_predicted'].apply(len)\n",
    "\n",
    "    print(\"filtering sequences by term lengths\")\n",
    "   #currently only using sequences with 256 terms, need to change later \n",
    "    # seq_2_terms = seq_2_terms[term_lengths == max_terms]\n",
    "\n",
    "    train_ids =  pd.DataFrame(features_ids, columns=['qseqid'])\n",
    "    seq_2_terms = seq_2_terms.merge(train_ids, on='qseqid', how='inner')    \n",
    "\n",
    "    out = {'seq_2_terms': seq_2_terms,\n",
    "           'train_terms': train_terms,\n",
    "           'features_embeds': features_embeds_dict,\n",
    "           'go_embeds': embeddings_dict,\n",
    "           }\n",
    "    \n",
    "    return out\n",
    "\n",
    "data_paths = {\n",
    "    'seq_2_terms_df':       '/mnt/d/ML/Kaggle/CAFA6-new/seq_2_terms.parquet',\n",
    "    'train_terms_df':       '/mnt/d/ML/Kaggle/CAFA6/cafa-6-protein-function-prediction/Train/train_terms.tsv',\n",
    "    \"features_embeds_path\": \"/mnt/d/ML/Kaggle/CAFA6-new/Dataset/archive/protein_embeddings.npy\",\n",
    "    \"features_ids_path\":    \"/mnt/d/ML/Kaggle/CAFA6-new/Dataset/archive/protein_ids.npy\",\n",
    "    'go_embeds_paths':      '/mnt/d/ML/Kaggle/CAFA6-new/uniprot/go_embeddings.pkl'\n",
    "}\n",
    "\n",
    "data = load_data(data_paths, max_terms=64, aspect='F')\n",
    "data['seq_2_terms'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb1adc",
   "metadata": {},
   "source": [
    "## Data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e832a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_terms_with_neighbors(terms, go_graph, go_embeds, max_terms=256):\n",
    "    \"\"\"\n",
    "    Pad a list of GO terms with neighboring terms from the GO graph.\n",
    "    \n",
    "    Args:\n",
    "        terms: List of GO term IDs\n",
    "        go_graph: networkx graph of GO ontology\n",
    "        go_embeds: Dictionary of GO embeddings (to check if term has embedding)\n",
    "        max_terms: Maximum number of terms to return\n",
    "    \n",
    "    Returns:\n",
    "        List of GO terms padded to max_terms\n",
    "    \"\"\"\n",
    "    padded_terms = list(terms)\n",
    "    \n",
    "    if len(padded_terms) >= max_terms:\n",
    "        return padded_terms[:max_terms]\n",
    "    \n",
    "    # Use a set for faster lookup\n",
    "    terms_set = set(padded_terms)\n",
    "    \n",
    "    # Collect neighbors from existing terms\n",
    "    candidates = set()\n",
    "    for term in padded_terms:\n",
    "        if term in go_graph:\n",
    "            # Get parents (is_a relationships)\n",
    "            if 'is_a' in go_graph.nodes[term]:\n",
    "                parents = go_graph.nodes[term].get('is_a', [])\n",
    "                if isinstance(parents, str):\n",
    "                    parents = [parents]\n",
    "                candidates.update(parents)\n",
    "            \n",
    "            # Get successors (children) and predecessors (parents)\n",
    "            candidates.update(go_graph.successors(term))\n",
    "            candidates.update(go_graph.predecessors(term))\n",
    "    \n",
    "    # Remove terms already in the list\n",
    "    candidates = candidates - terms_set\n",
    "    \n",
    "    # Filter candidates to only those with embeddings\n",
    "    candidates = [c for c in candidates if c in go_embeds]\n",
    "    \n",
    "    # Add candidates until we reach max_terms\n",
    "    for candidate in candidates:\n",
    "        if len(padded_terms) >= max_terms:\n",
    "            break\n",
    "        padded_terms.append(candidate)\n",
    "        terms_set.add(candidate)\n",
    "    \n",
    "    # If still not enough, try neighbors of neighbors\n",
    "    if len(padded_terms) < max_terms:\n",
    "        second_level_candidates = set()\n",
    "        for term in candidates[:100]:  # Limit to avoid too much computation\n",
    "            if term in go_graph:\n",
    "                if 'is_a' in go_graph.nodes[term]:\n",
    "                    parents = go_graph.nodes[term].get('is_a', [])\n",
    "                    if isinstance(parents, str):\n",
    "                        parents = [parents]\n",
    "                    second_level_candidates.update(parents)\n",
    "                second_level_candidates.update(go_graph.successors(term))\n",
    "                second_level_candidates.update(go_graph.predecessors(term))\n",
    "        \n",
    "        second_level_candidates = second_level_candidates - terms_set\n",
    "        second_level_candidates = [c for c in second_level_candidates if c in go_embeds]\n",
    "        \n",
    "        for candidate in second_level_candidates:\n",
    "            if len(padded_terms) >= max_terms:\n",
    "                break\n",
    "            padded_terms.append(candidate)\n",
    "    \n",
    "    return padded_terms\n",
    "\n",
    "def build_neighbor_cache(go_graph, go_embeds):\n",
    "    \"\"\"Pre-compute neighbors for all GO terms to avoid repeated graph queries.\"\"\"\n",
    "    print(\"Building neighbor cache for all GO terms...\")\n",
    "    go_embeds_set = set(go_embeds.keys())\n",
    "    cache = {}\n",
    "    \n",
    "    for term in tqdm(go_embeds_set, desc=\"Caching neighbors\"):\n",
    "        if term not in go_graph:\n",
    "            cache[term] = set()\n",
    "            continue\n",
    "            \n",
    "        neighbors = set()\n",
    "        # Get is_a parents\n",
    "        parents = go_graph.nodes[term].get('is_a', [])\n",
    "        if isinstance(parents, str):\n",
    "            neighbors.add(parents)\n",
    "        else:\n",
    "            neighbors.update(parents)\n",
    "        # Get successors and predecessors\n",
    "        neighbors.update(go_graph.successors(term))\n",
    "        neighbors.update(go_graph.predecessors(term))\n",
    "        \n",
    "        # Only keep neighbors that have embeddings\n",
    "        cache[term] = neighbors & go_embeds_set\n",
    "    \n",
    "    print(f\"Neighbor cache built for {len(cache)} terms\")\n",
    "    return cache\n",
    "\n",
    "def pad_terms_with_random_walk(terms, neighbor_cache, max_terms=256, seed=None):\n",
    "    \"\"\"\n",
    "    Fast random walk padding using pre-computed neighbor cache.\n",
    "    \n",
    "    Args:\n",
    "        terms: List of GO term IDs\n",
    "        neighbor_cache: Pre-computed dictionary of term -> neighbors\n",
    "        max_terms: Maximum number of terms to return\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        List of GO terms padded to max_terms\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        rng = np.random.RandomState(seed)\n",
    "    else:\n",
    "        rng = np.random.RandomState()\n",
    "    \n",
    "    if len(terms) >= max_terms:\n",
    "        return list(terms)[:max_terms]\n",
    "    \n",
    "    padded_terms = list(terms)\n",
    "    terms_set = set(padded_terms)\n",
    "    \n",
    "    # First level: Collect neighbors from original terms\n",
    "    candidates = set()\n",
    "    for term in padded_terms:\n",
    "        neighbors = neighbor_cache.get(term, set())\n",
    "        valid_neighbors = neighbors - terms_set\n",
    "        if valid_neighbors:\n",
    "            # Sample 30% of neighbors for speed\n",
    "            n_sample = max(1, len(valid_neighbors) // 3)\n",
    "            if n_sample < len(valid_neighbors):\n",
    "                sampled = set(rng.choice(list(valid_neighbors), size=n_sample, replace=False))\n",
    "                candidates.update(sampled)\n",
    "            else:\n",
    "                candidates.update(valid_neighbors)\n",
    "    \n",
    "    # Add first-level candidates\n",
    "    candidates = list(candidates)\n",
    "    rng.shuffle(candidates)\n",
    "    num_needed = max_terms - len(padded_terms)\n",
    "    num_to_add = min(num_needed, len(candidates))\n",
    "    padded_terms.extend(candidates[:num_to_add])\n",
    "    terms_set.update(candidates[:num_to_add])\n",
    "    \n",
    "    # Second level if still needed\n",
    "    if len(padded_terms) < max_terms:\n",
    "        num_needed = max_terms - len(padded_terms)\n",
    "        # Expand only first 20 candidates\n",
    "        second_level = set()\n",
    "        for term in candidates[:min(20, len(candidates))]:\n",
    "            if len(second_level) >= num_needed * 2:\n",
    "                break\n",
    "            neighbors = neighbor_cache.get(term, set())\n",
    "            valid_neighbors = neighbors - terms_set\n",
    "            if valid_neighbors:\n",
    "                # Sample 20% at second level\n",
    "                n_sample = max(1, len(valid_neighbors) // 5)\n",
    "                if n_sample < len(valid_neighbors):\n",
    "                    sampled = set(rng.choice(list(valid_neighbors), size=n_sample, replace=False))\n",
    "                    second_level.update(sampled)\n",
    "                else:\n",
    "                    second_level.update(valid_neighbors)\n",
    "        \n",
    "        second_level = list(second_level)\n",
    "        rng.shuffle(second_level)\n",
    "        padded_terms.extend(second_level[:num_needed])\n",
    "    \n",
    "    return padded_terms\n",
    "\n",
    "def pad_dataframe_terms(seq_2_terms_df, go_graph, go_embeds, max_terms=256, use_random_walk=False, seed=None, neighbor_cache=None):\n",
    "    \"\"\"\n",
    "    Pad the terms_predicted column in the dataframe in-place using GO graph neighbors.\n",
    "    \n",
    "    Args:\n",
    "        seq_2_terms_df: DataFrame with 'terms_predicted' column\n",
    "        go_graph: networkx graph of GO ontology\n",
    "        go_embeds: Dictionary of GO embeddings\n",
    "        max_terms: Maximum number of terms per row\n",
    "        use_random_walk: If True, use random walk padding instead of deterministic\n",
    "        seed: Random seed for random walk (only used if use_random_walk=True)\n",
    "        neighbor_cache: Pre-computed neighbor cache (will be built if None and use_random_walk=True)\n",
    "    \"\"\"\n",
    "    padding_method = \"random walk\" if use_random_walk else \"deterministic neighbors\"\n",
    "    print(f\"Padding terms_predicted with GO graph {padding_method}...\")\n",
    "    \n",
    "    if use_random_walk:\n",
    "        # Build neighbor cache if not provided\n",
    "        if neighbor_cache is None:\n",
    "            neighbor_cache = build_neighbor_cache(go_graph, go_embeds)\n",
    "        \n",
    "        # Use tqdm progress bar for apply operation\n",
    "        tqdm.pandas(desc=f\"Padding with {padding_method}\")\n",
    "        seq_2_terms_df['terms_predicted'] = seq_2_terms_df['terms_predicted'].progress_apply(\n",
    "            lambda terms: pad_terms_with_random_walk(terms, neighbor_cache, max_terms, seed)\n",
    "        )\n",
    "    else:\n",
    "        tqdm.pandas(desc=f\"Padding with {padding_method}\")\n",
    "        seq_2_terms_df['terms_predicted'] = seq_2_terms_df['terms_predicted'].progress_apply(\n",
    "            lambda terms: pad_terms_with_neighbors(terms, go_graph, go_embeds, max_terms)\n",
    "        )\n",
    "    \n",
    "    print(f\"Padding complete. Average terms per row: {seq_2_terms_df['terms_predicted'].apply(len).mean():.2f}\")\n",
    "    return seq_2_terms_df\n",
    "\n",
    "def pad_with_random_terms(terms, max_terms, all_terms_array, existing_terms_set=None):\n",
    "    \"\"\"Optimized padding with random GO terms using rejection sampling.\"\"\"\n",
    "    if len(terms) >= max_terms:\n",
    "        return terms[:max_terms]\n",
    "    \n",
    "    # Calculate how many more terms we need\n",
    "    num_needed = max_terms - len(terms)\n",
    "    \n",
    "    # Use pre-computed set if available, otherwise create it\n",
    "    if existing_terms_set is None:\n",
    "        existing_terms_set = set(terms)\n",
    "    \n",
    "    # For small num_needed relative to available terms, use rejection sampling\n",
    "    # This avoids creating the full boolean mask\n",
    "    if num_needed < len(all_terms_array) * 0.1:  # If we need < 10% of total terms\n",
    "        random_terms = []\n",
    "        max_attempts = num_needed * 10  # Prevent infinite loop\n",
    "        attempts = 0\n",
    "        \n",
    "        while len(random_terms) < num_needed and attempts < max_attempts:\n",
    "            # Sample with replacement first (fast)\n",
    "            batch_size = min(num_needed * 2, 1000)  # Sample in batches\n",
    "            candidates = np.random.choice(all_terms_array, size=batch_size, replace=True)\n",
    "            \n",
    "            # Filter out existing terms\n",
    "            for candidate in candidates:\n",
    "                if candidate not in existing_terms_set:\n",
    "                    random_terms.append(candidate)\n",
    "                    existing_terms_set.add(candidate)\n",
    "                    if len(random_terms) >= num_needed:\n",
    "                        break\n",
    "            \n",
    "            attempts += batch_size\n",
    "        \n",
    "        # If rejection sampling didn't get enough, fall back to full filtering\n",
    "        if len(random_terms) < num_needed:\n",
    "            available_mask = np.isin(all_terms_array, list(existing_terms_set), invert=True)\n",
    "            available_terms = all_terms_array[available_mask]\n",
    "            remaining_needed = num_needed - len(random_terms)\n",
    "            \n",
    "            if len(available_terms) >= remaining_needed:\n",
    "                additional = np.random.choice(available_terms, size=remaining_needed, replace=False).tolist()\n",
    "            else:\n",
    "                additional = np.random.choice(available_terms, size=remaining_needed, replace=True).tolist()\n",
    "            \n",
    "            random_terms.extend(additional)\n",
    "    else:\n",
    "        # For large num_needed, use vectorized filtering (more efficient for bulk)\n",
    "        available_mask = np.isin(all_terms_array, list(existing_terms_set), invert=True)\n",
    "        available_terms = all_terms_array[available_mask]\n",
    "        \n",
    "        # Randomly sample\n",
    "        if len(available_terms) >= num_needed:\n",
    "            random_terms = np.random.choice(available_terms, size=num_needed, replace=False).tolist()\n",
    "        else:\n",
    "            # If not enough unique terms, sample with replacement\n",
    "            random_terms = np.random.choice(available_terms, size=num_needed, replace=True).tolist()\n",
    "    \n",
    "    return terms + random_terms[:num_needed]\n",
    "    \n",
    "\n",
    "def prepare_data(data_paths, max_terms=256, aspect=None, oversample_dict=None):\n",
    "    \"\"\"\n",
    "    Prepare data with optional oversampling using different random walk seeds.\n",
    "    \n",
    "    Args:\n",
    "        data_paths: Dictionary of file paths\n",
    "        max_terms: Maximum number of terms per sequence\n",
    "        aspect: Filter by specific GO aspect (C, F, P) or None for all\n",
    "        oversample_dict: Dictionary mapping protein_id (qseqid) to sampling factor.\n",
    "                        Each protein will be sampled 'factor' times with different random walk seeds.\n",
    "                        Example: {'A0A023FBW4': 3, 'P12345': 2} will create 3 copies of the first\n",
    "                        protein and 2 copies of the second, each with different padding.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing seq_2_terms, train_terms, features_embeds, go_embeds, go_graph\n",
    "    \"\"\"\n",
    "    \n",
    "    knn_terms_df = data_paths['knn_terms_df']\n",
    "    train_terms_df = data_paths['train_terms_df']\n",
    "    features_embeds_path = data_paths['features_embeds_path']\n",
    "    features_ids_path = data_paths['features_ids_path']\n",
    "\n",
    "    go_embeds_paths = data_paths['go_embeds_paths']\n",
    "\n",
    "    seq_2_terms = pd.read_parquet(knn_terms_df, engine='fastparquet')\n",
    "    train_terms = pd.read_csv(train_terms_df, sep='\\t')\n",
    "\n",
    "    term_to_aspect = train_terms.groupby('term')['aspect'].first().to_dict()\n",
    "\n",
    "    go_graph = obonet.read_obo(data_paths['go_obo_path'])\n",
    "        \n",
    "    with open(go_embeds_paths, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        embeddings_dict = data['embeddings']\n",
    "        go_ids = data['go_ids']\n",
    "\n",
    "    # Filter to keep only terms from a specific aspect if aspect is provided\n",
    "    if aspect is not None:\n",
    "        seq_2_terms['terms_predicted'] = seq_2_terms['terms_predicted'].apply(\n",
    "            lambda terms: [t for t in terms if term_to_aspect.get(t) == aspect]\n",
    "        )\n",
    "        seq_2_terms['terms_true'] = seq_2_terms['terms_true'].apply(\n",
    "            lambda terms: [t for t in terms if term_to_aspect.get(t) == aspect]\n",
    "        )\n",
    "        # Remove rows where terms_predicted or terms_true is now empty\n",
    "        seq_2_terms = seq_2_terms[seq_2_terms['terms_predicted'].apply(len) > 0]\n",
    "        seq_2_terms = seq_2_terms[seq_2_terms['terms_true'].apply(len) > 0]\n",
    "\n",
    "\n",
    "    # features_embeds = np.load(features_embeds_path, allow_pickle=True)\n",
    "    # # features_ids = np.load(features_ids_path, allow_pickle=True)\n",
    "\n",
    "    # features_embeds_dict = {feat_id: embed for feat_id, embed in zip(features_ids, features_embeds)}\n",
    "\n",
    "    # Store original predicted terms before padding (needed for oversampling)\n",
    "    seq_2_terms['original_predicted'] = seq_2_terms['terms_predicted'].copy()\n",
    "    \n",
    "    # Build neighbor cache once if using random walk\n",
    "    neighbor_cache = build_neighbor_cache(go_graph, embeddings_dict)\n",
    "    \n",
    "    # Apply oversampling if requested\n",
    "    if oversample_dict is not None and len(oversample_dict) > 0:\n",
    "        print(f\"Applying oversampling to {len(oversample_dict)} proteins...\")\n",
    "        \n",
    "        oversampled_rows = []\n",
    "        proteins_to_oversample = set(oversample_dict.keys())\n",
    "        \n",
    "        # Separate proteins to oversample from the rest\n",
    "        oversample_mask = seq_2_terms['qseqid'].isin(proteins_to_oversample)\n",
    "        proteins_to_sample = seq_2_terms[oversample_mask].copy()\n",
    "        proteins_not_sampled = seq_2_terms[~oversample_mask].copy()\n",
    "        \n",
    "        # Pad non-oversampled proteins with default seed\n",
    "        print(f\"Padding {len(proteins_not_sampled)} non-oversampled proteins (seed=42)...\")\n",
    "        tqdm.pandas(desc=\"Padding non-oversampled\")\n",
    "        proteins_not_sampled['terms_predicted'] = proteins_not_sampled['original_predicted'].progress_apply(\n",
    "            lambda terms: pad_terms_with_random_walk(terms, neighbor_cache, max_terms, seed=42)\n",
    "        )\n",
    "        \n",
    "        # Process each oversampled protein\n",
    "        print(f\"Padding and oversampling {len(proteins_to_sample)} proteins...\")\n",
    "        for idx, row in tqdm(proteins_to_sample.iterrows(), total=len(proteins_to_sample), desc=\"Oversampling proteins\"):\n",
    "            qseqid = row['qseqid']\n",
    "            factor = oversample_dict.get(qseqid, 1)\n",
    "            \n",
    "            # Create 'factor' copies with different seeds\n",
    "            for copy_idx in range(factor):\n",
    "                seed = 42 + copy_idx * 1000  # Use different seeds for each copy\n",
    "                new_row = row.copy()\n",
    "                \n",
    "                # Apply random walk with unique seed\n",
    "                padded = pad_terms_with_random_walk(new_row['original_predicted'], neighbor_cache, max_terms, seed=seed)\n",
    "                new_row['terms_predicted'] = padded\n",
    "                new_row['oversample_idx'] = copy_idx  # Track which copy this is\n",
    "                new_row['oversample_seed'] = seed\n",
    "                \n",
    "                oversampled_rows.append(new_row)\n",
    "        \n",
    "        # Combine non-oversampled and oversampled proteins\n",
    "        if oversampled_rows:\n",
    "            oversampled_df = pd.DataFrame(oversampled_rows)\n",
    "            seq_2_terms = pd.concat([proteins_not_sampled, oversampled_df], ignore_index=True)\n",
    "        else:\n",
    "            seq_2_terms = proteins_not_sampled\n",
    "        \n",
    "        print(f\"After oversampling: {len(seq_2_terms)} total samples (from {len(proteins_not_sampled) + len(proteins_to_sample)} original)\")\n",
    "    else:\n",
    "        # No oversampling, just apply random walk padding with default seed\n",
    "        seq_2_terms = pad_dataframe_terms(seq_2_terms, go_graph, embeddings_dict, max_terms=max_terms, \n",
    "                                         use_random_walk=True, seed=42, neighbor_cache=neighbor_cache)\n",
    "    \n",
    "    # Pad remaining sequences that still don't have max_terms with random GO terms\n",
    "    term_lengths = seq_2_terms['terms_predicted'].apply(len)\n",
    "    all_go_terms = np.array(list(embeddings_dict.keys()))  # Convert to numpy array once\n",
    "    \n",
    "    print(f\"Padding remaining sequences with random GO terms...\")\n",
    "    tqdm.pandas(desc=\"Random term padding\")\n",
    "    seq_2_terms['terms_predicted'] = seq_2_terms['terms_predicted'].progress_apply(\n",
    "        lambda terms: pad_with_random_terms(terms, max_terms, all_go_terms)\n",
    "    )\n",
    "    \n",
    "    # Verify all sequences now have max_terms\n",
    "    term_lengths_after = seq_2_terms['terms_predicted'].apply(len)\n",
    "    print(f\"After random padding - Min: {term_lengths_after.min()}, Max: {term_lengths_after.max()}, Mean: {term_lengths_after.mean():.2f}\")\n",
    "\n",
    "    # Drop the temporary column\n",
    "    seq_2_terms = seq_2_terms.drop(columns=['original_predicted'], errors='ignore')\n",
    "    \n",
    "    # train_ids =  pd.DataFrame(features_ids, columns=['qseqid'])\n",
    "    # seq_2_terms = seq_2_terms.merge(train_ids, on='qseqid', how='inner')    \n",
    "\n",
    "    out = {'seq_2_terms': seq_2_terms,\n",
    "           'train_terms': train_terms,\n",
    "           'go_embeds': embeddings_dict,\n",
    "           'go_graph': go_graph\n",
    "           }      \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6883b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read sampling dict\n",
    "sampling_dict = pickle.load(open('/mnt/d/ML/Kaggle/CAFA6-new/sampling_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1d5e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = {\n",
    "    'knn_terms_df':         '/mnt/d/ML/Kaggle/CAFA6-new/uniprot/diamond_knn_predictions.parquet',\n",
    "    'train_terms_df':       '/mnt/d/ML/Kaggle/CAFA6/cafa-6-protein-function-prediction/Train/train_terms.tsv',\n",
    "    'go_obo_path':          '/mnt/d/ML/Kaggle/CAFA6/cafa-6-protein-function-prediction/Train/go-basic.obo',\n",
    "    \"features_embeds_path\": \"/mnt/d/ML/Kaggle/CAFA6-new/Dataset/archive/protein_embeddings.npy\",\n",
    "    \"features_ids_path\":    \"/mnt/d/ML/Kaggle/CAFA6-new/Dataset/archive/protein_ids.npy\",\n",
    "    'go_embeds_paths':      '/mnt/d/ML/Kaggle/CAFA6-new/uniprot/go_embeddings.pkl'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfccc0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building neighbor cache for all GO terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2702a7df2c47448e40c95c983792d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Caching neighbors:   0%|          | 0/40122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor cache built for 40122 terms\n",
      "Padding terms_predicted with GO graph random walk...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6ed6028a994571b4115ed5c9255402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Padding with random walk:   0%|          | 0/82201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding complete. Average terms per row: 201.82\n",
      "Padding remaining sequences with random GO terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926ab75d18ec4159bc6f9ae61c9b0a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Random term padding:   0%|          | 0/82201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After random padding - Min: 256, Max: 256, Mean: 256.00\n"
     ]
    }
   ],
   "source": [
    "aspect = None\n",
    "data = prepare_data(data_paths, max_terms=256, aspect=aspect) #, oversample_dict=sampling_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03672f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C max term 42\n",
    "# F max terms 34\n",
    "# P max terms 188  128 -> 42k/53k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6526d769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82201, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qseqid</th>\n",
       "      <th>terms_predicted</th>\n",
       "      <th>terms_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A023FBW4</td>\n",
       "      <td>[GO:0019958, GO:0005576, GO:0043230, GO:001995...</td>\n",
       "      <td>[GO:0019958]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A023FBW7</td>\n",
       "      <td>[GO:0019957, GO:0005576, GO:0035716, GO:000560...</td>\n",
       "      <td>[GO:0019957]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A023FDY8</td>\n",
       "      <td>[GO:0019957, GO:0005576, GO:0035716, GO:000560...</td>\n",
       "      <td>[GO:0019957]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A023FF81</td>\n",
       "      <td>[GO:0019958, GO:0005576, GO:0043230, GO:001995...</td>\n",
       "      <td>[GO:0019958]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A023FFB5</td>\n",
       "      <td>[GO:0047387, GO:0005674, GO:0008989, GO:000583...</td>\n",
       "      <td>[GO:0019957]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qseqid                                    terms_predicted    terms_true\n",
       "0  A0A023FBW4  [GO:0019958, GO:0005576, GO:0043230, GO:001995...  [GO:0019958]\n",
       "1  A0A023FBW7  [GO:0019957, GO:0005576, GO:0035716, GO:000560...  [GO:0019957]\n",
       "2  A0A023FDY8  [GO:0019957, GO:0005576, GO:0035716, GO:000560...  [GO:0019957]\n",
       "3  A0A023FF81  [GO:0019958, GO:0005576, GO:0043230, GO:001995...  [GO:0019958]\n",
       "4  A0A023FFB5  [GO:0047387, GO:0005674, GO:0008989, GO:000583...  [GO:0019957]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_2_terms = data['seq_2_terms'][['qseqid',\t'terms_predicted',\t'terms_true']]\n",
    "print(seq_2_terms.shape)\n",
    "seq_2_terms.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe143bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180.50 MB\n"
     ]
    }
   ],
   "source": [
    "print_size(seq_2_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "864f8b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_2_terms.to_parquet(f'/mnt/d/ML/Kaggle/CAFA6-new/seq_2_terms.parquet', engine='fastparquet', index=False)\n",
    "# seq_2_terms.to_csv(f'/mnt/d/ML/Kaggle/CAFA6-new/seq_2_terms.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc5a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_with_random_terms(terms, max_terms, all_terms_array, existing_terms_set=None):\n",
    "    \"\"\"Optimized padding with random GO terms using rejection sampling.\"\"\"\n",
    "    if len(terms) >= max_terms:\n",
    "        return terms[:max_terms]\n",
    "    \n",
    "    # Calculate how many more terms we need\n",
    "    num_needed = max_terms - len(terms)\n",
    "    \n",
    "    # Use pre-computed set if available, otherwise create it\n",
    "    if existing_terms_set is None:\n",
    "        existing_terms_set = set(terms)\n",
    "    \n",
    "    # For small num_needed relative to available terms, use rejection sampling\n",
    "    # This avoids creating the full boolean mask\n",
    "    if num_needed < len(all_terms_array) * 0.1:  # If we need < 10% of total terms\n",
    "        random_terms = []\n",
    "        max_attempts = num_needed * 10  # Prevent infinite loop\n",
    "        attempts = 0\n",
    "        \n",
    "        while len(random_terms) < num_needed and attempts < max_attempts:\n",
    "            # Sample with replacement first (fast)\n",
    "            batch_size = min(num_needed * 2, 1000)  # Sample in batches\n",
    "            candidates = np.random.choice(all_terms_array, size=batch_size, replace=True)\n",
    "            \n",
    "            # Filter out existing terms\n",
    "            for candidate in candidates:\n",
    "                if candidate not in existing_terms_set:\n",
    "                    random_terms.append(candidate)\n",
    "                    existing_terms_set.add(candidate)\n",
    "                    if len(random_terms) >= num_needed:\n",
    "                        break\n",
    "            \n",
    "            attempts += batch_size\n",
    "        \n",
    "        # If rejection sampling didn't get enough, fall back to full filtering\n",
    "        if len(random_terms) < num_needed:\n",
    "            available_mask = np.isin(all_terms_array, list(existing_terms_set), invert=True)\n",
    "            available_terms = all_terms_array[available_mask]\n",
    "            remaining_needed = num_needed - len(random_terms)\n",
    "            \n",
    "            if len(available_terms) >= remaining_needed:\n",
    "                additional = np.random.choice(available_terms, size=remaining_needed, replace=False).tolist()\n",
    "            else:\n",
    "                additional = np.random.choice(available_terms, size=remaining_needed, replace=True).tolist()\n",
    "            \n",
    "            random_terms.extend(additional)\n",
    "    else:\n",
    "        # For large num_needed, use vectorized filtering (more efficient for bulk)\n",
    "        available_mask = np.isin(all_terms_array, list(existing_terms_set), invert=True)\n",
    "        available_terms = all_terms_array[available_mask]\n",
    "        \n",
    "        # Randomly sample\n",
    "        if len(available_terms) >= num_needed:\n",
    "            random_terms = np.random.choice(available_terms, size=num_needed, replace=False).tolist()\n",
    "        else:\n",
    "            # If not enough unique terms, sample with replacement\n",
    "            random_terms = np.random.choice(available_terms, size=num_needed, replace=True).tolist()\n",
    "    \n",
    "    return terms + random_terms[:num_needed]\n",
    "\n",
    "def load_data(data_paths, max_terms=256, aspect=None):\n",
    "    \n",
    "    seq_2_terms_df = data_paths['seq_2_terms_df']\n",
    "    train_terms_df = data_paths['train_terms_df']\n",
    "    features_embeds_path = data_paths['features_embeds_path']\n",
    "    features_ids_path = data_paths['features_ids_path']\n",
    "\n",
    "    go_embeds_paths = data_paths['go_embeds_paths']\n",
    "\n",
    "    seq_2_terms = pd.read_parquet(seq_2_terms_df, engine='fastparquet')\n",
    "    train_terms = pd.read_csv(train_terms_df, sep='\\t')\n",
    "\n",
    "    term_to_aspect = train_terms.groupby('term')['aspect'].first().to_dict()\n",
    "        \n",
    "    with open(go_embeds_paths, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        embeddings_dict = data['embeddings']\n",
    "        go_ids = data['go_ids']\n",
    "\n",
    "    # Filter to keep only terms from a specific aspect if aspect is provided\n",
    "    print('filtering by aspect:', aspect)\n",
    "    if aspect is not None:\n",
    "        seq_2_terms['terms_predicted'] = seq_2_terms['terms_predicted'].apply(\n",
    "            lambda terms: [t for t in terms if term_to_aspect.get(t) == aspect]\n",
    "        )\n",
    "        seq_2_terms['terms_true'] = seq_2_terms['terms_true'].apply(\n",
    "            lambda terms: [t for t in terms if term_to_aspect.get(t) == aspect]\n",
    "        )\n",
    "        # Remove rows where terms_predicted or terms_true is now empty\n",
    "        seq_2_terms = seq_2_terms[seq_2_terms['terms_predicted'].apply(len) > 0]\n",
    "        seq_2_terms = seq_2_terms[seq_2_terms['terms_true'].apply(len) > 0]\n",
    "\n",
    "        # Pad terms with random terms from the same aspect\n",
    "        print(f\"Padding terms_predicted with random terms from aspect {aspect}...\")\n",
    "        # Get all terms from this aspect that have embeddings\n",
    "        aspect_terms = [term for term, asp in term_to_aspect.items() if asp == aspect and term in embeddings_dict]\n",
    "        all_aspect_terms = np.array(aspect_terms)\n",
    "        \n",
    "        tqdm.pandas(desc=f\"Padding with random {aspect} terms\")\n",
    "        seq_2_terms['terms_predicted'] = seq_2_terms['terms_predicted'].progress_apply(\n",
    "            lambda terms: pad_with_random_terms(terms, max_terms, all_aspect_terms)\n",
    "        )\n",
    "        \n",
    "        # Verify padding\n",
    "        term_lengths_after = seq_2_terms['terms_predicted'].apply(len)\n",
    "        print(f\"After padding - Min: {term_lengths_after.min()}, Max: {term_lengths_after.max()}, Mean: {term_lengths_after.mean():.2f}\")\n",
    "\n",
    "    \n",
    "    print(\"loading features embeddings and ids\")\n",
    "    features_embeds = np.load(features_embeds_path, allow_pickle=True)\n",
    "    features_ids = np.load(features_ids_path, allow_pickle=True)\n",
    "    \n",
    "    print(\"creating features embeddings dict\")\n",
    "    features_embeds_dict = {feat_id: embed for feat_id, embed in zip(features_ids, features_embeds)}\n",
    "\n",
    "    term_lengths = seq_2_terms['terms_predicted'].apply(len)\n",
    "\n",
    "    print(\"filtering sequences by term lengths\")\n",
    "   #currently only using sequences with 256 terms, need to change later \n",
    "    # seq_2_terms = seq_2_terms[term_lengths == max_terms]\n",
    "\n",
    "    train_ids =  pd.DataFrame(features_ids, columns=['qseqid'])\n",
    "    seq_2_terms = seq_2_terms.merge(train_ids, on='qseqid', how='inner')    \n",
    "\n",
    "    out = {'seq_2_terms': seq_2_terms,\n",
    "           'train_terms': train_terms,\n",
    "           'features_embeds': features_embeds_dict,\n",
    "           'go_embeds': embeddings_dict,\n",
    "           }\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc08327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering by aspect: F\n",
      "Padding terms_predicted with random terms from aspect F...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae36357f80244bb99c937230e7df5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Padding with random F terms:   0%|          | 0/144028 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After padding - Min: 64, Max: 64, Mean: 64.00\n",
      "loading features embeddings and ids\n",
      "creating features embeddings dict\n",
      "filtering sequences by term lengths\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(144028, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_paths = {\n",
    "    'seq_2_terms_df':       '/mnt/d/ML/Kaggle/CAFA6-new/seq_2_terms.parquet',\n",
    "    'train_terms_df':       '/mnt/d/ML/Kaggle/CAFA6/cafa-6-protein-function-prediction/Train/train_terms.tsv',\n",
    "    \"features_embeds_path\": \"/mnt/d/ML/Kaggle/CAFA6-new/Dataset/archive/protein_embeddings.npy\",\n",
    "    \"features_ids_path\":    \"/mnt/d/ML/Kaggle/CAFA6-new/Dataset/archive/protein_ids.npy\",\n",
    "    'go_embeds_paths':      '/mnt/d/ML/Kaggle/CAFA6-new/uniprot/go_embeddings.pkl'\n",
    "}\n",
    "\n",
    "aspect = 'F'\n",
    "data = load_data(data_paths, max_terms=64, aspect=aspect)\n",
    "data['seq_2_terms'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "895c85ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min terms: 64\n",
      "Max terms: 64\n",
      "Mean terms: 64.00\n",
      "Median terms: 64.00\n",
      "No. empty predicted terms: 0\n",
      "\n",
      "Sample row with padded terms:\n",
      "Number of predicted terms: 64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of terms_predicted lengths after padding\n",
    "term_lengths = data['seq_2_terms']['terms_predicted'].apply(len)\n",
    "print(f\"Min terms: {term_lengths.min()}\")\n",
    "print(f\"Max terms: {term_lengths.max()}\")\n",
    "print(f\"Mean terms: {term_lengths.mean():.2f}\")\n",
    "print(f\"Median terms: {term_lengths.median():.2f}\")\n",
    "print(f\"No. empty predicted terms: {(term_lengths == 0).sum()}\")\n",
    "print(f\"\\nSample row with padded terms:\")\n",
    "print(f\"Number of predicted terms: {len(data['seq_2_terms'].iloc[0]['terms_predicted'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "705dc874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min terms: 1\n",
      "Max terms: 34\n",
      "Mean terms: 2.04\n",
      "Median terms: 2.00\n",
      "No. empty predicted terms: 0\n",
      "\n",
      "Sample row with padded terms:\n",
      "Number of predicted terms: 64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of terms_predicted lengths after padding\n",
    "term_lengths = data['seq_2_terms']['terms_true'].apply(len)\n",
    "print(f\"Min terms: {term_lengths.min()}\")\n",
    "print(f\"Max terms: {term_lengths.max()}\")\n",
    "print(f\"Mean terms: {term_lengths.mean():.2f}\")\n",
    "print(f\"Median terms: {term_lengths.median():.2f}\") \n",
    "print(f\"No. empty predicted terms: {(term_lengths == 0).sum()}\")\n",
    "print(f\"\\nSample row with padded terms:\")\n",
    "print(f\"Number of predicted terms: {len(data['seq_2_terms'].iloc[0]['terms_predicted'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0588be47",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4af23152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_terms_grouped shape: (58001, 2)\n",
      "\n",
      "Sample of grouped train_terms:\n",
      "       qseqid train_terms_all\n",
      "0  A0A023FBW4    [GO:0019958]\n",
      "1  A0A023FBW7    [GO:0019957]\n",
      "2  A0A023FDY8    [GO:0019957]\n",
      "3  A0A023FF81    [GO:0019958]\n",
      "4  A0A023FFB5    [GO:0019957]\n"
     ]
    }
   ],
   "source": [
    "# Group train_terms by EntryID and concatenate all terms into a list\n",
    "if aspect is not None:\n",
    "    data['train_terms'] = data['train_terms'][data['train_terms']['aspect'] == aspect]\n",
    "    \n",
    "train_terms_grouped = data['train_terms'].groupby('EntryID')['term'].apply(list).reset_index()\n",
    "train_terms_grouped.columns = ['qseqid', 'train_terms_all']\n",
    "\n",
    "print(f\"train_terms_grouped shape: {train_terms_grouped.shape}\")\n",
    "print(f\"\\nSample of grouped train_terms:\")\n",
    "print(train_terms_grouped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b53ded0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min terms: 1\n",
      "Max terms: 34\n",
      "Mean terms: 2.21\n",
      "Median terms: 2.00\n",
      "No. empty predicted terms: 0\n",
      "\n",
      "Sample row with padded terms:\n",
      "Number of predicted terms: 64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of terms_predicted lengths after padding\n",
    "term_lengths = train_terms_grouped['train_terms_all'].apply(len)\n",
    "print(f\"Min terms: {term_lengths.min()}\")\n",
    "print(f\"Max terms: {term_lengths.max()}\")\n",
    "print(f\"Mean terms: {term_lengths.mean():.2f}\")\n",
    "print(f\"Median terms: {term_lengths.median():.2f}\")\n",
    "print(f\"No. empty predicted terms: {(term_lengths == 0).sum()}\")\n",
    "print(f\"\\nSample row with padded terms:\")\n",
    "print(f\"Number of predicted terms: {len(data['seq_2_terms'].iloc[0]['terms_predicted'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "217e3a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataframe shape: (144028, 5)\n",
      "Columns: ['qseqid', 'terms_predicted', 'terms_true', 'oversample_idx', 'train_terms_all']\n",
      "\n",
      "Sample of merged data:\n"
     ]
    }
   ],
   "source": [
    "# Merge train_terms_grouped with seq_2_terms\n",
    "merged_df = data['seq_2_terms'].merge(train_terms_grouped, on='qseqid', how='left')\n",
    "\n",
    "print(f\"Merged dataframe shape: {merged_df.shape}\")\n",
    "print(f\"Columns: {merged_df.columns.tolist()}\")\n",
    "print(f\"\\nSample of merged data:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103f4b47",
   "metadata": {},
   "source": [
    "### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b55cb35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Coverage Analysis ===\n",
      "\n",
      "Overall Statistics:\n",
      "Average coverage: 91.52%\n",
      "Median coverage: 100.00%\n",
      "Min coverage: 0.00%\n",
      "Max coverage: 100.00%\n",
      "\n",
      "Rows with 100% coverage: 129730 / 144028\n",
      "Rows with 0% coverage: 10400 / 144028\n",
      "\n",
      "Average missing terms per row: 0.15\n",
      "Total train terms checked: 294358\n",
      "Total covered terms: 273050\n",
      "Total missing terms: 21308\n"
     ]
    }
   ],
   "source": [
    "# Check if all true predictions from train_terms_all are present in terms_predicted\n",
    "def check_coverage(row):\n",
    "    \"\"\"Check what percentage of train_terms_all are in terms_predicted\"\"\"\n",
    "    # Check if train_terms_all is None or empty list\n",
    "    true_key  = 'train_terms_all'\n",
    "    if row[true_key] is None or (isinstance(row[true_key], list) and len(row[true_key]) == 0):\n",
    "        return None\n",
    "    \n",
    "    train_set = set(row[true_key])\n",
    "    pred_set = set(row['terms_predicted'])\n",
    "    \n",
    "    # Find terms in train_terms_all that are in terms_predicted\n",
    "    covered_terms = train_set.intersection(pred_set)\n",
    "    \n",
    "    # Calculate coverage percentage\n",
    "    coverage = len(covered_terms) / len(train_set) * 100 if len(train_set) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total_train_terms': len(train_set),\n",
    "        'covered_terms': len(covered_terms),\n",
    "        'missing_terms': len(train_set) - len(covered_terms),\n",
    "        'coverage_pct': coverage,\n",
    "        'missing_term_list': list(train_set - covered_terms)\n",
    "    }\n",
    "\n",
    "# Apply the check\n",
    "coverage_results = merged_df.apply(check_coverage, axis=1)\n",
    "coverage_df = pd.DataFrame(coverage_results.tolist())\n",
    "\n",
    "# Combine with original data\n",
    "analysis_df = pd.concat([merged_df, coverage_df], axis=1)\n",
    "\n",
    "print(\"=== Coverage Analysis ===\")\n",
    "print(f\"\\nOverall Statistics:\")\n",
    "print(f\"Average coverage: {coverage_df['coverage_pct'].mean():.2f}%\")\n",
    "print(f\"Median coverage: {coverage_df['coverage_pct'].median():.2f}%\")\n",
    "print(f\"Min coverage: {coverage_df['coverage_pct'].min():.2f}%\")\n",
    "print(f\"Max coverage: {coverage_df['coverage_pct'].max():.2f}%\")\n",
    "print(f\"\\nRows with 100% coverage: {(coverage_df['coverage_pct'] == 100).sum()} / {len(coverage_df)}\")\n",
    "print(f\"Rows with 0% coverage: {(coverage_df['coverage_pct'] == 0).sum()} / {len(coverage_df)}\")\n",
    "print(f\"\\nAverage missing terms per row: {coverage_df['missing_terms'].mean():.2f}\")\n",
    "print(f\"Total train terms checked: {coverage_df['total_train_terms'].sum()}\")\n",
    "print(f\"Total covered terms: {coverage_df['covered_terms'].sum()}\")\n",
    "print(f\"Total missing terms: {coverage_df['missing_terms'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e16f6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Examples of Coverage Levels ===\n",
      "\n",
      "Example with 0% coverage:\n",
      "EntryID: A0A023FFB5\n",
      "Train terms: ['GO:0019957']... (total: 1)\n",
      "Predicted terms (first 5): [np.str_('GO:0042822'), np.str_('GO:0018974'), np.str_('GO:0097651'), np.str_('GO:2000921'), np.str_('GO:0090471')]...\n",
      "Missing terms: ['GO:0019957']...\n",
      "\n",
      "Example with ~50% coverage:\n",
      "EntryID: A0A0A7EPL0\n",
      "Train terms: ['GO:0016925', 'GO:0019789', 'GO:0051176', 'GO:0009651', 'GO:0009737', 'GO:0006970', 'GO:0060966', 'GO:0005515']\n",
      "Covered: 4 / 8\n",
      "Missing terms: ['GO:0006970', 'GO:0009651', 'GO:0051176', 'GO:0009737']\n",
      "\n",
      "Example with 100% coverage:\n",
      "EntryID: A0A023FBW4\n",
      "Train terms: ['GO:0019958']\n",
      "All 1 terms are present in predicted terms!\n",
      "\n",
      "\n",
      "=== Coverage Distribution ===\n",
      "coverage_pct\n",
      "(-0.001, 25.0]     15166\n",
      "(25.0, 50.0]        3790\n",
      "(50.0, 75.0]        3853\n",
      "(75.0, 100.0]     166277\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Show examples of rows with different coverage levels\n",
    "print(\"=== Examples of Coverage Levels ===\\n\")\n",
    "\n",
    "# Example with 0% coverage\n",
    "print(\"Example with 0% coverage:\")\n",
    "zero_coverage = analysis_df[analysis_df['coverage_pct'] == 0].iloc[0]\n",
    "print(f\"EntryID: {zero_coverage['qseqid']}\")\n",
    "print(f\"Train terms: {zero_coverage['train_terms_all'][:5]}... (total: {zero_coverage['total_train_terms']})\")\n",
    "print(f\"Predicted terms (first 5): {zero_coverage['terms_predicted'][:5]}...\")\n",
    "print(f\"Missing terms: {zero_coverage['missing_term_list'][:5]}...\")\n",
    "print()\n",
    "\n",
    "# Example with partial coverage\n",
    "print(\"Example with ~50% coverage:\")\n",
    "partial_coverage = analysis_df[(analysis_df['coverage_pct'] > 45) & (analysis_df['coverage_pct'] < 55)].iloc[0]\n",
    "print(f\"EntryID: {partial_coverage['qseqid']}\")\n",
    "print(f\"Train terms: {partial_coverage['train_terms_all']}\")\n",
    "print(f\"Covered: {partial_coverage['covered_terms']} / {partial_coverage['total_train_terms']}\")\n",
    "print(f\"Missing terms: {partial_coverage['missing_term_list']}\")\n",
    "print()\n",
    "\n",
    "# Example with 100% coverage\n",
    "print(\"Example with 100% coverage:\")\n",
    "full_coverage = analysis_df[analysis_df['coverage_pct'] == 100].iloc[0]\n",
    "print(f\"EntryID: {full_coverage['qseqid']}\")\n",
    "print(f\"Train terms: {full_coverage['train_terms_all']}\")\n",
    "print(f\"All {full_coverage['total_train_terms']} terms are present in predicted terms!\")\n",
    "print()\n",
    "\n",
    "# Distribution of coverage\n",
    "print(\"\\n=== Coverage Distribution ===\")\n",
    "bins = [0, 25, 50, 75, 100]\n",
    "coverage_bins = pd.cut(coverage_df['coverage_pct'], bins=bins, include_lowest=True)\n",
    "print(coverage_bins.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96747401",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f275bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsDataset(Dataset):\n",
    "    \"\"\"Dataset that yields raw embeddings; tokenization is done in collate_fn for batching.\"\"\"\n",
    "    def __init__(self, \n",
    "                 data, \n",
    "                 max_go_embeds = 256,  \n",
    "                 oversample_indices=None\n",
    "                ):\n",
    "        \n",
    "        self.data = data\n",
    "        self.max_go_embeds = max_go_embeds\n",
    "        self.oversample_indices = oversample_indices if oversample_indices is not None else list(range(len(self.data['seq_2_terms'])))\n",
    "        self.mask_embed = np.zeros(next(iter(self.data['go_embeds'].values())).shape, dtype=np.float32)\n",
    "        #ensure len of predicted go terms is less than max_go_embeds\n",
    "        #self.data['seq_2_terms'] = self.data['seq_2_terms'][self\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.oversample_indices)         \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx = self.oversample_indices[idx]\n",
    "\n",
    "        row = self.data['seq_2_terms'].iloc[sample_idx]\n",
    "        qseqid = row['qseqid']\n",
    "\n",
    "        feature_embed = self.data['features_embeds'][qseqid]\n",
    "\n",
    "        true_terms_set = set(row['terms_true'])\n",
    "        predicted_terms = row['terms_predicted']\n",
    "        \n",
    "        # Filter terms that have embeddings (should be all of them after padding)\n",
    "        # valid_terms = [term for term in predicted_terms if term in self.data['go_embeds']]\n",
    "        valid_terms = predicted_terms\n",
    "        # Vectorized operations using list comprehensions\n",
    "        go_embeds = np.array([self.data['go_embeds'].get(term, self.mask_embed) for term in valid_terms])\n",
    "        label = np.array([term in true_terms_set for term in valid_terms], dtype=np.float32)\n",
    "        \n",
    "        return {\n",
    "            'entryID'   : qseqid,\n",
    "            'feature'   : feature_embed,\n",
    "            'go_embed'  : go_embeds,\n",
    "            'label'     : label,\n",
    "            'predicted_terms': valid_terms,\n",
    "            'true_terms': row['terms_true']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3592df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EmbeddingsDataset(data)\n",
    "\n",
    "aa = dataset[1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bef64d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4681f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_orthogonal_vectors_qr(dimension, num_vectors=None):\n",
    "    \"\"\"\n",
    "    Generates a set of orthogonal vectors using QR factorization.\n",
    "    \n",
    "    Args:\n",
    "        dimension (int): The dimensionality of the vectors.\n",
    "        num_vectors (int, optional): The number of vectors to generate. \n",
    "                                     Defaults to 'dimension' for a full basis.\n",
    "    Returns:\n",
    "        numpy.ndarray: A matrix where each column is an orthogonal vector.\n",
    "    \"\"\"\n",
    "    if num_vectors is None:\n",
    "        num_vectors = dimension\n",
    "        \n",
    "    # Generate a random matrix\n",
    "    A = np.random.rand(dimension, num_vectors)\n",
    "    \n",
    "    # Perform QR factorization\n",
    "    # The 'Q' matrix contains the orthogonal columns\n",
    "    Q, R = np.linalg.qr(A)\n",
    "    \n",
    "    # Return the first 'num_vectors' columns of Q\n",
    "    return Q[:, :num_vectors]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "028807d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dot product of the first two vectors: 2.7755575615628914e-17\n",
      "(1280, 1280)\n"
     ]
    }
   ],
   "source": [
    "vectors = generate_orthogonal_vectors_qr(1280 ) #, num_vectors=512)\n",
    "\n",
    "# Verification (dot product of any two distinct columns should be near zero):\n",
    "print(\"\\nDot product of the first two vectors:\", np.dot(vectors[:, 0], vectors[:, 1]))\n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c761773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 1280)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "307e06bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmbedTokenizer(nn.Module):\n",
    "    def __init__(self, D, d, N, rng=None):  \n",
    "        super(EmbedTokenizer, self).__init__()\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        D : int\n",
    "            Dimension of the embedding space.\n",
    "        d : int\n",
    "            Dimension of the token space.\n",
    "        N : int\n",
    "            Number of tokens.\n",
    "        rng : np.random.Generator or None\n",
    "            Random generator. If None, use default.\n",
    "        \"\"\"\n",
    "        if rng is None:\n",
    "            rng = np.random.default_rng()\n",
    "\n",
    "        V = generate_orthogonal_vectors_qr(D)\n",
    "        K = D\n",
    "        # Build P as a single tensor of shape (N, d, D) and register as buffer so it's moved with .to()\n",
    "        P_list = []\n",
    "        for i in range(N):\n",
    "            indices = np.arange(D)\n",
    "            sampled_idx = np.random.choice(indices, size=d, replace=False)\n",
    "            p = V[:, sampled_idx].T\n",
    "            P_list.append(p)\n",
    "\n",
    "        P_np = np.stack(P_list, axis=0).astype(np.float32)  # (N, d, D)\n",
    "        P_tensor = torch.from_numpy(P_np)\n",
    "        # register as buffer so it's not a parameter but moves with the module\n",
    "        self.register_buffer('P_buffer', P_tensor)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Tensor, shape (D)\n",
    "            Input embeddings.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tokens : Tensor, shape (batch_size, N, d)\n",
    "            Token representations.\n",
    "        \"\"\"\n",
    "        # x: (batch_size, D) or (D,) -> ensure batch\n",
    "        squeeze_output = False\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "            squeeze_output = True\n",
    "\n",
    "        # Get P and move to correct device/dtype\n",
    "        P = self.P_buffer.to(dtype=x.dtype, device=x.device)  # (N, d, D)\n",
    "\n",
    "        # Vectorized matmul: (batch_size, D) @ (D, N*d) -> (batch_size, N*d) -> reshape\n",
    "        D = x.shape[1]\n",
    "        P_2d = P.permute(2, 0, 1).reshape(D, -1)  # (D, N*d)\n",
    "        tokens = torch.matmul(x, P_2d)  # (batch_size, N*d)\n",
    "        tokens = tokens.reshape(x.shape[0], P.shape[0], P.shape[1])  # (batch_size, N, d)\n",
    "\n",
    "        if squeeze_output:\n",
    "            tokens = tokens.squeeze(0)\n",
    "\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a882a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = EmbedTokenizer(D=1280, d=512, N=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3099468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2364,  0.2243, -0.0518,  ...,  0.2313, -0.4923, -0.4198],\n",
       "          [-0.1851,  0.1678,  0.0376,  ..., -0.4407, -0.2901, -0.1140],\n",
       "          [-0.3032, -0.0757, -0.2510,  ..., -0.3959, -0.0704,  0.0801],\n",
       "          ...,\n",
       "          [-0.6705, -0.0571,  0.1888,  ...,  0.1879, -0.2961, -0.3327],\n",
       "          [ 0.1733,  0.0521, -0.3762,  ..., -0.1296, -0.1628, -0.1476],\n",
       "          [ 0.0667,  0.3748, -0.2992,  ...,  0.2143,  0.1407,  0.1951]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(torch.tensor(aa['feature'], dtype=torch.float32).to(device).unsqueeze(0)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10afa4c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfeatures_embeds\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "data['features_embeds'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9beff280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_tokenize(batch, tokenizer, device=None, dtype=torch.float32):\n",
    "    \"\"\"Custom collate function to handle variable-length data.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of samples from the dataset\n",
    "        tokenizer: Tokenizer to apply to features\n",
    "        device: Device to move tensors to (cuda or cpu)\n",
    "        dtype: Target dtype for tensors (torch.float32, torch.float16, or torch.bfloat16)\n",
    "    \"\"\"\n",
    "    features = torch.stack([torch.from_numpy(item['feature']) for item in batch])\n",
    "    features = features.to(dtype=dtype, device=device) if device else features.to(dtype=dtype)\n",
    "    features = tokenizer(features)\n",
    "    \n",
    "    go_embed = torch.stack([torch.from_numpy(item['go_embed']) for item in batch])\n",
    "    go_embed = go_embed.to(dtype=dtype, device=device) if device else go_embed.to(dtype=dtype)\n",
    "    \n",
    "    label = torch.stack([torch.from_numpy(item['label']) for item in batch])\n",
    "    label = label.to(dtype=dtype, device=device) if device else label.to(dtype=dtype)\n",
    "    \n",
    "    return {\n",
    "        'entryID': [item['entryID'] for item in batch],\n",
    "        'feature': features,\n",
    "        'go_embed': go_embed,\n",
    "        'label': label,\n",
    "        'predicted_terms': [item['predicted_terms'] for item in batch],\n",
    "        'true_terms': [item['true_terms'] for item in batch]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4952f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options: torch.float32 (default), torch.float16, torch.bfloat16\n",
    "target_dtype = torch.float32  # Change this to torch.float16 or torch.bfloat16 if needed\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=lambda b: collate_tokenize(b, tokenizer, device, dtype=target_dtype)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7c23812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefetchLoader:\n",
    "    \"\"\"\n",
    "    Prefetch loader that loads batches asynchronously to GPU for faster training.\n",
    "    Overlaps data transfer with computation by loading the next batch while \n",
    "    the model processes the current batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        self.stream = torch.cuda.Stream() if device.type == 'cuda' else None\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.stream is not None:\n",
    "            # CUDA prefetching\n",
    "            return self._cuda_iter()\n",
    "        else:\n",
    "            # CPU fallback - no prefetching needed\n",
    "            return iter(self.dataloader)\n",
    "    \n",
    "    def _cuda_iter(self):\n",
    "        \"\"\"Iterator with CUDA stream prefetching.\"\"\"\n",
    "        loader_iter = iter(self.dataloader)\n",
    "        \n",
    "        # Preload first batch\n",
    "        try:\n",
    "            with torch.cuda.stream(self.stream):\n",
    "                next_batch = next(loader_iter)\n",
    "                next_batch = self._to_device(next_batch)\n",
    "        except StopIteration:\n",
    "            return\n",
    "        \n",
    "        while True:\n",
    "            # Wait for the prefetch stream to finish\n",
    "            torch.cuda.current_stream().wait_stream(self.stream)\n",
    "            batch = next_batch\n",
    "            \n",
    "            # Make sure tensors are ready before yielding\n",
    "            if isinstance(batch, dict):\n",
    "                for k, v in batch.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        v.record_stream(torch.cuda.current_stream())\n",
    "            \n",
    "            # Start loading next batch in background\n",
    "            try:\n",
    "                with torch.cuda.stream(self.stream):\n",
    "                    next_batch = next(loader_iter)\n",
    "                    next_batch = self._to_device(next_batch)\n",
    "            except StopIteration:\n",
    "                yield batch\n",
    "                break\n",
    "                    \n",
    "            yield batch\n",
    "    \n",
    "    def _to_device(self, batch):\n",
    "        \"\"\"Move batch to device (already moved in collate_fn, but ensure it's there).\"\"\"\n",
    "        if isinstance(batch, dict):\n",
    "            # Batch is already on device from collate_fn, just return it\n",
    "            return batch\n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "773c16a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length: 2300\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Create prefetch loader\n",
    "prefetch_loader = PrefetchLoader(dataloader, device)\n",
    "\n",
    "print(f\"DataLoader length: {len(prefetch_loader)}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be48d34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing regular dataloader:\n",
      "Regular loader time (5 batches): 0.9673s\n",
      "\n",
      "Testing prefetch loader:\n",
      "Prefetch loader time (5 batches): 0.6510s\n",
      "Speedup: 1.49x\n"
     ]
    }
   ],
   "source": [
    "# Test prefetch loader - compare timing\n",
    "import time\n",
    "\n",
    "# Test without prefetch\n",
    "print(\"Testing regular dataloader:\")\n",
    "start = time.time()\n",
    "for i, batch in enumerate(dataloader):\n",
    "    if i >= 5:  # Just test 5 batches\n",
    "        break\n",
    "    # Simulate some processing\n",
    "    _ = batch['feature'].sum()\n",
    "regular_time = time.time() - start\n",
    "print(f\"Regular loader time (5 batches): {regular_time:.4f}s\")\n",
    "\n",
    "# Test with prefetch\n",
    "print(\"\\nTesting prefetch loader:\")\n",
    "start = time.time()\n",
    "for i, batch in enumerate(prefetch_loader):\n",
    "    if i >= 5:  # Just test 5 batches\n",
    "        break\n",
    "    # Simulate some processing\n",
    "    _ = batch['feature'].sum()\n",
    "prefetch_time = time.time() - start\n",
    "print(f\"Prefetch loader time (5 batches): {prefetch_time:.4f}s\")\n",
    "print(f\"Speedup: {regular_time/prefetch_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f3f66",
   "metadata": {},
   "source": [
    "## Check Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e29010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c8e5068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32, Num terms per sample: 256, Embedding dim: 512\n",
      "\n",
      "======================================================================\n",
      "PER-INSTANCE ANALYSIS\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26270/4193956610.py:83: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)\n",
      "  instance_stats['positive_sim_std'].append(pos_sim.std().item())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance   Positives    Random Sim      Pos Sim         Pos-Neg Sim    \n",
      "----------------------------------------------------------------------\n",
      "0          6            0.0143          0.0272          0.0255         \n",
      "1          5            0.0182          0.0240          0.0190         \n",
      "2          2            0.0112          0.0540          0.0261         \n",
      "3          7            0.0163          0.0677          0.0238         \n",
      "4          3            0.0039          -0.0007         0.0201         \n",
      "5          23           0.0156          0.0115          0.0163         \n",
      "6          5            0.0073          -0.0186         0.0129         \n",
      "7          4            0.0151          -0.0136         0.0104         \n",
      "8          14           0.0146          0.0282          0.0223         \n",
      "9          1            0.0182          N/A             -0.0413        \n",
      "... (22 more instances)\n",
      "\n",
      "======================================================================\n",
      "AGGREGATE STATISTICS ACROSS ALL INSTANCES\n",
      "======================================================================\n",
      "\n",
      "Average number of positives per instance: 6.84\n",
      "Average number of negatives per instance: 249.16\n",
      "\n",
      "=== Random Pairs (within each instance) ===\n",
      "Mean across instances: 0.0132\n",
      "Std across instances: 0.0049\n",
      "Min: 0.0039, Max: 0.0246\n",
      "\n",
      "=== Positive Pairs (within each instance) ===\n",
      "Mean across instances: 0.0290\n",
      "Std across instances: 0.0436\n",
      "Min: -0.0527, Max: 0.1539\n",
      "Instances with positive pairs: 26 / 32\n",
      "\n",
      "=== Positive vs Negative Pairs (within each instance) ===\n",
      "Mean across instances: 0.0205\n",
      "Std across instances: 0.0184\n",
      "Min: -0.0413, Max: 0.0534\n",
      "Instances with both pos and neg: 31 / 32\n"
     ]
    }
   ],
   "source": [
    "# Analyze cosine similarity per instance (within each sample's 256 token embeddings)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "go_embeds = batch['go_embed']  # Shape: (batch_size, 256, 512)\n",
    "labels = batch['label']  # Shape: (batch_size, 256)\n",
    "\n",
    "batch_size, num_terms, embed_dim = go_embeds.shape\n",
    "print(f\"Batch size: {batch_size}, Num terms per sample: {num_terms}, Embedding dim: {embed_dim}\")\n",
    "\n",
    "# Store results for each instance\n",
    "instance_stats = {\n",
    "    'random_sim_mean': [],\n",
    "    'random_sim_std': [],\n",
    "    'positive_sim_mean': [],\n",
    "    'positive_sim_std': [],\n",
    "    'pos_neg_sim_mean': [],\n",
    "    'pos_neg_sim_std': [],\n",
    "    'num_positives': [],\n",
    "    'num_negatives': []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PER-INSTANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    # Get embeddings and labels for this instance\n",
    "    instance_embeds = go_embeds[i]  # (256, 512)\n",
    "    instance_labels = labels[i]  # (256,)\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    normalized = F.normalize(instance_embeds, p=2, dim=1)\n",
    "    \n",
    "    # Count positive and negative labels\n",
    "    num_pos = (instance_labels == 1).sum().item()\n",
    "    num_neg = (instance_labels == 0).sum().item()\n",
    "    instance_stats['num_positives'].append(num_pos)\n",
    "    instance_stats['num_negatives'].append(num_neg)\n",
    "    \n",
    "    # 1. Random pairs within this instance\n",
    "    num_samples = min(1000, num_terms * (num_terms - 1) // 2)\n",
    "    idx1 = torch.randint(0, num_terms, (num_samples,))\n",
    "    idx2 = torch.randint(0, num_terms, (num_samples,))\n",
    "    # Filter out same indices\n",
    "    mask = idx1 != idx2\n",
    "    idx1 = idx1[mask]\n",
    "    idx2 = idx2[mask]\n",
    "    \n",
    "    if len(idx1) > 0:\n",
    "        random_sim = F.cosine_similarity(normalized[idx1], normalized[idx2], dim=1)\n",
    "        instance_stats['random_sim_mean'].append(random_sim.mean().item())\n",
    "        instance_stats['random_sim_std'].append(random_sim.std().item())\n",
    "    else:\n",
    "        instance_stats['random_sim_mean'].append(0.0)\n",
    "        instance_stats['random_sim_std'].append(0.0)\n",
    "    \n",
    "    # 2. Positive pairs\n",
    "    pos_indices = torch.where(instance_labels == 1)[0]\n",
    "    if len(pos_indices) > 1:\n",
    "        # Compute all pairs or sample\n",
    "        if len(pos_indices) > 50:\n",
    "            num_pos_samples = min(1000, len(pos_indices) * (len(pos_indices) - 1) // 2)\n",
    "            pos_idx1 = pos_indices[torch.randint(0, len(pos_indices), (num_pos_samples,))]\n",
    "            pos_idx2 = pos_indices[torch.randint(0, len(pos_indices), (num_pos_samples,))]\n",
    "            mask = pos_idx1 != pos_idx2\n",
    "            pos_idx1 = pos_idx1[mask]\n",
    "            pos_idx2 = pos_idx2[mask]\n",
    "        else:\n",
    "            # All pairs\n",
    "            pos_idx1 = []\n",
    "            pos_idx2 = []\n",
    "            for pi in range(len(pos_indices)):\n",
    "                for pj in range(pi+1, len(pos_indices)):\n",
    "                    pos_idx1.append(pos_indices[pi])\n",
    "                    pos_idx2.append(pos_indices[pj])\n",
    "            if len(pos_idx1) > 0:\n",
    "                pos_idx1 = torch.stack(pos_idx1)\n",
    "                pos_idx2 = torch.stack(pos_idx2)\n",
    "        \n",
    "        if len(pos_idx1) > 0:\n",
    "            pos_sim = F.cosine_similarity(normalized[pos_idx1], normalized[pos_idx2], dim=1)\n",
    "            instance_stats['positive_sim_mean'].append(pos_sim.mean().item())\n",
    "            instance_stats['positive_sim_std'].append(pos_sim.std().item())\n",
    "        else:\n",
    "            instance_stats['positive_sim_mean'].append(float('nan'))\n",
    "            instance_stats['positive_sim_std'].append(float('nan'))\n",
    "    else:\n",
    "        instance_stats['positive_sim_mean'].append(float('nan'))\n",
    "        instance_stats['positive_sim_std'].append(float('nan'))\n",
    "    \n",
    "    # 3. Positive vs Negative pairs\n",
    "    neg_indices = torch.where(instance_labels == 0)[0]\n",
    "    if len(pos_indices) > 0 and len(neg_indices) > 0:\n",
    "        num_cross = min(1000, len(pos_indices) * len(neg_indices))\n",
    "        cross_pos_idx = pos_indices[torch.randint(0, len(pos_indices), (num_cross,))]\n",
    "        cross_neg_idx = neg_indices[torch.randint(0, len(neg_indices), (num_cross,))]\n",
    "        \n",
    "        cross_sim = F.cosine_similarity(normalized[cross_pos_idx], normalized[cross_neg_idx], dim=1)\n",
    "        instance_stats['pos_neg_sim_mean'].append(cross_sim.mean().item())\n",
    "        instance_stats['pos_neg_sim_std'].append(cross_sim.std().item())\n",
    "    else:\n",
    "        instance_stats['pos_neg_sim_mean'].append(float('nan'))\n",
    "        instance_stats['pos_neg_sim_std'].append(float('nan'))\n",
    "\n",
    "# Convert to tensors for easy computation (ignoring NaN values)\n",
    "import numpy as np\n",
    "\n",
    "print(f\"\\n{'Instance':<10} {'Positives':<12} {'Random Sim':<15} {'Pos Sim':<15} {'Pos-Neg Sim':<15}\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(min(10, batch_size)):  # Show first 10 instances\n",
    "    random_sim = instance_stats['random_sim_mean'][i]\n",
    "    pos_sim = instance_stats['positive_sim_mean'][i]\n",
    "    pos_neg_sim = instance_stats['pos_neg_sim_mean'][i]\n",
    "    num_pos = instance_stats['num_positives'][i]\n",
    "    \n",
    "    pos_sim_str = f\"{pos_sim:.4f}\" if not np.isnan(pos_sim) else \"N/A\"\n",
    "    pos_neg_str = f\"{pos_neg_sim:.4f}\" if not np.isnan(pos_neg_sim) else \"N/A\"\n",
    "    \n",
    "    print(f\"{i:<10} {num_pos:<12} {random_sim:<15.4f} {pos_sim_str:<15} {pos_neg_str:<15}\")\n",
    "\n",
    "if batch_size > 10:\n",
    "    print(f\"... ({batch_size - 10} more instances)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AGGREGATE STATISTICS ACROSS ALL INSTANCES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute means across all instances (excluding NaN)\n",
    "random_sim_array = np.array(instance_stats['random_sim_mean'])\n",
    "pos_sim_array = np.array(instance_stats['positive_sim_mean'])\n",
    "pos_neg_sim_array = np.array(instance_stats['pos_neg_sim_mean'])\n",
    "\n",
    "print(f\"\\nAverage number of positives per instance: {np.mean(instance_stats['num_positives']):.2f}\")\n",
    "print(f\"Average number of negatives per instance: {np.mean(instance_stats['num_negatives']):.2f}\")\n",
    "\n",
    "print(f\"\\n=== Random Pairs (within each instance) ===\")\n",
    "print(f\"Mean across instances: {np.mean(random_sim_array):.4f}\")\n",
    "print(f\"Std across instances: {np.std(random_sim_array):.4f}\")\n",
    "print(f\"Min: {np.min(random_sim_array):.4f}, Max: {np.max(random_sim_array):.4f}\")\n",
    "\n",
    "valid_pos_sim = pos_sim_array[~np.isnan(pos_sim_array)]\n",
    "if len(valid_pos_sim) > 0:\n",
    "    print(f\"\\n=== Positive Pairs (within each instance) ===\")\n",
    "    print(f\"Mean across instances: {np.mean(valid_pos_sim):.4f}\")\n",
    "    print(f\"Std across instances: {np.std(valid_pos_sim):.4f}\")\n",
    "    print(f\"Min: {np.min(valid_pos_sim):.4f}, Max: {np.max(valid_pos_sim):.4f}\")\n",
    "    print(f\"Instances with positive pairs: {len(valid_pos_sim)} / {batch_size}\")\n",
    "else:\n",
    "    print(f\"\\n=== Positive Pairs ===\")\n",
    "    print(\"No instances with multiple positive labels\")\n",
    "\n",
    "valid_pos_neg_sim = pos_neg_sim_array[~np.isnan(pos_neg_sim_array)]\n",
    "if len(valid_pos_neg_sim) > 0:\n",
    "    print(f\"\\n=== Positive vs Negative Pairs (within each instance) ===\")\n",
    "    print(f\"Mean across instances: {np.mean(valid_pos_neg_sim):.4f}\")\n",
    "    print(f\"Std across instances: {np.std(valid_pos_neg_sim):.4f}\")\n",
    "    print(f\"Min: {np.min(valid_pos_neg_sim):.4f}, Max: {np.max(valid_pos_neg_sim):.4f}\")\n",
    "    print(f\"Instances with both pos and neg: {len(valid_pos_neg_sim)} / {batch_size}\")\n",
    "else:\n",
    "    print(f\"\\n=== Positive vs Negative Pairs ===\")\n",
    "    print(\"No instances with both positive and negative labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06234e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing entire dataset for instances with no positive labels...\n",
      "======================================================================\n",
      "Processing batches...\n",
      "Processed 100 batches (12800 instances)...\n",
      "Processed 200 batches (25600 instances)...\n",
      "Processed 300 batches (38400 instances)...\n",
      "Processed 400 batches (51200 instances)...\n",
      "Processed 500 batches (64000 instances)...\n",
      "\n",
      "Total instances in dataset: 73584\n",
      "Instances with NO positive labels: 999 (1.36%)\n",
      "Instances with at least 1 positive label: 72585 (98.64%)\n",
      "\n",
      "=== Positive Label Distribution ===\n",
      "Mean positive labels per instance: 6.67\n",
      "Median positive labels per instance: 4\n",
      "Std deviation: 8.07\n",
      "Min positive labels: 0\n",
      "Max positive labels: 192\n",
      "\n",
      "=== Distribution of Positive Label Counts ===\n",
      "Num Positives   Count      Percentage\n",
      "----------------------------------------\n",
      "0               999        1.36%\n",
      "1               12549      17.05%\n",
      "2               9485       12.89%\n",
      "3               8270       11.24%\n",
      "4               6950       9.44%\n",
      "5               5838       7.93%\n",
      "6               4733       6.43%\n",
      "7               3896       5.29%\n",
      "8               3197       4.34%\n",
      "9               2707       3.68%\n",
      "10              2193       2.98%\n",
      "11              1746       2.37%\n",
      "12              1522       2.07%\n",
      "13              1148       1.56%\n",
      "14              1093       1.49%\n",
      "15              858        1.17%\n",
      "16              729        0.99%\n",
      "17              674        0.92%\n",
      "18              498        0.68%\n",
      "19              468        0.64%\n",
      "... (97 more categories)\n",
      "\n",
      "=== Sample Instances with 0 Positive Labels ===\n",
      "\n",
      "Instance 0:\n",
      "  EntryID: A0A0B5A051\n",
      "  True terms: ['GO:0005515']\n",
      "  Predicted terms (first 5): ['GO:0004659', 'GO:0016114', 'GO:0010355', 'GO:0009941', 'GO:0009266']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 1:\n",
      "  EntryID: A0A0F6MY85\n",
      "  True terms: ['GO:0046661', 'GO:0030238', 'GO:0048024']\n",
      "  Predicted terms (first 5): ['GO:0003730', 'GO:0003723', 'GO:0048255', 'GO:0000398', 'GO:0030027']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 2:\n",
      "  EntryID: A0A0H2URK1\n",
      "  True terms: ['GO:0005515', 'GO:0044010', 'GO:0052031', 'GO:0009275']\n",
      "  Predicted terms (first 5): ['GO:0007155', 'GO:0005576', 'GO:0032311', 'GO:0007162', 'GO:0031395']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 3:\n",
      "  EntryID: A0A1D5NS60\n",
      "  True terms: ['GO:0031643', 'GO:0022010', 'GO:0048714']\n",
      "  Predicted terms (first 5): ['GO:0140944', 'GO:0030511', 'GO:1904520', 'GO:0006355', 'GO:0007411']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 4:\n",
      "  EntryID: A0A1D8PNR5\n",
      "  True terms: ['GO:0036170', 'GO:0044182', 'GO:0036180', 'GO:0009267', 'GO:0051454', 'GO:0015606', 'GO:0015848', 'GO:0071248', 'GO:0030447']\n",
      "  Predicted terms (first 5): ['GO:0005886', 'GO:0031402', 'GO:0015804', 'GO:1905039', 'GO:0015807']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 5:\n",
      "  EntryID: A0A2I1BT15\n",
      "  True terms: ['GO:0016854', 'GO:0140782']\n",
      "  Predicted terms (first 5): ['GO:0016787', 'GO:0005576', 'GO:0019786', 'GO:0032311', 'GO:0016825']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 6:\n",
      "  EntryID: A0A482N8M8\n",
      "  True terms: ['GO:0140781', 'GO:0016854', 'GO:0016218']\n",
      "  Predicted terms (first 5): ['GO:0016020', 'GO:0046872', 'GO:0051539', 'GO:0016491', 'GO:0005737']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 7:\n",
      "  EntryID: A0A7J6KD88\n",
      "  True terms: ['GO:0005615', 'GO:0097570']\n",
      "  Predicted terms (first 5): ['GO:0016301', 'GO:0009608', 'GO:0005634', 'GO:0005524', 'GO:0005516']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 8:\n",
      "  EntryID: A0SVK0\n",
      "  True terms: ['GO:0042802', 'GO:0010162', 'GO:2000033', 'GO:0009738', 'GO:0005515', 'GO:0010182']\n",
      "  Predicted terms (first 5): ['GO:0005634', 'GO:0005737', 'GO:0003674', 'GO:0043565', 'GO:0006351']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 9:\n",
      "  EntryID: A1A4F0\n",
      "  True terms: ['GO:0005515']\n",
      "  Predicted terms (first 5): ['GO:0061459', 'GO:1903401', 'GO:0005765', 'GO:0015819', 'GO:1903826']\n",
      "  True terms in predicted: None\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Check the entire dataset for instances with no positive labels using prefetch_loader\n",
    "print(\"Analyzing entire dataset for instances with no positive labels...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "instances_with_no_positives = 0\n",
    "instances_with_positives = 0\n",
    "positive_label_counts = []\n",
    "zero_pos_samples = []  # Store samples with 0 positives for later inspection\n",
    "\n",
    "# Use dataloader for faster iteration\n",
    "full_dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=128,  # Larger batch size for faster processing\n",
    "    shuffle=False,  # Don't shuffle to maintain order\n",
    "    num_workers=0, \n",
    "    collate_fn=lambda b: collate_tokenize(b, tokenizer, device, dtype=target_dtype)\n",
    ")\n",
    "\n",
    "print(\"Processing batches...\")\n",
    "batch_count = 0\n",
    "for batch_data in full_dataloader:\n",
    "    batch_count += 1\n",
    "    if batch_count % 100 == 0:\n",
    "        print(f\"Processed {batch_count} batches ({batch_count * 128} instances)...\")\n",
    "    \n",
    "    labels_batch = batch_data['label']  # Shape: (batch_size, 256)\n",
    "    \n",
    "    # Count positives for each instance in the batch\n",
    "    num_positives_per_instance = (labels_batch == 1).sum(dim=1).cpu().numpy()\n",
    "    \n",
    "    for i, num_pos in enumerate(num_positives_per_instance):\n",
    "        positive_label_counts.append(num_pos)\n",
    "        \n",
    "        if num_pos == 0:\n",
    "            instances_with_no_positives += 1\n",
    "            # Store sample info for later (only store first few to save memory)\n",
    "            if len(zero_pos_samples) < 10:\n",
    "                zero_pos_samples.append({\n",
    "                    'entryID': batch_data['entryID'][i],\n",
    "                    'true_terms': batch_data['true_terms'][i],\n",
    "                    'predicted_terms': batch_data['predicted_terms'][i]\n",
    "                })\n",
    "        else:\n",
    "            instances_with_positives += 1\n",
    "\n",
    "total_instances = len(dataset)\n",
    "\n",
    "print(f\"\\nTotal instances in dataset: {total_instances}\")\n",
    "print(f\"Instances with NO positive labels: {instances_with_no_positives} ({instances_with_no_positives/total_instances*100:.2f}%)\")\n",
    "print(f\"Instances with at least 1 positive label: {instances_with_positives} ({instances_with_positives/total_instances*100:.2f}%)\")\n",
    "\n",
    "# Statistics on positive label distribution\n",
    "positive_label_counts = np.array(positive_label_counts)\n",
    "print(f\"\\n=== Positive Label Distribution ===\")\n",
    "print(f\"Mean positive labels per instance: {positive_label_counts.mean():.2f}\")\n",
    "print(f\"Median positive labels per instance: {np.median(positive_label_counts):.0f}\")\n",
    "print(f\"Std deviation: {positive_label_counts.std():.2f}\")\n",
    "print(f\"Min positive labels: {positive_label_counts.min()}\")\n",
    "print(f\"Max positive labels: {positive_label_counts.max()}\")\n",
    "\n",
    "# Show histogram of positive label counts\n",
    "print(f\"\\n=== Distribution of Positive Label Counts ===\")\n",
    "unique, counts = np.unique(positive_label_counts, return_counts=True)\n",
    "print(f\"{'Num Positives':<15} {'Count':<10} {'Percentage':<10}\")\n",
    "print(\"-\" * 40)\n",
    "for num_pos, count in zip(unique[:20], counts[:20]):  # Show first 20\n",
    "    print(f\"{int(num_pos):<15} {count:<10} {count/total_instances*100:.2f}%\")\n",
    "if len(unique) > 20:\n",
    "    print(f\"... ({len(unique) - 20} more categories)\")\n",
    "\n",
    "# Show sample instances with 0 positives\n",
    "if instances_with_no_positives > 0 and len(zero_pos_samples) > 0:\n",
    "    print(f\"\\n=== Sample Instances with 0 Positive Labels ===\")\n",
    "    for i, sample in enumerate(zero_pos_samples):\n",
    "        print(f\"\\nInstance {i}:\")\n",
    "        print(f\"  EntryID: {sample['entryID']}\")\n",
    "        print(f\"  True terms: {sample['true_terms']}\")\n",
    "        print(f\"  Predicted terms (first 5): {sample['predicted_terms'][:5]}\")\n",
    "        # Check if true terms are in predicted terms\n",
    "        true_in_pred = set(sample['true_terms']).intersection(set(sample['predicted_terms']))\n",
    "        print(f\"  True terms in predicted: {true_in_pred if true_in_pred else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80755c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "885c5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model.Query2Label import Query2Label\n",
    "\n",
    "\n",
    "model = Query2Label(num_classes = 256,\n",
    "                    in_dim = 512,\n",
    "                    nheads = 8,\n",
    "                    num_encoder_layers = 1,\n",
    "                    num_decoder_layers = 2,\n",
    "                    dim_feedforward = 2048,\n",
    "                    dropout = 0.1,\n",
    "                    use_positional_encoding = True,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70ab20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from Utils.AsymetricLoss import AsymmetricLossOptimized, AsymmetricLoss\n",
    "from Utils.RankLoss import RankLossPair\n",
    "from Model.Query2Label import Query2Label\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class Query2Label_pl(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        in_dim: int,\n",
    "        nheads: int = 8,\n",
    "        num_encoder_layers: int = 1,\n",
    "        num_decoder_layers: int = 2,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        use_positional_encoding: bool = True,\n",
    "        lr: float = 1e-4,\n",
    "        weight_decay: float = 1e-5,\n",
    "        # Loss selection: 'ASL' or 'BCE'\n",
    "        loss_function: str = 'ASL',\n",
    "        # Asymmetric loss parameters\n",
    "        gamma_neg: float = 4.0,\n",
    "        gamma_pos: float = 0.0,\n",
    "        clip: float = 0.05,\n",
    "        loss_eps: float = 1e-8,\n",
    "        disable_torch_grad_focal_loss: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = Query2Label(\n",
    "            num_classes=num_classes,\n",
    "            in_dim=in_dim,\n",
    "            nheads=nheads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            use_positional_encoding=use_positional_encoding,\n",
    "        )\n",
    "\n",
    "    \n",
    "        # Instantiate chosen loss function\n",
    "        lf = (loss_function or 'ASL').upper()\n",
    "        if lf in ('ASL', 'ASYMMETRIC', 'ASYMMETRIC_LOSS'):\n",
    "            # prefer optimized implementation\n",
    "            try:\n",
    "                self.criterion = AsymmetricLossOptimized(\n",
    "                    gamma_neg=gamma_neg,\n",
    "                    gamma_pos=gamma_pos,\n",
    "                    clip=clip,\n",
    "                    eps=loss_eps,\n",
    "                    disable_torch_grad_focal_loss=disable_torch_grad_focal_loss,\n",
    "                )\n",
    "            except Exception:\n",
    "                self.criterion = AsymmetricLoss(\n",
    "                    gamma_neg=gamma_neg,\n",
    "                    gamma_pos=gamma_pos,\n",
    "                    clip=clip,\n",
    "                    eps=loss_eps,\n",
    "                    disable_torch_grad_focal_loss=disable_torch_grad_focal_loss,\n",
    "                )\n",
    "        elif lf in ('BCE', 'BCEWITHLOGITS', 'BCEWITHLOGITSLOSS'):\n",
    "            # Use BCEWithLogitsLoss; set reduction='sum' to match ASL sum-based scale\n",
    "            self.criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "        elif lf in ('RANK', 'RANKLOSS', 'RANK_LOSS'):\n",
    "            # RankLoss uses RankLossPair class\n",
    "            self.criterion = RankLossPair(reduction='mean')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss_function: {loss_function}. Supported: 'ASL', 'BCE', 'RankLoss'.\")\n",
    "        \n",
    "        # Store loss function name for training step\n",
    "        self.loss_function = lf\n",
    "\n",
    "        # Store validation step outputs for on_validation_epoch_end\n",
    "        self.validation_step_outputs = []\n",
    "        # Test step outputs\n",
    "        self.test_step_outputs = []\n",
    "        # For GO term-based F1 computation (store predictions per GO term)\n",
    "        self._val_go_predictions = {}  # {go_term_id: list of probabilities}\n",
    "        self._val_go_targets = {}      # {go_term_id: list of binary labels}\n",
    "        self._rank_score_list = []\n",
    "        self._cutoff_score_list = []\n",
    "\n",
    "    def forward(self, x: torch.Tensor, f: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x, f) \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Standard training step.\n",
    "\n",
    "        Expects `batch` to be a dict with keys `'tokens'` and `'label'` where\n",
    "        - `tokens` is a float Tensor of shape (B, L, C_in)\n",
    "        - `label` is a binary Tensor of shape (B, num_classes)\n",
    "        \"\"\"\n",
    "        x = batch['go_embed']   # (B, L, C_in)\n",
    "        f = batch['feature']   \n",
    "        y = batch['label']\n",
    "\n",
    "        logits = self.forward(x, f)\n",
    "        \n",
    "        # Calculate loss based on loss function type\n",
    "        if self.loss_function in ('RANK', 'RANKLOSS', 'RANK_LOSS'):\n",
    "            # RankLoss: compute loss for each class separately and sum\n",
    "            losses = []\n",
    "            for class_idx in range(logits.shape[1]):\n",
    "                scores = logits[:, class_idx]  # (B,)\n",
    "                labels = y[:, class_idx]  # (B,)\n",
    "                \n",
    "                # Only compute loss if we have both positive and negative samples\n",
    "                if labels.sum() > 0 and labels.sum() < len(labels):\n",
    "                    class_loss = self.criterion(scores, labels)\n",
    "                    losses.append(class_loss)\n",
    "            \n",
    "            if losses:\n",
    "                loss = torch.stack(losses).mean()\n",
    "        else:\n",
    "            # Standard loss functions (ASL, BCE)\n",
    "            loss = self.criterion(logits, y)\n",
    "\n",
    "        # Log loss\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Validation step computes loss and stores for F-max computation.\"\"\"\n",
    "        x = batch['go_embed']   # (B, L, C_in)\n",
    "        f = batch['feature']   \n",
    "        y = batch['label']\n",
    "\n",
    "        logits = self.forward(x, f)\n",
    "        \n",
    "        # Calculate loss based on loss function type\n",
    "        if self.loss_function in ('RANK', 'RANKLOSS', 'RANK_LOSS'):\n",
    "            # RankLoss: compute loss for each class separately and sum\n",
    "            losses = []\n",
    "            for class_idx in range(logits.shape[1]):\n",
    "                scores = logits[:, class_idx]  # (B,)\n",
    "                labels = y[:, class_idx]  # (B,)\n",
    "                \n",
    "                # Only compute loss if we have both positive and negative samples\n",
    "                if labels.sum() > 0 and labels.sum() < len(labels):\n",
    "                    class_loss = self.criterion(scores, labels)\n",
    "                    losses.append(class_loss)\n",
    "            \n",
    "            if losses:\n",
    "                loss = torch.stack(losses).mean()\n",
    "            else:\n",
    "                # Fallback to BCE if no valid pairs\n",
    "                loss = nn.functional.binary_cross_entropy_with_logits(logits, y, reduction='mean')\n",
    "        else:\n",
    "            # Standard loss functions (ASL, BCE)\n",
    "            loss = self.criterion(logits, y)\n",
    "\n",
    "        # Log per-step validation loss (will be reduced automatically)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        # Store for epoch end aggregation\n",
    "        self.validation_step_outputs.append({'val_loss': loss.detach()})\n",
    "        \n",
    "        # Store GO term-specific predictions for macro/micro F1\n",
    "        probs = torch.sigmoid(logits)\n",
    "        targets = (y > 0.5).int()\n",
    "        try:\n",
    "            probs_np = probs.detach().cpu().numpy()\n",
    "            targets_np = targets.detach().cpu().numpy()\n",
    "            predicted_terms = batch['predicted_terms']  # List of lists of GO term IDs\n",
    "            \n",
    "            #Calculate rankscore and cutoff score\n",
    "            rank_score = self._compute_rank_score(probs_np, targets_np, predicted_terms)\n",
    "            cutoff_score = self._compute_cutoff_score(probs_np, targets_np, predicted_terms)\n",
    "\n",
    "            self._rank_score_list.append(rank_score)\n",
    "            self._cutoff_score_list.append(cutoff_score)\n",
    "\n",
    "            # For each sample in the batch\n",
    "            for sample_idx in range(len(predicted_terms)):\n",
    "                sample_terms = predicted_terms[sample_idx]\n",
    "                sample_probs = probs_np[sample_idx]  # Shape: (num_terms_for_sample,)\n",
    "                sample_targets = targets_np[sample_idx]  # Shape: (num_terms_for_sample,)\n",
    "                \n",
    "                # For each GO term in this sample\n",
    "                for term_idx, go_term in enumerate(sample_terms):\n",
    "                    if go_term not in self._val_go_predictions:\n",
    "                        self._val_go_predictions[go_term] = []\n",
    "                        self._val_go_targets[go_term] = []\n",
    "                    \n",
    "                    self._val_go_predictions[go_term].append(float(sample_probs[term_idx]))\n",
    "                    self._val_go_targets[go_term].append(int(sample_targets[term_idx]))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not store GO term predictions: {e}\")\n",
    "            pass\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        \"\"\"Called at the end of validation epoch. Compute F-max only.\"\"\"\n",
    "        # compute mean validation loss for the epoch\n",
    "        if not self.validation_step_outputs:\n",
    "            return\n",
    "        losses = torch.stack([o['val_loss'] for o in self.validation_step_outputs])\n",
    "        mean_loss = losses.mean()\n",
    "\n",
    "        \n",
    "        self.log('val_loss_epoch', mean_loss, prog_bar=True, logger=True)\n",
    "        \n",
    "        mean_rank_score = float(np.mean(self._rank_score_list)) if self._rank_score_list else 0.0\n",
    "        mean_cutoff_score = float(np.mean(self._cutoff_score_list)) if self._cutoff_score_list else 0.0\n",
    "        self.log('val_rank_score', mean_rank_score, prog_bar=True, logger=True)\n",
    "        self.log('val_cutoff_score', mean_cutoff_score, prog_bar=True, logger=True)\n",
    "\n",
    "\n",
    "        # Compute GO term-based macro and micro F1 scores\n",
    "        try:\n",
    "            if len(self._val_go_predictions) > 0:\n",
    "                macro_f1, micro_f1 = self._compute_go_f1_scores()\n",
    "                self.log('val_f1_macro_go', macro_f1, prog_bar=True, logger=True)\n",
    "                self.log('val_f1_micro_go', micro_f1, prog_bar=True, logger=True)\n",
    "                \n",
    "                try:\n",
    "                    rank = getattr(self, 'global_rank', 0)\n",
    "                except Exception:\n",
    "                    rank = 0\n",
    "                if rank == 0:\n",
    "                    print(f\"Validation GO-based Macro F1: {macro_f1:.4f}, Micro F1: {micro_f1:.4f}\")\n",
    "                \n",
    "                # Clear GO term buffers\n",
    "                self._val_go_predictions.clear()\n",
    "                self._val_go_targets.clear()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not compute GO-based F1 scores: {e}\")\n",
    "            pass\n",
    "        \n",
    "        # Clear stored outputs for next epoch\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"Test step computes loss and logs it.\"\"\"\n",
    "        x = batch['tokens']\n",
    "        y = batch['label'].float()\n",
    "\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "\n",
    "        # Log per-step test loss\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        self.test_step_outputs.append({'test_loss': loss.detach()})\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        \"\"\"Aggregate test metrics at epoch end and log them.\"\"\"\n",
    "        if not self.test_step_outputs:\n",
    "            return\n",
    "        losses = torch.stack([o['test_loss'] for o in self.test_step_outputs])\n",
    "        mean_loss = losses.mean()\n",
    "\n",
    "        self.log('test_loss_epoch', mean_loss, prog_bar=True, logger=True)\n",
    "        print(f\"Test Loss: {mean_loss:.4f}\")\n",
    "\n",
    "        self.test_step_outputs.clear()\n",
    "\n",
    "    def _compute_rank_score(self, probs_np, targets_np, predicted_terms):\n",
    "        \"\"\"Compute rank score based on GO term predictions.\n",
    "        \n",
    "        For each instance:\n",
    "        1. Threshold = number of positive labels\n",
    "        2. Sort predictions in descending order\n",
    "        3. Count how many true labels are in top `threshold` positions\n",
    "        4. Score = (true labels in top k) / (total true labels)\n",
    "        \"\"\"\n",
    "        batch_scores = []\n",
    "        \n",
    "        for sample_idx in range(len(probs_np)):\n",
    "            sample_probs = probs_np[sample_idx]\n",
    "            sample_targets = targets_np[sample_idx]\n",
    "            \n",
    "            # Threshold is the number of positive labels\n",
    "            num_positives = int(sample_targets.sum())\n",
    "            \n",
    "            # Skip if no positives\n",
    "            if num_positives == 0:\n",
    "                continue\n",
    "            \n",
    "            # Get indices of predictions sorted in descending order\n",
    "            sorted_indices = np.argsort(sample_probs)[::-1]\n",
    "            \n",
    "            # Get top k indices where k = number of positive labels\n",
    "            top_k_indices = sorted_indices[:num_positives]\n",
    "            \n",
    "            # Check how many of the top k predictions are true positives\n",
    "            true_positives_in_top_k = sample_targets[top_k_indices].sum()\n",
    "            \n",
    "            # Calculate score for this sample\n",
    "            score = true_positives_in_top_k / num_positives\n",
    "            batch_scores.append(score)\n",
    "        \n",
    "        # Return average score across all samples in batch\n",
    "        return float(np.mean(batch_scores)) if batch_scores else 0.0\n",
    "        \n",
    "\n",
    "    def _compute_cutoff_score(self, probs_np, targets_np, predicted_terms):\n",
    "        \"\"\"Compute cutoff score based on GO term predictions.\n",
    "        \n",
    "        For each instance:\n",
    "        1. Sort predictions in descending order\n",
    "        2. Find the index of the last (lowest ranked) true label\n",
    "        3. That index is the cutoff score\n",
    "        \"\"\"\n",
    "        batch_scores = []\n",
    "        \n",
    "        for sample_idx in range(len(probs_np)):\n",
    "            sample_probs = probs_np[sample_idx]\n",
    "            sample_targets = targets_np[sample_idx]\n",
    "            \n",
    "            # Skip if no positives\n",
    "            num_positives = int(sample_targets.sum())\n",
    "            if num_positives == 0:\n",
    "                continue\n",
    "            \n",
    "            # Get indices of predictions sorted in descending order\n",
    "            sorted_indices = np.argsort(sample_probs)[::-1]\n",
    "            \n",
    "            # Find positions of true labels in the sorted order\n",
    "            true_label_positions = []\n",
    "            for idx, sorted_idx in enumerate(sorted_indices):\n",
    "                if sample_targets[sorted_idx] == 1:\n",
    "                    true_label_positions.append(idx)\n",
    "            \n",
    "            # The cutoff score is the index of the last true label\n",
    "            if true_label_positions:\n",
    "                cutoff_score = max(true_label_positions)\n",
    "                batch_scores.append(cutoff_score)\n",
    "        \n",
    "        # Return average score across all samples in batch\n",
    "        return float(np.mean(batch_scores)) if batch_scores else 0.0\n",
    "\n",
    "\n",
    "    def _compute_go_f1_scores(self, threshold=0.5):\n",
    "        \"\"\"Compute macro and micro F1 scores based on GO term predictions.\n",
    "        \n",
    "        For each GO term, we have multiple predictions across different samples.\n",
    "        Macro F1: Average F1 score across all GO terms\n",
    "        Micro F1: F1 score computed from global TP, FP, FN counts\n",
    "        \"\"\"\n",
    "        eps = 1e-8\n",
    "        per_term_f1_scores = []\n",
    "        \n",
    "        # Global counts for micro F1\n",
    "        global_tp = 0\n",
    "        global_fp = 0\n",
    "        global_fn = 0\n",
    "        \n",
    "        # Compute F1 for each GO term\n",
    "        for go_term, predictions in self._val_go_predictions.items():\n",
    "            targets = self._val_go_targets[go_term]\n",
    "            \n",
    "            # Convert to numpy for easier computation\n",
    "            preds_array = np.array(predictions)\n",
    "            targets_array = np.array(targets)\n",
    "            \n",
    "            # Binarize predictions\n",
    "            preds_binary = (preds_array >= threshold).astype(int)\n",
    "            \n",
    "            # Compute TP, FP, FN for this GO term\n",
    "            tp = np.sum((preds_binary == 1) & (targets_array == 1))\n",
    "            fp = np.sum((preds_binary == 1) & (targets_array == 0))\n",
    "            fn = np.sum((preds_binary == 0) & (targets_array == 1))\n",
    "            \n",
    "            # Accumulate for micro F1\n",
    "            global_tp += tp\n",
    "            global_fp += fp\n",
    "            global_fn += fn\n",
    "            \n",
    "            # Compute F1 for this term\n",
    "            precision = tp / (tp + fp + eps)\n",
    "            recall = tp / (tp + fn + eps)\n",
    "            f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "            \n",
    "            per_term_f1_scores.append(f1)\n",
    "        \n",
    "        # Compute macro F1 (average across all GO terms)\n",
    "        macro_f1 = float(np.mean(per_term_f1_scores)) if per_term_f1_scores else 0.0\n",
    "        \n",
    "        # Compute micro F1 (global counts)\n",
    "        micro_precision = global_tp / (global_tp + global_fp + eps)\n",
    "        micro_recall = global_tp / (global_tp + global_fn + eps)\n",
    "        micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall + eps)\n",
    "        micro_f1 = float(micro_f1)\n",
    "        \n",
    "        return macro_f1, micro_f1\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Use AdamW; hyperparameters were saved in self.hparams by save_hyperparameters()\n",
    "        lr = self.hparams.get('lr', 1e-4) if isinstance(self.hparams, dict) else getattr(self.hparams, 'lr', 1e-4)\n",
    "        weight_decay = self.hparams.get('weight_decay', 1e-5) if isinstance(self.hparams, dict) else getattr(self.hparams, 'weight_decay', 1e-5)\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # Use ReduceLROnPlateau scheduler (monitoring validation metric 'val_fmax_macro').\n",
    "        # Parameters can be provided via hparams: 'plateau_factor', 'plateau_patience', 'min_lr', 'plateau_threshold'.\n",
    "        if isinstance(self.hparams, dict):\n",
    "            plateau_factor = self.hparams.get('plateau_factor', 0.5)\n",
    "            plateau_patience = int(self.hparams.get('plateau_patience', 3))\n",
    "            plateau_min_lr = self.hparams.get('min_lr', 1e-6)\n",
    "            plateau_threshold = self.hparams.get('plateau_threshold', 1e-4)\n",
    "        else:\n",
    "            plateau_factor = getattr(self.hparams, 'plateau_factor', 0.5)\n",
    "            plateau_patience = int(getattr(self.hparams, 'plateau_patience', 3))\n",
    "            plateau_min_lr = getattr(self.hparams, 'min_lr', 1e-6)\n",
    "            plateau_threshold = getattr(self.hparams, 'plateau_threshold', 1e-4)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='max',\n",
    "            factor=float(plateau_factor),\n",
    "            patience=int(plateau_patience),\n",
    "            min_lr=float(plateau_min_lr),\n",
    "            threshold=float(plateau_threshold)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_f1_macro_go',\n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1,\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1ddf3",
   "metadata": {},
   "source": [
    "## Test Rank Score and Cutoff Score Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94fbce18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST: Rank Score and Cutoff Score Functions\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "SAMPLE DATA\n",
      "======================================================================\n",
      "\n",
      "Instance 1:\n",
      "  Predictions: [0.9  0.85 0.7  0.65 0.5  0.45 0.3  0.2 ]\n",
      "  True labels: [1 1 0 1 0 0 1 0]\n",
      "  Num positives: 4\n",
      "  Sorted indices: [0 1 2 3 4 5 6 7]\n",
      "  Sorted predictions: [0.9  0.85 0.7  0.65 0.5  0.45 0.3  0.2 ]\n",
      "  Sorted labels: [1 1 0 1 0 0 1 0]\n",
      "  True label positions in sorted order: [0, 1, 3, 6]\n",
      "  --> Rank Score: 3/4 = 0.75\n",
      "  --> Cutoff Score: 6 (index of last true label)\n",
      "\n",
      "Instance 2:\n",
      "  Predictions: [0.95 0.8  0.75 0.6  0.4  0.3  0.25 0.15]\n",
      "  True labels: [1 0 1 0 1 0 0 0]\n",
      "  Num positives: 3\n",
      "  Sorted indices: [0 1 2 3 4 5 6 7]\n",
      "  Sorted predictions: [0.95 0.8  0.75 0.6  0.4  0.3  0.25 0.15]\n",
      "  Sorted labels: [1 0 1 0 1 0 0 0]\n",
      "  True label positions in sorted order: [0, 2, 4]\n",
      "  --> Rank Score: 2/3 = 0.67\n",
      "  --> Cutoff Score: 4 (index of last true label)\n",
      "\n",
      "Instance 3:\n",
      "  Predictions: [0.7  0.6  0.5  0.4  0.3  0.2  0.1  0.05]\n",
      "  True labels: [0 0 0 1 1 0 0 0]\n",
      "  Num positives: 2\n",
      "  Sorted indices: [0 1 2 3 4 5 6 7]\n",
      "  Sorted predictions: [0.7  0.6  0.5  0.4  0.3  0.2  0.1  0.05]\n",
      "  Sorted labels: [0 0 0 1 1 0 0 0]\n",
      "  True label positions in sorted order: [3, 4]\n",
      "  --> Rank Score: 0/2 = 0.00\n",
      "  --> Cutoff Score: 4 (index of last true label)\n",
      "\n",
      "======================================================================\n",
      "FUNCTION RESULTS\n",
      "======================================================================\n",
      "\n",
      "Average Rank Score across batch: 0.4722\n",
      "Average Cutoff Score across batch: 4.67\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION\n",
      "======================================================================\n",
      "\n",
      "Rank Score:\n",
      "  - Measures how many true labels are in the top-k predictions\n",
      "  - k = number of true labels for each instance\n",
      "  - Higher is better (1.0 = perfect ranking)\n",
      "\n",
      "Cutoff Score:\n",
      "  - Index position of the last (lowest-ranked) true label\n",
      "  - Lower is better (all true labels ranked high)\n",
      "  - Useful for determining prediction threshold\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create test data for rank score and cutoff score functions\n",
    "print(\"=\"*70)\n",
    "print(\"TEST: Rank Score and Cutoff Score Functions\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create sample data with same length (8 elements each) to allow stacking\n",
    "# Instance 1: 4 true labels, predictions with varying scores\n",
    "sample1_probs = np.array([0.9, 0.85, 0.7, 0.65, 0.5, 0.45, 0.3, 0.2])\n",
    "sample1_targets = np.array([1, 1, 0, 1, 0, 0, 1, 0])  # 4 positives at indices 0, 1, 3, 6\n",
    "\n",
    "# Instance 2: 3 true labels (padded to 8)\n",
    "sample2_probs = np.array([0.95, 0.8, 0.75, 0.6, 0.4, 0.3, 0.25, 0.15])\n",
    "sample2_targets = np.array([1, 0, 1, 0, 1, 0, 0, 0])  # 3 positives at indices 0, 2, 4\n",
    "\n",
    "# Instance 3: 2 true labels with low predictions (padded to 8)\n",
    "sample3_probs = np.array([0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.05])\n",
    "sample3_targets = np.array([0, 0, 0, 1, 1, 0, 0, 0])  # 2 positives at indices 3, 4 (lowest scores)\n",
    "\n",
    "# Stack into batch format (all same length now)\n",
    "probs_np = np.stack([sample1_probs, sample2_probs, sample3_probs])\n",
    "targets_np = np.stack([sample1_targets, sample2_targets, sample3_targets])\n",
    "\n",
    "# Create dummy predicted_terms (not used in computation but required by function signature)\n",
    "predicted_terms = [\n",
    "    [f\"GO:000000{i}\" for i in range(len(sample1_probs))],\n",
    "    [f\"GO:000000{i}\" for i in range(len(sample2_probs))],\n",
    "    [f\"GO:000000{i}\" for i in range(len(sample3_probs))]\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analyze each instance\n",
    "for idx, (probs, targets) in enumerate(zip(probs_np, targets_np)):\n",
    "    print(f\"\\nInstance {idx + 1}:\")\n",
    "    print(f\"  Predictions: {probs}\")\n",
    "    print(f\"  True labels: {targets}\")\n",
    "    print(f\"  Num positives: {int(targets.sum())}\")\n",
    "    \n",
    "    # Show sorted order\n",
    "    sorted_indices = np.argsort(probs)[::-1]\n",
    "    print(f\"  Sorted indices: {sorted_indices}\")\n",
    "    print(f\"  Sorted predictions: {probs[sorted_indices]}\")\n",
    "    print(f\"  Sorted labels: {targets[sorted_indices]}\")\n",
    "    \n",
    "    # Find positions of true labels in sorted order\n",
    "    true_positions = [i for i, idx in enumerate(sorted_indices) if targets[idx] == 1]\n",
    "    print(f\"  True label positions in sorted order: {true_positions}\")\n",
    "    \n",
    "    # Calculate metrics for this instance\n",
    "    num_pos = int(targets.sum())\n",
    "    if num_pos > 0:\n",
    "        top_k_indices = sorted_indices[:num_pos]\n",
    "        true_in_top_k = targets[top_k_indices].sum()\n",
    "        rank_score = true_in_top_k / num_pos\n",
    "        cutoff_score = max(true_positions) if true_positions else 0\n",
    "        \n",
    "        print(f\"  --> Rank Score: {true_in_top_k}/{num_pos} = {rank_score:.2f}\")\n",
    "        print(f\"  --> Cutoff Score: {cutoff_score} (index of last true label)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FUNCTION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create a model instance to access the methods\n",
    "test_model = Query2Label_pl(num_classes=8, in_dim=512)\n",
    "\n",
    "# Test the functions\n",
    "rank_score = test_model._compute_rank_score(probs_np, targets_np, predicted_terms)\n",
    "cutoff_score = test_model._compute_cutoff_score(probs_np, targets_np, predicted_terms)\n",
    "\n",
    "print(f\"\\nAverage Rank Score across batch: {rank_score:.4f}\")\n",
    "print(f\"Average Cutoff Score across batch: {cutoff_score:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nRank Score:\")\n",
    "print(\"  - Measures how many true labels are in the top-k predictions\")\n",
    "print(\"  - k = number of true labels for each instance\")\n",
    "print(\"  - Higher is better (1.0 = perfect ranking)\")\n",
    "print(\"\\nCutoff Score:\")\n",
    "print(\"  - Index position of the last (lowest-ranked) true label\")\n",
    "print(\"  - Lower is better (all true labels ranked high)\")\n",
    "print(\"  - Useful for determining prediction threshold\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ea9631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_np = np.ones((128, 64))\n",
    "true_np.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bf2a336",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot specify order when the array has no fields.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m pred_np = np.random.rand(\u001b[32m128\u001b[39m, \u001b[32m64\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mpred_np\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquicksort\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mascending\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m pred_np\n",
      "\u001b[31mValueError\u001b[39m: Cannot specify order when the array has no fields."
     ]
    }
   ],
   "source": [
    "# pred_np = np.random.rand(128, 64)\n",
    "\n",
    "# pred_np.sort(axis=1, kind='quicksort', order='ascending')\n",
    "# pred_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7cabb1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7fc4cd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['go_embed'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ac9fad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['feature'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c8dcebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(batch['go_embed'], batch['feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53018c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f241a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5255e8c2",
   "metadata": {},
   "source": [
    "## -------Analysis----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c7fab6",
   "metadata": {},
   "source": [
    "### Test Random Walk with Different Seeds\n",
    "\n",
    "Test how different seeds produce different predicted terms for entries with rare terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb792e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82201, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_2_terms = pd.read_parquet(data_paths['knn_terms_df'], engine='fastparquet')\n",
    "train_terms = pd.read_csv(data_paths['train_terms_df'], sep='\\t')\n",
    "seq_2_terms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bfb79c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO graph loaded: 40122 nodes\n",
      "GO embeddings: 40122 terms\n"
     ]
    }
   ],
   "source": [
    "# Load GO graph and embeddings for testing\n",
    "go_graph = obonet.read_obo(data_paths['go_obo_path'])\n",
    "\n",
    "with open(data_paths['go_embeds_paths'], 'rb') as f:\n",
    "    data_pkl = pickle.load(f)\n",
    "    embeddings_dict = data_pkl['embeddings']\n",
    "    go_ids = data_pkl['go_ids']\n",
    "\n",
    "print(f\"GO graph loaded: {len(go_graph)} nodes\")\n",
    "print(f\"GO embeddings: {len(embeddings_dict)} terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3c3a5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique terms: 26125\n",
      "Rare terms (<=10 occurrences): 18868\n",
      "\n",
      "Examples of rare terms:\n",
      "term\n",
      "GO:0007113    10\n",
      "GO:0048630    10\n",
      "GO:0090281    10\n",
      "GO:0060982    10\n",
      "GO:0008046    10\n",
      "GO:0002804    10\n",
      "GO:0001849    10\n",
      "GO:0017162    10\n",
      "GO:0060736    10\n",
      "GO:1905461    10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Find rare terms (terms that appear in few proteins)\n",
    "term_counts = train_terms['term'].value_counts()\n",
    "rare_terms = term_counts[term_counts <= 10].index.tolist()  # Terms that appear in <= 10 proteins\n",
    "\n",
    "print(f\"Total unique terms: {len(term_counts)}\")\n",
    "print(f\"Rare terms (<=10 occurrences): {len(rare_terms)}\")\n",
    "print(f\"\\nExamples of rare terms:\")\n",
    "print(term_counts[term_counts <= 10].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "904dac62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proteins with 1-2 rare true labels AND needing padding (with >0 original terms): 19601 / 82201\n",
      "\n",
      "Distribution of original term counts in filtered proteins:\n",
      "count    19601.000000\n",
      "mean        63.667823\n",
      "std         59.607553\n",
      "min          1.000000\n",
      "25%         21.000000\n",
      "50%         41.000000\n",
      "75%         88.000000\n",
      "max        255.000000\n",
      "Name: original_term_count, dtype: float64\n",
      "\n",
      "Distribution of rare true label counts:\n",
      "rare_true_count\n",
      "1    13513\n",
      "2     6088\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample proteins:\n",
      "        qseqid  original_term_count  rare_true_count\n",
      "5   A0A023FFD0                    4                1\n",
      "14  A0A026W182                   30                2\n",
      "15  A0A044RE18                  203                1\n",
      "16  A0A059TC02                   68                1\n",
      "17  A0A060A682                   20                2\n",
      "        qseqid  original_term_count  rare_true_count\n",
      "5   A0A023FFD0                    4                1\n",
      "14  A0A026W182                   30                2\n",
      "15  A0A044RE18                  203                1\n",
      "16  A0A059TC02                   68                1\n",
      "17  A0A060A682                   20                2\n"
     ]
    }
   ],
   "source": [
    "# Find proteins that have rare TRUE labels (1 or 2 rare labels)\n",
    "rare_terms_set = set(rare_terms)\n",
    "\n",
    "# Count rare terms in true labels for each protein\n",
    "def count_rare_true_labels(terms_list):\n",
    "    return len(set(terms_list) & rare_terms_set)\n",
    "\n",
    "seq_2_terms['rare_true_count'] = seq_2_terms['terms_true'].apply(count_rare_true_labels)\n",
    "seq_2_terms['original_term_count'] = seq_2_terms['terms_predicted'].apply(len)\n",
    "\n",
    "# Filter: proteins with 1-2 rare true labels AND 0 < original terms < 256 (need padding but have some terms)\n",
    "proteins_with_rare = seq_2_terms[\n",
    "    (seq_2_terms['rare_true_count'].isin([1, 2])) & \n",
    "    (seq_2_terms['original_term_count'] > 0) &  # Must have at least 1 predicted term\n",
    "    (seq_2_terms['original_term_count'] < 256)\n",
    "].copy()\n",
    "\n",
    "print(f\"Proteins with 1-2 rare true labels AND needing padding (with >0 original terms): {len(proteins_with_rare)} / {len(seq_2_terms)}\")\n",
    "print(f\"\\nDistribution of original term counts in filtered proteins:\")\n",
    "print(proteins_with_rare['original_term_count'].describe())\n",
    "print(f\"\\nDistribution of rare true label counts:\")\n",
    "print(proteins_with_rare['rare_true_count'].value_counts())\n",
    "print(f\"\\nSample proteins:\")\n",
    "print(proteins_with_rare[['qseqid', 'original_term_count', 'rare_true_count']].head(5))\n",
    "print(proteins_with_rare[['qseqid', 'original_term_count', 'rare_true_count']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cde153c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building neighbor cache for all GO terms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab6d579d4804388974e7be3fbda773a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Caching neighbors:   0%|          | 0/40122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor cache built for 40122 terms\n"
     ]
    }
   ],
   "source": [
    "# Build neighbor cache for testing\n",
    "neighbor_cache = build_neighbor_cache(go_graph, embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72112ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Protein: O33283\n",
      "Original predicted terms count: 1\n",
      "True labels count: 4\n",
      "Rare true labels (2): ['GO:0033963', 'GO:0097176']\n",
      "Original predicted terms (first 10): ['GO:0018744']\n",
      "\n",
      "Comparison with seed 42 (baseline):\n",
      "  Seed 123: 3/256 overlap (100.0%), 0 new terms, 0 removed\n",
      "  Seed 456: 2/256 overlap (66.7%), 1 new terms, 1 removed\n",
      "  Seed 789: 3/256 overlap (100.0%), 0 new terms, 0 removed\n",
      "  Seed 999: 2/256 overlap (66.7%), 1 new terms, 1 removed\n",
      "\n",
      "================================================================================\n",
      "Protein: A5A618\n",
      "Original predicted terms count: 1\n",
      "True labels count: 4\n",
      "Rare true labels (1): ['GO:0070069']\n",
      "Original predicted terms (first 10): ['GO:0005886']\n",
      "\n",
      "Comparison with seed 42 (baseline):\n",
      "  Seed 123: 8/256 overlap (44.4%), 45 new terms, 10 removed\n",
      "  Seed 456: 9/256 overlap (50.0%), 7 new terms, 9 removed\n",
      "  Seed 789: 4/256 overlap (22.2%), 12 new terms, 14 removed\n",
      "  Seed 999: 8/256 overlap (44.4%), 45 new terms, 10 removed\n",
      "\n",
      "================================================================================\n",
      "Protein: G2X4M1\n",
      "Original predicted terms count: 1\n",
      "True labels count: 2\n",
      "Rare true labels (1): ['GO:0008908']\n",
      "Original predicted terms (first 10): ['GO:0016787']\n",
      "\n",
      "Comparison with seed 42 (baseline):\n",
      "  Seed 123: 5/256 overlap (38.5%), 9 new terms, 8 removed\n",
      "  Seed 456: 2/256 overlap (15.4%), 13 new terms, 11 removed\n",
      "  Seed 789: 3/256 overlap (23.1%), 10 new terms, 10 removed\n",
      "  Seed 999: 1/256 overlap (7.7%), 12 new terms, 12 removed\n",
      "\n",
      "================================================================================\n",
      "Protein: A0A1D6GNR3\n",
      "Original predicted terms count: 1\n",
      "True labels count: 4\n",
      "Rare true labels (1): ['GO:0051851']\n",
      "Original predicted terms (first 10): ['GO:0005576']\n",
      "\n",
      "Comparison with seed 42 (baseline):\n",
      "  Seed 123: 5/256 overlap (5.2%), 8 new terms, 91 removed\n",
      "  Seed 456: 4/256 overlap (4.2%), 7 new terms, 92 removed\n",
      "  Seed 789: 5/256 overlap (5.2%), 6 new terms, 91 removed\n",
      "  Seed 999: 3/256 overlap (3.1%), 7 new terms, 93 removed\n",
      "\n",
      "================================================================================\n",
      "Protein: O31989\n",
      "Original predicted terms count: 1\n",
      "True labels count: 2\n",
      "Rare true labels (1): ['GO:0030153']\n",
      "Original predicted terms (first 10): ['GO:0020002']\n",
      "\n",
      "Comparison with seed 42 (baseline):\n",
      "  Seed 123: 2/256 overlap (40.0%), 3 new terms, 3 removed\n",
      "  Seed 456: 3/256 overlap (60.0%), 2 new terms, 2 removed\n",
      "  Seed 789: 1/256 overlap (20.0%), 2 new terms, 4 removed\n",
      "  Seed 999: 2/256 overlap (40.0%), 1 new terms, 3 removed\n",
      "\n",
      "================================================================================\n",
      "Summary complete!\n"
     ]
    }
   ],
   "source": [
    "# Test random walk with different seeds on a few samples\n",
    "num_test_samples = 5\n",
    "test_seeds = [42, 123, 456, 789, 999]\n",
    "\n",
    "# Select a few proteins with rare terms (prioritize those with fewer original terms)\n",
    "test_proteins = proteins_with_rare.nsmallest(num_test_samples * 2, 'original_term_count').sample(n=num_test_samples, random_state=42)\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in test_proteins.iterrows():\n",
    "    qseqid = row['qseqid']\n",
    "    original_terms = row['terms_predicted']\n",
    "    true_terms = row['terms_true']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Protein: {qseqid}\")\n",
    "    print(f\"Original predicted terms count: {len(original_terms)}\")\n",
    "    print(f\"True labels count: {len(true_terms)}\")\n",
    "    \n",
    "    # Find rare terms in true labels\n",
    "    rare_in_true = [t for t in true_terms if t in rare_terms_set]\n",
    "    print(f\"Rare true labels ({len(rare_in_true)}): {rare_in_true}\")\n",
    "    print(f\"Original predicted terms (first 10): {original_terms[:10]}\")\n",
    "    \n",
    "    # Apply random walk with different seeds\n",
    "    seed_results = {\n",
    "        'qseqid': qseqid, \n",
    "        'original_count': len(original_terms),\n",
    "        'true_labels': true_terms,\n",
    "        'rare_true_labels': rare_in_true\n",
    "    }\n",
    "    \n",
    "    for seed in test_seeds:\n",
    "        padded = pad_terms_with_random_walk(original_terms, neighbor_cache, max_terms=256, seed=seed)\n",
    "        seed_results[f'seed_{seed}'] = padded\n",
    "        \n",
    "    results.append(seed_results)\n",
    "    \n",
    "    # Compare the first seed with others\n",
    "    baseline = seed_results[f'seed_{test_seeds[0]}']\n",
    "    print(f\"\\nComparison with seed {test_seeds[0]} (baseline):\")\n",
    "    \n",
    "    for seed in test_seeds[1:]:\n",
    "        current = seed_results[f'seed_{seed}']\n",
    "        \n",
    "        # Calculate overlap\n",
    "        baseline_set = set(baseline)\n",
    "        current_set = set(current)\n",
    "        \n",
    "        overlap = baseline_set & current_set\n",
    "        only_baseline = baseline_set - current_set\n",
    "        only_current = current_set - baseline_set\n",
    "        \n",
    "        overlap_pct = len(overlap) / len(baseline) * 100\n",
    "        \n",
    "        print(f\"  Seed {seed}: {len(overlap)}/256 overlap ({overlap_pct:.1f}%), \"\n",
    "              f\"{len(only_current)} new terms, {len(only_baseline)} removed\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Summary complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe547e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed analysis for protein: O33283\n",
      "Original term count: 1\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Seed 42 vs Seed 123:\n",
      "  Terms in common: 3/256\n",
      "  Added in seed 123: 0\n",
      "  Removed in seed 123: 0\n",
      "\n",
      "Seed 42 vs Seed 456:\n",
      "  Terms in common: 2/256\n",
      "  Added in seed 456: 1\n",
      "    Examples: [np.str_('GO:0016803')]\n",
      "  Removed in seed 456: 1\n",
      "    Examples: [np.str_('GO:0008096')]\n",
      "\n",
      "Seed 42 vs Seed 789:\n",
      "  Terms in common: 3/256\n",
      "  Added in seed 789: 0\n",
      "  Removed in seed 789: 0\n",
      "\n",
      "Seed 42 vs Seed 999:\n",
      "  Terms in common: 2/256\n",
      "  Added in seed 999: 1\n",
      "    Examples: [np.str_('GO:0019118')]\n",
      "  Removed in seed 999: 1\n",
      "    Examples: [np.str_('GO:0008096')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Detailed comparison: Show which terms are different\n",
    "sample_idx = 0  # Look at first test sample\n",
    "sample_result = results[sample_idx]\n",
    "\n",
    "print(f\"Detailed analysis for protein: {sample_result['qseqid']}\")\n",
    "print(f\"Original term count: {sample_result['original_count']}\")\n",
    "print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "baseline_seed = test_seeds[0]\n",
    "baseline_terms = set(sample_result[f'seed_{baseline_seed}'])\n",
    "\n",
    "for seed in test_seeds[1:]:\n",
    "    current_terms = set(sample_result[f'seed_{seed}'])\n",
    "    \n",
    "    added = current_terms - baseline_terms\n",
    "    removed = baseline_terms - current_terms\n",
    "    \n",
    "    print(f\"Seed {baseline_seed} vs Seed {seed}:\")\n",
    "    print(f\"  Terms in common: {len(baseline_terms & current_terms)}/256\")\n",
    "    print(f\"  Added in seed {seed}: {len(added)}\")\n",
    "    if added:\n",
    "        print(f\"    Examples: {list(added)[:5]}\")\n",
    "    print(f\"  Removed in seed {seed}: {len(removed)}\")\n",
    "    if removed:\n",
    "        print(f\"    Examples: {list(removed)[:5]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a126ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average overlap across all samples: 73.30%\n",
      "Std deviation: 33.55%\n",
      "Min overlap: 19.92%\n",
      "Max overlap: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Visualize the variability across all test samples\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate pairwise overlaps for each sample\n",
    "overlap_data = []\n",
    "\n",
    "for result in results:\n",
    "    sample_overlaps = []\n",
    "    baseline = set(result[f'seed_{test_seeds[0]}'])\n",
    "    \n",
    "    for seed in test_seeds[1:]:\n",
    "        current = set(result[f'seed_{seed}'])\n",
    "        overlap_pct = len(baseline & current) / 256 * 100\n",
    "        sample_overlaps.append(overlap_pct)\n",
    "    \n",
    "    overlap_data.append(sample_overlaps)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for i, overlaps in enumerate(overlap_data):\n",
    "    ax.plot(test_seeds[1:], overlaps, marker='o', label=f'Sample {i+1}')\n",
    "\n",
    "ax.set_xlabel('Random Seed')\n",
    "ax.set_ylabel('Overlap with Baseline (seed 42) %')\n",
    "ax.set_title('Random Walk Variability Across Different Seeds\\n(For Proteins with Rare Terms)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage overlap across all samples: {np.mean(overlap_data):.2f}%\")\n",
    "print(f\"Std deviation: {np.std(overlap_data):.2f}%\")\n",
    "print(f\"Min overlap: {np.min(overlap_data):.2f}%\")\n",
    "print(f\"Max overlap: {np.max(overlap_data):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9bdf6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82201, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_2_terms = pd.read_parquet(data_paths['knn_terms_df'], engine='fastparquet')\n",
    "train_terms = pd.read_csv(data_paths['train_terms_df'], sep='\\t')\n",
    "seq_2_terms.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c1b84d",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c3cac41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EntryID</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A023FBW4</td>\n",
       "      <td>{GO:0019958}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A023FBW7</td>\n",
       "      <td>{GO:0019957}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A023FDY8</td>\n",
       "      <td>{GO:0019957}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A023FF81</td>\n",
       "      <td>{GO:0019958}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A023FFB5</td>\n",
       "      <td>{GO:0019957}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      EntryID          term\n",
       "0  A0A023FBW4  {GO:0019958}\n",
       "1  A0A023FBW7  {GO:0019957}\n",
       "2  A0A023FDY8  {GO:0019957}\n",
       "3  A0A023FF81  {GO:0019958}\n",
       "4  A0A023FFB5  {GO:0019957}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_terms_grouped = train_terms.groupby('EntryID')['term'].apply(set).reset_index()\n",
    "train_terms_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3153f0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EntryID</th>\n",
       "      <th>term</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A023FBW4</td>\n",
       "      <td>{GO:0019958}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A023FBW7</td>\n",
       "      <td>{GO:0019957}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A023FDY8</td>\n",
       "      <td>{GO:0019957}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A023FF81</td>\n",
       "      <td>{GO:0019958}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A023FFB5</td>\n",
       "      <td>{GO:0019957}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82399</th>\n",
       "      <td>X2JI34</td>\n",
       "      <td>{GO:0051762, GO:0106223}</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82400</th>\n",
       "      <td>X4Y2L4</td>\n",
       "      <td>{GO:0030214, GO:0033906}</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82401</th>\n",
       "      <td>X5JA13</td>\n",
       "      <td>{GO:0009506, GO:0070062, GO:0009536, GO:000588...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82402</th>\n",
       "      <td>X5JB51</td>\n",
       "      <td>{GO:0009506, GO:0070062, GO:0005886, GO:000014...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82403</th>\n",
       "      <td>X5M5N0</td>\n",
       "      <td>{GO:0006972, GO:0140694, GO:0005515, GO:014069...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82404 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          EntryID                                               term  len\n",
       "0      A0A023FBW4                                       {GO:0019958}    1\n",
       "1      A0A023FBW7                                       {GO:0019957}    1\n",
       "2      A0A023FDY8                                       {GO:0019957}    1\n",
       "3      A0A023FF81                                       {GO:0019958}    1\n",
       "4      A0A023FFB5                                       {GO:0019957}    1\n",
       "...           ...                                                ...  ...\n",
       "82399      X2JI34                           {GO:0051762, GO:0106223}    2\n",
       "82400      X4Y2L4                           {GO:0030214, GO:0033906}    2\n",
       "82401      X5JA13  {GO:0009506, GO:0070062, GO:0009536, GO:000588...    8\n",
       "82402      X5JB51  {GO:0009506, GO:0070062, GO:0005886, GO:000014...    8\n",
       "82403      X5M5N0  {GO:0006972, GO:0140694, GO:0005515, GO:014069...    8\n",
       "\n",
       "[82404 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get label frequency \n",
    "label_frequency = train_terms['term'].value_counts().to_dict()\n",
    "\n",
    "#get number of labels for each instance\n",
    "train_terms_grouped['len']  = train_terms_grouped['term'].apply(len)\n",
    "train_terms_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "518deb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance with rare labels: 31786, Instances with <= 10 total labels: 68811\n",
      "Instances with rare labels and <= 10 total labels: 21098 = 25.60%\n"
     ]
    }
   ],
   "source": [
    "#create mask for rare labels\n",
    "rare_label_threshold = 10\n",
    "rare_labels = {term for term, freq in label_frequency.items() if freq <= rare_label_threshold}\n",
    "\n",
    "rare_labels_mask = train_terms_grouped['term'].apply(lambda terms: any(term in rare_labels for term in terms))\n",
    "rare_labels_mask.sum()\n",
    "\n",
    "len_threshold = 10\n",
    "len_mask = train_terms_grouped['len'] <= len_threshold\n",
    "len_mask.sum()\n",
    "\n",
    "\n",
    "combined_mask = rare_labels_mask & len_mask\n",
    "print(f\"Instance with rare labels: {rare_labels_mask.sum()}, Instances with <= {len_threshold} total labels: {len_mask.sum()}\")\n",
    "print(f\"Instances with rare labels and <= {len_threshold} total labels: {combined_mask.sum()} = {combined_mask.sum() / len(train_terms_grouped) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aab7060a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_freq</th>\n",
       "      <th>num_label</th>\n",
       "      <th>factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_freq  num_label  factor\n",
       "0           5          5       8\n",
       "1          10         10       4\n",
       "2          20         20       2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_segments = { \n",
    "    'label_freq' : [5, 10, 20],\n",
    "    'num_label'  : [5, 10, 20],\n",
    "    'factor'     : [8, 4, 2]\n",
    "}\n",
    "split_segments_df = pd.DataFrame(split_segments)\n",
    "split_segments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846cdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in split_segments_df.index:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b5613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_segments_df.iloc[0]split_segments_df.iloc[0]['label_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31fcb4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RangeIndex' object has no attribute 'reverse'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msplit_segments_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreverse\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'RangeIndex' object has no attribute 'reverse'"
     ]
    }
   ],
   "source": [
    "split_segments_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f581d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances with rare labels < 20 and <= 20 total labels: 39193 = 47.56% \t factor 2\n",
      "Instances with rare labels < 10 and <= 10 total labels: 21098 = 25.60% \t factor 4\n",
      "Instances with rare labels < 5 and <= 5 total labels: 6394 = 7.76% \t factor 8\n"
     ]
    }
   ],
   "source": [
    "#create mask for rare labels\n",
    "\n",
    "train_terms_grouped['sampling_factor'] = 1\n",
    "\n",
    "for i in range(len(split_segments_df) -1, -1, -1 ):\n",
    "\n",
    "    row = split_segments_df.iloc[i]\n",
    "    rare_label_threshold = row['label_freq']\n",
    "    len_threshold = row['num_label']\n",
    "\n",
    "    rare_labels = {term for term, freq in label_frequency.items() if freq <= rare_label_threshold}\n",
    "\n",
    "    rare_labels_mask = train_terms_grouped['term'].apply(lambda terms: any(term in rare_labels for term in terms))\n",
    "    rare_labels_mask.sum()\n",
    "\n",
    "    \n",
    "    len_mask = train_terms_grouped['len'] <= len_threshold\n",
    "    len_mask.sum()\n",
    "\n",
    "    \n",
    "    combined_mask = rare_labels_mask & len_mask\n",
    "    train_terms_grouped['sampling_factor'] = np.where(combined_mask, row['factor'], train_terms_grouped['sampling_factor'])\n",
    "    # print(f\"Instance with rare labels  < {rare_label_threshold}: {rare_labels_mask.sum()}, Instances with <= {len_threshold} total labels: {len_mask.sum()}\")\n",
    "    print(f\"Instances with rare labels < {rare_label_threshold} and <= {len_threshold} total labels: {combined_mask.sum()} = {combined_mask.sum() / len(train_terms_grouped) * 100:.2f}% \\t factor { row['factor']}\")\n",
    "\n",
    "# sampling_dict =dict(zip(train_terms_grouped['EntryID'], train_terms_grouped['sampling_factor']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f66dab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(189369)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_terms_grouped['sampling_factor'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca1b4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_dict =dict(zip(train_terms_grouped['EntryID'], train_terms_grouped['sampling_factor']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "827ad65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save sampling dict\n",
    "import pickle\n",
    "with open('./sampling_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(sampling_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2f0a579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampling_factor\n",
       "1    0.524380\n",
       "2    0.219589\n",
       "4    0.178438\n",
       "8    0.077593\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_terms_grouped['sampling_factor'].value_counts(normalize=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c270b",
   "metadata": {},
   "source": [
    "rare    len                 factor                    addiction     max term addition\n",
    "10      10      =  25.6%    x4     max=(4 * 10 = 30)  102.4%        +768%\n",
    "20      20      =  21.96%   X2     max=(2 * 20 = 20)  26.96%        +439% \n",
    "Total                                                 129%          +1207% \n",
    "\n",
    "Not all are in "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67de8f6",
   "metadata": {},
   "source": [
    "### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b56a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(row):\n",
    "    \"\"\"Check what percentage of train_terms_all are in terms_predicted\"\"\"\n",
    "    # Check if train_terms_all is None or empty list\n",
    "    true_key  = 'terms_true'\n",
    "    if row[true_key] is None or (isinstance(row[true_key], list) and len(row[true_key]) == 0):\n",
    "        return None\n",
    "    \n",
    "    train_set = set(row[true_key])\n",
    "    pred_set = set(row['terms_predicted'])\n",
    "    \n",
    "    # Find terms in train_terms_all that are in terms_predicted\n",
    "    covered_terms = train_set.intersection(pred_set)\n",
    "    \n",
    "    # Calculate coverage percentage\n",
    "    coverage = len(covered_terms) / len(train_set) * 100 if len(train_set) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total_train_terms': len(train_set),\n",
    "        'covered_terms': len(covered_terms),\n",
    "        'missing_terms': len(train_set) - len(covered_terms),\n",
    "        'coverage_pct': coverage,\n",
    "        'missing_term_list': list(train_set - covered_terms)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58fa132e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qseqid</th>\n",
       "      <th>terms_predicted</th>\n",
       "      <th>terms_true</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A023FBW4</td>\n",
       "      <td>[GO:0019958, GO:0005576]</td>\n",
       "      <td>[GO:0019958]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A023FBW7</td>\n",
       "      <td>[GO:0019957, GO:0005576]</td>\n",
       "      <td>[GO:0019957]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A023FDY8</td>\n",
       "      <td>[GO:0019957, GO:0005576]</td>\n",
       "      <td>[GO:0019957]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A023FF81</td>\n",
       "      <td>[GO:0019958, GO:0005576]</td>\n",
       "      <td>[GO:0019958]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A023FFB5</td>\n",
       "      <td>[]</td>\n",
       "      <td>[GO:0019957]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qseqid           terms_predicted    terms_true  len\n",
       "0  A0A023FBW4  [GO:0019958, GO:0005576]  [GO:0019958]    2\n",
       "1  A0A023FBW7  [GO:0019957, GO:0005576]  [GO:0019957]    2\n",
       "2  A0A023FDY8  [GO:0019957, GO:0005576]  [GO:0019957]    2\n",
       "3  A0A023FF81  [GO:0019958, GO:0005576]  [GO:0019958]    2\n",
       "4  A0A023FFB5                        []  [GO:0019957]    0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_paths = {\n",
    "    'knn_terms_df':         '/mnt/d/ML/Kaggle/CAFA6-new/uniprot/diamond_knn_predictions.parquet',\n",
    "    'train_terms_df':       '/mnt/d/ML/Kaggle/CAFA6/cafa-6-protein-function-prediction/Train/train_terms.tsv',\n",
    "    'go_obo_path':          '/mnt/d/ML/Kaggle/CAFA6/cafa-6-protein-function-prediction/Train/go-basic.obo',\n",
    "    'features_embeds_path': '/mnt/d/ML/Kaggle/CAFA6-new/Dataset/esm_t33_650M/train_embeds.npy',\n",
    "    'features_ids_path':    '/mnt/d/ML/Kaggle/CAFA6-new/Dataset/esm_t33_650M/train_ids.npy',\n",
    "    'go_embeds_paths':      '/mnt/d/ML/Kaggle/CAFA6-new/uniprot/go_embeddings.pkl'\n",
    "}\n",
    "\n",
    "seq_2_terms = pd.read_parquet(data_paths['knn_terms_df'], engine='fastparquet')\n",
    "\n",
    "seq_2_terms['len'] = seq_2_terms['terms_predicted'].apply(len)\n",
    "seq_2_terms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7da784cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_results= seq_2_terms.apply(check_coverage, axis=1)\n",
    "coverage_df = pd.DataFrame(coverage_results.tolist())\n",
    "\n",
    "# seq_2_terms['coverage'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e4c65fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average coverage: 89.30%\n",
      "Median coverage: 100.00%\n",
      "Instances with 100% coverage: 68712 / 82201\n",
      "Instances with 0% coverage: 6836 / 82201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7389fe99d700>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Average coverage: {coverage_df['coverage_pct'].mean():.2f}%\")\n",
    "print(f\"Median coverage: {coverage_df['coverage_pct'].median():.2f}%\")\n",
    "print(f\"Instances with 100% coverage: {(coverage_df['coverage_pct'] == 100).sum()} / {len(coverage_df)}\")\n",
    "print(f\"Instances with 0% coverage: {(coverage_df['coverage_pct'] == 0).sum()} / {len(coverage_df)}\")\n",
    "plt.plot(coverage_df['coverage_pct'].sort_values().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51035053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cafa6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
