{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27d7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import obonet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30fa9f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def print_size(data):\n",
    "    print(f'{sys.getsizeof(data) / (1024 * 1024):.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e832a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_terms_with_neighbors(terms, go_graph, go_embeds, max_terms=256):\n",
    "    \"\"\"\n",
    "    Pad a list of GO terms with neighboring terms from the GO graph.\n",
    "    \n",
    "    Args:\n",
    "        terms: List of GO term IDs\n",
    "        go_graph: networkx graph of GO ontology\n",
    "        go_embeds: Dictionary of GO embeddings (to check if term has embedding)\n",
    "        max_terms: Maximum number of terms to return\n",
    "    \n",
    "    Returns:\n",
    "        List of GO terms padded to max_terms\n",
    "    \"\"\"\n",
    "    padded_terms = list(terms)\n",
    "    \n",
    "    if len(padded_terms) >= max_terms:\n",
    "        return padded_terms[:max_terms]\n",
    "    \n",
    "    # Use a set for faster lookup\n",
    "    terms_set = set(padded_terms)\n",
    "    \n",
    "    # Collect neighbors from existing terms\n",
    "    candidates = set()\n",
    "    for term in padded_terms:\n",
    "        if term in go_graph:\n",
    "            # Get parents (is_a relationships)\n",
    "            if 'is_a' in go_graph.nodes[term]:\n",
    "                parents = go_graph.nodes[term].get('is_a', [])\n",
    "                if isinstance(parents, str):\n",
    "                    parents = [parents]\n",
    "                candidates.update(parents)\n",
    "            \n",
    "            # Get successors (children) and predecessors (parents)\n",
    "            candidates.update(go_graph.successors(term))\n",
    "            candidates.update(go_graph.predecessors(term))\n",
    "    \n",
    "    # Remove terms already in the list\n",
    "    candidates = candidates - terms_set\n",
    "    \n",
    "    # Filter candidates to only those with embeddings\n",
    "    candidates = [c for c in candidates if c in go_embeds]\n",
    "    \n",
    "    # Add candidates until we reach max_terms\n",
    "    for candidate in candidates:\n",
    "        if len(padded_terms) >= max_terms:\n",
    "            break\n",
    "        padded_terms.append(candidate)\n",
    "        terms_set.add(candidate)\n",
    "    \n",
    "    # If still not enough, try neighbors of neighbors\n",
    "    if len(padded_terms) < max_terms:\n",
    "        second_level_candidates = set()\n",
    "        for term in candidates[:100]:  # Limit to avoid too much computation\n",
    "            if term in go_graph:\n",
    "                if 'is_a' in go_graph.nodes[term]:\n",
    "                    parents = go_graph.nodes[term].get('is_a', [])\n",
    "                    if isinstance(parents, str):\n",
    "                        parents = [parents]\n",
    "                    second_level_candidates.update(parents)\n",
    "                second_level_candidates.update(go_graph.successors(term))\n",
    "                second_level_candidates.update(go_graph.predecessors(term))\n",
    "        \n",
    "        second_level_candidates = second_level_candidates - terms_set\n",
    "        second_level_candidates = [c for c in second_level_candidates if c in go_embeds]\n",
    "        \n",
    "        for candidate in second_level_candidates:\n",
    "            if len(padded_terms) >= max_terms:\n",
    "                break\n",
    "            padded_terms.append(candidate)\n",
    "    \n",
    "    return padded_terms\n",
    "\n",
    "def pad_dataframe_terms(seq_2_terms_df, go_graph, go_embeds, max_terms=256):\n",
    "    \"\"\"\n",
    "    Pad the terms_predicted column in the dataframe in-place using GO graph neighbors.\n",
    "    \n",
    "    Args:\n",
    "        seq_2_terms_df: DataFrame with 'terms_predicted' column\n",
    "        go_graph: networkx graph of GO ontology\n",
    "        go_embeds: Dictionary of GO embeddings\n",
    "        max_terms: Maximum number of terms per row\n",
    "    \"\"\"\n",
    "    print(\"Padding terms_predicted with GO graph neighbors...\")\n",
    "    seq_2_terms_df['terms_predicted'] = seq_2_terms_df['terms_predicted'].apply(\n",
    "        lambda terms: pad_terms_with_neighbors(terms, go_graph, go_embeds, max_terms)\n",
    "    )\n",
    "    print(f\"Padding complete. Average terms per row: {seq_2_terms_df['terms_predicted'].apply(len).mean():.2f}\")\n",
    "    return seq_2_terms_df\n",
    "\n",
    "def prepare_data(data_paths, max_terms=256, aspect=None):\n",
    "    \n",
    "    knn_terms_df = data_paths['knn_terms_df']\n",
    "    train_terms_df = data_paths['train_terms_df']\n",
    "    features_embeds_path = data_paths['features_embeds_path']\n",
    "    features_ids_path = data_paths['features_ids_path']\n",
    "\n",
    "    go_embeds_paths = data_paths['go_embeds_paths']\n",
    "\n",
    "    seq_2_terms = pd.read_parquet(knn_terms_df, engine='fastparquet')\n",
    "    train_terms = pd.read_csv(train_terms_df, sep='\\t')\n",
    "\n",
    "    term_to_aspect = train_terms.groupby('term')['aspect'].first().to_dict()\n",
    "\n",
    "    go_graph = obonet.read_obo(data_paths['go_obo_path'])\n",
    "        \n",
    "    with open(go_embeds_paths, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        embeddings_dict = data['embeddings']\n",
    "        go_ids = data['go_ids']\n",
    "\n",
    "    # Filter to keep only terms from a specific aspect if aspect is provided\n",
    "    if aspect is not None:\n",
    "        seq_2_terms['terms_predicted'] = seq_2_terms['terms_predicted'].apply(\n",
    "            lambda terms: [t for t in terms if term_to_aspect.get(t) == aspect]\n",
    "        )\n",
    "        seq_2_terms['terms_true'] = seq_2_terms['terms_true'].apply(\n",
    "            lambda terms: [t for t in terms if term_to_aspect.get(t) == aspect]\n",
    "        )\n",
    "        # Remove rows where terms_predicted or terms_true is now empty\n",
    "        seq_2_terms = seq_2_terms[seq_2_terms['terms_predicted'].apply(len) > 0]\n",
    "        seq_2_terms = seq_2_terms[seq_2_terms['terms_true'].apply(len) > 0]\n",
    "\n",
    "\n",
    "    features_embeds = np.load(features_embeds_path, allow_pickle=True)\n",
    "    features_ids = np.load(features_ids_path, allow_pickle=True)\n",
    "\n",
    "    features_embeds_dict = {feat_id: embed for feat_id, embed in zip(features_ids, features_embeds)}\n",
    "\n",
    "    # Pad terms_predicted in the dataframe with GO graph neighbors\n",
    "    seq_2_terms = pad_dataframe_terms(seq_2_terms, go_graph, embeddings_dict, max_terms=max_terms)\n",
    "    #remove seq_2_terms row for which len(predicted_terms == 0)\n",
    "\n",
    "    term_lengths = seq_2_terms['terms_predicted'].apply(len)\n",
    "\n",
    "    #currently only using sequences with 256 terms, need to change later \n",
    "    seq_2_terms = seq_2_terms[term_lengths == max_terms]\n",
    "\n",
    "    train_ids =  pd.DataFrame(features_ids, columns=['qseqid'])\n",
    "    seq_2_terms = seq_2_terms.merge(train_ids, on='qseqid', how='inner')    \n",
    "\n",
    "    out = {'seq_2_terms': seq_2_terms,\n",
    "           'train_terms': train_terms,\n",
    "           'features_embeds': features_embeds_dict,\n",
    "           'go_embeds': embeddings_dict,\n",
    "           'go_graph': go_graph\n",
    "           }\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfccc0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding terms_predicted with GO graph neighbors...\n",
      "Padding complete. Average terms per row: 185.86\n"
     ]
    }
   ],
   "source": [
    "data_paths = {\n",
    "    'knn_terms_df':         '/mnt/d/ML/Kaggle/CAFA6-new/uniprot/diamond_knn_predictions.parquet',\n",
    "    'train_terms_df':       '/mnt/d/ML/Kaggle/CAFA6/cafa-6-protein-function-prediction/Train/train_terms.tsv',\n",
    "    'go_obo_path':          '/mnt/d/ML/Kaggle/CAFA6/cafa-6-protein-function-prediction/Train/go-basic.obo',\n",
    "    'features_embeds_path': '/mnt/d/ML/Kaggle/CAFA6-new/Dataset/esm_t33_650M/train_embeds.npy',\n",
    "    'features_ids_path':    '/mnt/d/ML/Kaggle/CAFA6-new/Dataset/esm_t33_650M/train_ids.npy',\n",
    "    'go_embeds_paths':      '/mnt/d/ML/Kaggle/CAFA6-new/uniprot/go_embeddings.pkl'\n",
    "}\n",
    "data = prepare_data(data_paths, max_terms=192, aspect='P')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03672f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C max term 42\n",
    "# F max terms 34\n",
    "# P max terms 188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c83dbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50404, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['seq_2_terms'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf8d1c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "num_weak = nx.number_weakly_connected_components(data['go_graph'])\n",
    "print(num_weak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f043c4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6220611\n",
      "0.67442715\n",
      "-0.42725486\n"
     ]
    }
   ],
   "source": [
    "embed1 = data['go_embeds']['GO:0000001'] #P\n",
    "embed2 = data['go_embeds']['GO:0000006'] #F\n",
    "\n",
    "embed3 = data['go_embeds']['GO:0000785'] #C\n",
    "print(np.dot(embed1, embed2))\n",
    "print(np.dot(embed2, embed3))\n",
    "print(np.dot(embed1, embed3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "895c85ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min terms: 192\n",
      "Max terms: 192\n",
      "Mean terms: 192.00\n",
      "Median terms: 192.00\n",
      "No. empty predicted terms: 0\n",
      "\n",
      "Sample row with padded terms:\n",
      "Number of predicted terms: 192\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of terms_predicted lengths after padding\n",
    "term_lengths = data['seq_2_terms']['terms_predicted'].apply(len)\n",
    "print(f\"Min terms: {term_lengths.min()}\")\n",
    "print(f\"Max terms: {term_lengths.max()}\")\n",
    "print(f\"Mean terms: {term_lengths.mean():.2f}\")\n",
    "print(f\"Median terms: {term_lengths.median():.2f}\")\n",
    "print(f\"No. empty predicted terms: {(term_lengths == 0).sum()}\")\n",
    "print(f\"\\nSample row with padded terms:\")\n",
    "print(f\"Number of predicted terms: {len(data['seq_2_terms'].iloc[0]['terms_predicted'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "705dc874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min terms: 1\n",
      "Max terms: 188\n",
      "Mean terms: 4.57\n",
      "Median terms: 3.00\n",
      "No. empty predicted terms: 0\n",
      "\n",
      "Sample row with padded terms:\n",
      "Number of predicted terms: 192\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of terms_predicted lengths after padding\n",
    "term_lengths = data['seq_2_terms']['terms_true'].apply(len)\n",
    "print(f\"Min terms: {term_lengths.min()}\")\n",
    "print(f\"Max terms: {term_lengths.max()}\")\n",
    "print(f\"Mean terms: {term_lengths.mean():.2f}\")\n",
    "print(f\"Median terms: {term_lengths.median():.2f}\")\n",
    "print(f\"No. empty predicted terms: {(term_lengths == 0).sum()}\")\n",
    "print(f\"\\nSample row with padded terms:\")\n",
    "print(f\"Number of predicted terms: {len(data['seq_2_terms'].iloc[0]['terms_predicted'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a8b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "508afac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 512)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = data['seq_2_terms'].iloc[0]\n",
    "\n",
    "feature_embed = data['features_embeds'][row['qseqid']]\n",
    "\n",
    "go_embeds = np.array([data['go_embeds'][go_id] for go_id in row['terms_predicted']])\n",
    "go_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f275bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsDataset(Dataset):\n",
    "    \"\"\"Dataset that yields raw embeddings; tokenization is done in collate_fn for batching.\"\"\"\n",
    "    def __init__(self, \n",
    "                 data, \n",
    "                 max_go_embeds = 256,  \n",
    "                 oversample_indices=None\n",
    "                ):\n",
    "        \n",
    "        self.data = data\n",
    "        self.max_go_embeds = max_go_embeds\n",
    "        self.oversample_indices = oversample_indices if oversample_indices is not None else list(range(len(self.data['seq_2_terms'])))\n",
    "        self.mask_embed = np.zeros(next(iter(self.data['go_embeds'].values())).shape, dtype=np.float32)\n",
    "        #ensure len of predicted go terms is less than max_go_embeds\n",
    "        #self.data['seq_2_terms'] = self.data['seq_2_terms'][self\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.oversample_indices)         \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx = self.oversample_indices[idx]\n",
    "\n",
    "        row = self.data['seq_2_terms'].iloc[sample_idx]\n",
    "        qseqid = row['qseqid']\n",
    "\n",
    "        feature_embed = self.data['features_embeds'][qseqid]\n",
    "\n",
    "        true_terms_set = set(row['terms_true'])\n",
    "        predicted_terms = row['terms_predicted']\n",
    "        \n",
    "        # Filter terms that have embeddings (should be all of them after padding)\n",
    "        # valid_terms = [term for term in predicted_terms if term in self.data['go_embeds']]\n",
    "        valid_terms = predicted_terms\n",
    "        # Vectorized operations using list comprehensions\n",
    "        go_embeds = np.array([self.data['go_embeds'].get(term, self.mask_embed) for term in valid_terms])\n",
    "        label = np.array([term in true_terms_set for term in valid_terms], dtype=np.float32)\n",
    "        \n",
    "        return {\n",
    "            'entryID'   : qseqid,\n",
    "            'feature'   : feature_embed,\n",
    "            'go_embed'  : go_embeds,\n",
    "            'label'     : label,\n",
    "            'predicted_terms': valid_terms,\n",
    "            'true_terms': row['terms_true']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3592df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EmbeddingsDataset(data)\n",
    "\n",
    "aa = dataset[1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bef64d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50fd95ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07045657, -0.00120772,  0.02064155, ..., -0.09901655,\n",
       "       -0.0983785 ,  0.11195755], shape=(1280,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['features_embeds']['A0A023FBW4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4681f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_orthogonal_vectors_qr(dimension, num_vectors=None):\n",
    "    \"\"\"\n",
    "    Generates a set of orthogonal vectors using QR factorization.\n",
    "    \n",
    "    Args:\n",
    "        dimension (int): The dimensionality of the vectors.\n",
    "        num_vectors (int, optional): The number of vectors to generate. \n",
    "                                     Defaults to 'dimension' for a full basis.\n",
    "    Returns:\n",
    "        numpy.ndarray: A matrix where each column is an orthogonal vector.\n",
    "    \"\"\"\n",
    "    if num_vectors is None:\n",
    "        num_vectors = dimension\n",
    "        \n",
    "    # Generate a random matrix\n",
    "    A = np.random.rand(dimension, num_vectors)\n",
    "    \n",
    "    # Perform QR factorization\n",
    "    # The 'Q' matrix contains the orthogonal columns\n",
    "    Q, R = np.linalg.qr(A)\n",
    "    \n",
    "    # Return the first 'num_vectors' columns of Q\n",
    "    return Q[:, :num_vectors]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "028807d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dot product of the first two vectors: -1.734723475976807e-18\n",
      "(1280, 1280)\n"
     ]
    }
   ],
   "source": [
    "vectors = generate_orthogonal_vectors_qr(1280 ) #, num_vectors=512)\n",
    "\n",
    "# Verification (dot product of any two distinct columns should be near zero):\n",
    "print(\"\\nDot product of the first two vectors:\", np.dot(vectors[:, 0], vectors[:, 1]))\n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c761773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 512)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "307e06bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmbedTokenizer(nn.Module):\n",
    "    def __init__(self, D, d, N, rng=None):  \n",
    "        super(EmbedTokenizer, self).__init__()\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        D : int\n",
    "            Dimension of the embedding space.\n",
    "        d : int\n",
    "            Dimension of the token space.\n",
    "        N : int\n",
    "            Number of tokens.\n",
    "        rng : np.random.Generator or None\n",
    "            Random generator. If None, use default.\n",
    "        \"\"\"\n",
    "        if rng is None:\n",
    "            rng = np.random.default_rng()\n",
    "\n",
    "        V = generate_orthogonal_vectors_qr(D)\n",
    "        K = D\n",
    "        # Build P as a single tensor of shape (N, d, D) and register as buffer so it's moved with .to()\n",
    "        P_list = []\n",
    "        for i in range(N):\n",
    "            indices = np.arange(D)\n",
    "            sampled_idx = np.random.choice(indices, size=d, replace=False)\n",
    "            p = V[:, sampled_idx].T\n",
    "            P_list.append(p)\n",
    "\n",
    "        P_np = np.stack(P_list, axis=0).astype(np.float32)  # (N, d, D)\n",
    "        P_tensor = torch.from_numpy(P_np)\n",
    "        # register as buffer so it's not a parameter but moves with the module\n",
    "        self.register_buffer('P_buffer', P_tensor)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Tensor, shape (D)\n",
    "            Input embeddings.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tokens : Tensor, shape (batch_size, N, d)\n",
    "            Token representations.\n",
    "        \"\"\"\n",
    "        # x: (batch_size, D) or (D,) -> ensure batch\n",
    "        squeeze_output = False\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "            squeeze_output = True\n",
    "\n",
    "        # Get P and move to correct device/dtype\n",
    "        P = self.P_buffer.to(dtype=x.dtype, device=x.device)  # (N, d, D)\n",
    "\n",
    "        # Vectorized matmul: (batch_size, D) @ (D, N*d) -> (batch_size, N*d) -> reshape\n",
    "        D = x.shape[1]\n",
    "        P_2d = P.permute(2, 0, 1).reshape(D, -1)  # (D, N*d)\n",
    "        tokens = torch.matmul(x, P_2d)  # (batch_size, N*d)\n",
    "        tokens = tokens.reshape(x.shape[0], P.shape[0], P.shape[1])  # (batch_size, N, d)\n",
    "\n",
    "        if squeeze_output:\n",
    "            tokens = tokens.squeeze(0)\n",
    "\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a882a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = EmbedTokenizer(D=1280, d=512, N=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3099468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3775,  0.2384,  0.0141,  ...,  0.0190, -0.1658,  0.2868],\n",
       "          [ 0.2066, -0.4773, -0.2407,  ..., -0.3053,  0.0404, -0.0880],\n",
       "          [ 0.1087,  0.3423,  0.3232,  ..., -0.2379, -0.0897, -0.1857],\n",
       "          ...,\n",
       "          [ 0.1216, -0.1268, -0.1314,  ...,  0.0746, -0.1035,  0.2956],\n",
       "          [-0.2053, -0.3775,  0.0284,  ...,  0.2384, -0.3817,  0.0690],\n",
       "          [-0.0076, -0.1326,  0.1210,  ..., -0.3185, -0.3413, -0.1075]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(torch.tensor(aa['feature'], dtype=torch.float32).to(device).unsqueeze(0)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10afa4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['features_embeds'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9beff280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_tokenize(batch, tokenizer, device=None, dtype=torch.float32):\n",
    "    \"\"\"Custom collate function to handle variable-length data.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of samples from the dataset\n",
    "        tokenizer: Tokenizer to apply to features\n",
    "        device: Device to move tensors to (cuda or cpu)\n",
    "        dtype: Target dtype for tensors (torch.float32, torch.float16, or torch.bfloat16)\n",
    "    \"\"\"\n",
    "    features = torch.stack([torch.from_numpy(item['feature']) for item in batch])\n",
    "    features = features.to(dtype=dtype, device=device) if device else features.to(dtype=dtype)\n",
    "    features = tokenizer(features)\n",
    "    \n",
    "    go_embed = torch.stack([torch.from_numpy(item['go_embed']) for item in batch])\n",
    "    go_embed = go_embed.to(dtype=dtype, device=device) if device else go_embed.to(dtype=dtype)\n",
    "    \n",
    "    label = torch.stack([torch.from_numpy(item['label']) for item in batch])\n",
    "    label = label.to(dtype=dtype, device=device) if device else label.to(dtype=dtype)\n",
    "    \n",
    "    return {\n",
    "        'entryID': [item['entryID'] for item in batch],\n",
    "        'feature': features,\n",
    "        'go_embed': go_embed,\n",
    "        'label': label,\n",
    "        'predicted_terms': [item['predicted_terms'] for item in batch],\n",
    "        'true_terms': [item['true_terms'] for item in batch]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4952f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options: torch.float32 (default), torch.float16, torch.bfloat16\n",
    "target_dtype = torch.float32  # Change this to torch.float16 or torch.bfloat16 if needed\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=lambda b: collate_tokenize(b, tokenizer, device, dtype=target_dtype)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c23812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefetchLoader:\n",
    "    \"\"\"\n",
    "    Prefetch loader that loads batches asynchronously to GPU for faster training.\n",
    "    Overlaps data transfer with computation by loading the next batch while \n",
    "    the model processes the current batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        self.stream = torch.cuda.Stream() if device.type == 'cuda' else None\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.stream is not None:\n",
    "            # CUDA prefetching\n",
    "            return self._cuda_iter()\n",
    "        else:\n",
    "            # CPU fallback - no prefetching needed\n",
    "            return iter(self.dataloader)\n",
    "    \n",
    "    def _cuda_iter(self):\n",
    "        \"\"\"Iterator with CUDA stream prefetching.\"\"\"\n",
    "        loader_iter = iter(self.dataloader)\n",
    "        \n",
    "        # Preload first batch\n",
    "        try:\n",
    "            with torch.cuda.stream(self.stream):\n",
    "                next_batch = next(loader_iter)\n",
    "                next_batch = self._to_device(next_batch)\n",
    "        except StopIteration:\n",
    "            return\n",
    "        \n",
    "        while True:\n",
    "            # Wait for the prefetch stream to finish\n",
    "            torch.cuda.current_stream().wait_stream(self.stream)\n",
    "            batch = next_batch\n",
    "            \n",
    "            # Make sure tensors are ready before yielding\n",
    "            if isinstance(batch, dict):\n",
    "                for k, v in batch.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        v.record_stream(torch.cuda.current_stream())\n",
    "            \n",
    "            # Start loading next batch in background\n",
    "            try:\n",
    "                with torch.cuda.stream(self.stream):\n",
    "                    next_batch = next(loader_iter)\n",
    "                    next_batch = self._to_device(next_batch)\n",
    "            except StopIteration:\n",
    "                yield batch\n",
    "                break\n",
    "                    \n",
    "            yield batch\n",
    "    \n",
    "    def _to_device(self, batch):\n",
    "        \"\"\"Move batch to device (already moved in collate_fn, but ensure it's there).\"\"\"\n",
    "        if isinstance(batch, dict):\n",
    "            # Batch is already on device from collate_fn, just return it\n",
    "            return batch\n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "773c16a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length: 2300\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Create prefetch loader\n",
    "prefetch_loader = PrefetchLoader(dataloader, device)\n",
    "\n",
    "print(f\"DataLoader length: {len(prefetch_loader)}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "be48d34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing regular dataloader:\n",
      "Regular loader time (5 batches): 0.9385s\n",
      "\n",
      "Testing prefetch loader:\n",
      "Prefetch loader time (5 batches): 0.7915s\n",
      "Speedup: 1.19x\n"
     ]
    }
   ],
   "source": [
    "# Test prefetch loader - compare timing\n",
    "import time\n",
    "\n",
    "# Test without prefetch\n",
    "print(\"Testing regular dataloader:\")\n",
    "start = time.time()\n",
    "for i, batch in enumerate(dataloader):\n",
    "    if i >= 5:  # Just test 5 batches\n",
    "        break\n",
    "    # Simulate some processing\n",
    "    _ = batch['feature'].sum()\n",
    "regular_time = time.time() - start\n",
    "print(f\"Regular loader time (5 batches): {regular_time:.4f}s\")\n",
    "\n",
    "# Test with prefetch\n",
    "print(\"\\nTesting prefetch loader:\")\n",
    "start = time.time()\n",
    "for i, batch in enumerate(prefetch_loader):\n",
    "    if i >= 5:  # Just test 5 batches\n",
    "        break\n",
    "    # Simulate some processing\n",
    "    _ = batch['feature'].sum()\n",
    "prefetch_time = time.time() - start\n",
    "print(f\"Prefetch loader time (5 batches): {prefetch_time:.4f}s\")\n",
    "print(f\"Speedup: {regular_time/prefetch_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c77bbaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: Simply replace 'dataloader' with 'prefetch_loader' in your training loop!\n",
      "The prefetch loader will automatically overlap data loading with GPU computation.\n"
     ]
    }
   ],
   "source": [
    "# Example: How to use prefetch loader in a training loop\n",
    "# \n",
    "# # Old way (slower):\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in dataloader:\n",
    "#         outputs = model(batch['go_embed'], batch['feature'])\n",
    "#         loss = criterion(outputs, batch['label'])\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#\n",
    "# # New way (faster):\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in prefetch_loader:  # <-- Just replace dataloader with prefetch_loader\n",
    "#         outputs = model(batch['go_embed'], batch['feature'])\n",
    "#         loss = criterion(outputs, batch['label'])\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "print(\"Usage: Simply replace 'dataloader' with 'prefetch_loader' in your training loop!\")\n",
    "print(\"The prefetch loader will automatically overlap data loading with GPU computation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd083518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch =  next(iter(dataloader))\n",
    "batch['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d36869",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "885c5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model.Query2Label import Query2Label\n",
    "\n",
    "\n",
    "model = Query2Label(num_classes = 256,\n",
    "                    in_dim = 512,\n",
    "                    nheads = 8,\n",
    "                    num_encoder_layers = 1,\n",
    "                    num_decoder_layers = 2,\n",
    "                    dim_feedforward = 2048,\n",
    "                    dropout = 0.1,\n",
    "                    use_positional_encoding = True,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7cabb1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7fc4cd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['go_embed'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ac9fad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['feature'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c8dcebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(batch['go_embed'], batch['feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53018c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56a712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cafa6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
