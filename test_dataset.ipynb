{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27d7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import obonet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30fa9f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def print_size(data):\n",
    "    print(f'{sys.getsizeof(data) / (1024 * 1024):.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e832a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_terms_with_neighbors(terms, go_graph, go_embeds, max_terms=256):\n",
    "    \"\"\"\n",
    "    Pad a list of GO terms with neighboring terms from the GO graph.\n",
    "    \n",
    "    Args:\n",
    "        terms: List of GO term IDs\n",
    "        go_graph: networkx graph of GO ontology\n",
    "        go_embeds: Dictionary of GO embeddings (to check if term has embedding)\n",
    "        max_terms: Maximum number of terms to return\n",
    "    \n",
    "    Returns:\n",
    "        List of GO terms padded to max_terms\n",
    "    \"\"\"\n",
    "    padded_terms = list(terms)\n",
    "    \n",
    "    if len(padded_terms) >= max_terms:\n",
    "        return padded_terms[:max_terms]\n",
    "    \n",
    "    # Use a set for faster lookup\n",
    "    terms_set = set(padded_terms)\n",
    "    \n",
    "    # Collect neighbors from existing terms\n",
    "    candidates = set()\n",
    "    for term in padded_terms:\n",
    "        if term in go_graph:\n",
    "            # Get parents (is_a relationships)\n",
    "            if 'is_a' in go_graph.nodes[term]:\n",
    "                parents = go_graph.nodes[term].get('is_a', [])\n",
    "                if isinstance(parents, str):\n",
    "                    parents = [parents]\n",
    "                candidates.update(parents)\n",
    "            \n",
    "            # Get successors (children) and predecessors (parents)\n",
    "            candidates.update(go_graph.successors(term))\n",
    "            candidates.update(go_graph.predecessors(term))\n",
    "    \n",
    "    # Remove terms already in the list\n",
    "    candidates = candidates - terms_set\n",
    "    \n",
    "    # Filter candidates to only those with embeddings\n",
    "    candidates = [c for c in candidates if c in go_embeds]\n",
    "    \n",
    "    # Add candidates until we reach max_terms\n",
    "    for candidate in candidates:\n",
    "        if len(padded_terms) >= max_terms:\n",
    "            break\n",
    "        padded_terms.append(candidate)\n",
    "        terms_set.add(candidate)\n",
    "    \n",
    "    # If still not enough, try neighbors of neighbors\n",
    "    if len(padded_terms) < max_terms:\n",
    "        second_level_candidates = set()\n",
    "        for term in candidates[:100]:  # Limit to avoid too much computation\n",
    "            if term in go_graph:\n",
    "                if 'is_a' in go_graph.nodes[term]:\n",
    "                    parents = go_graph.nodes[term].get('is_a', [])\n",
    "                    if isinstance(parents, str):\n",
    "                        parents = [parents]\n",
    "                    second_level_candidates.update(parents)\n",
    "                second_level_candidates.update(go_graph.successors(term))\n",
    "                second_level_candidates.update(go_graph.predecessors(term))\n",
    "        \n",
    "        second_level_candidates = second_level_candidates - terms_set\n",
    "        second_level_candidates = [c for c in second_level_candidates if c in go_embeds]\n",
    "        \n",
    "        for candidate in second_level_candidates:\n",
    "            if len(padded_terms) >= max_terms:\n",
    "                break\n",
    "            padded_terms.append(candidate)\n",
    "    \n",
    "    return padded_terms\n",
    "\n",
    "def pad_dataframe_terms(seq_2_terms_df, go_graph, go_embeds, max_terms=256):\n",
    "    \"\"\"\n",
    "    Pad the terms_predicted column in the dataframe in-place using GO graph neighbors.\n",
    "    \n",
    "    Args:\n",
    "        seq_2_terms_df: DataFrame with 'terms_predicted' column\n",
    "        go_graph: networkx graph of GO ontology\n",
    "        go_embeds: Dictionary of GO embeddings\n",
    "        max_terms: Maximum number of terms per row\n",
    "    \"\"\"\n",
    "    print(\"Padding terms_predicted with GO graph neighbors...\")\n",
    "    seq_2_terms_df['terms_predicted'] = seq_2_terms_df['terms_predicted'].apply(\n",
    "        lambda terms: pad_terms_with_neighbors(terms, go_graph, go_embeds, max_terms)\n",
    "    )\n",
    "    print(f\"Padding complete. Average terms per row: {seq_2_terms_df['terms_predicted'].apply(len).mean():.2f}\")\n",
    "    return seq_2_terms_df\n",
    "\n",
    "def prepare_data(data_paths, max_terms=256, aspect=None):\n",
    "    \n",
    "    knn_terms_df = data_paths['knn_terms_df']\n",
    "    train_terms_df = data_paths['train_terms_df']\n",
    "    features_embeds_path = data_paths['features_embeds_path']\n",
    "    features_ids_path = data_paths['features_ids_path']\n",
    "\n",
    "    go_embeds_paths = data_paths['go_embeds_paths']\n",
    "\n",
    "    seq_2_terms = pd.read_parquet(knn_terms_df, engine='fastparquet')\n",
    "    train_terms = pd.read_csv(train_terms_df, sep='\\t')\n",
    "\n",
    "    term_to_aspect = train_terms.groupby('term')['aspect'].first().to_dict()\n",
    "\n",
    "    go_graph = obonet.read_obo(data_paths['go_obo_path'])\n",
    "        \n",
    "    with open(go_embeds_paths, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        embeddings_dict = data['embeddings']\n",
    "        go_ids = data['go_ids']\n",
    "\n",
    "    # Filter to keep only terms from a specific aspect if aspect is provided\n",
    "    if aspect is not None:\n",
    "        seq_2_terms['terms_predicted'] = seq_2_terms['terms_predicted'].apply(\n",
    "            lambda terms: [t for t in terms if term_to_aspect.get(t) == aspect]\n",
    "        )\n",
    "        seq_2_terms['terms_true'] = seq_2_terms['terms_true'].apply(\n",
    "            lambda terms: [t for t in terms if term_to_aspect.get(t) == aspect]\n",
    "        )\n",
    "        # Remove rows where terms_predicted or terms_true is now empty\n",
    "        seq_2_terms = seq_2_terms[seq_2_terms['terms_predicted'].apply(len) > 0]\n",
    "        seq_2_terms = seq_2_terms[seq_2_terms['terms_true'].apply(len) > 0]\n",
    "\n",
    "\n",
    "    features_embeds = np.load(features_embeds_path, allow_pickle=True)\n",
    "    features_ids = np.load(features_ids_path, allow_pickle=True)\n",
    "\n",
    "    features_embeds_dict = {feat_id: embed for feat_id, embed in zip(features_ids, features_embeds)}\n",
    "\n",
    "    # Pad terms_predicted in the dataframe with GO graph neighbors\n",
    "    seq_2_terms = pad_dataframe_terms(seq_2_terms, go_graph, embeddings_dict, max_terms=max_terms)\n",
    "    #remove seq_2_terms row for which len(predicted_terms == 0)\n",
    "\n",
    "    term_lengths = seq_2_terms['terms_predicted'].apply(len)\n",
    "\n",
    "    #currently only using sequences with 256 terms, need to change later \n",
    "    seq_2_terms = seq_2_terms[term_lengths == max_terms]\n",
    "\n",
    "    train_ids =  pd.DataFrame(features_ids, columns=['qseqid'])\n",
    "    seq_2_terms = seq_2_terms.merge(train_ids, on='qseqid', how='inner')    \n",
    "\n",
    "    out = {'seq_2_terms': seq_2_terms,\n",
    "           'train_terms': train_terms,\n",
    "           'features_embeds': features_embeds_dict,\n",
    "           'go_embeds': embeddings_dict,\n",
    "           'go_graph': go_graph\n",
    "           }  \n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfccc0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding terms_predicted with GO graph neighbors...\n",
      "Padding complete. Average terms per row: 63.43\n"
     ]
    }
   ],
   "source": [
    "data_paths = {\n",
    "    'knn_terms_df':         '/mnt/d/ML/Kaggle/CAFA6-new/uniprot/diamond_knn_predictions.parquet',\n",
    "    'train_terms_df':       '/mnt/d/ML/Kaggle/CAFA6/cafa-6-protein-function-prediction/Train/train_terms.tsv',\n",
    "    'go_obo_path':          '/mnt/d/ML/Kaggle/CAFA6/cafa-6-protein-function-prediction/Train/go-basic.obo',\n",
    "    'features_embeds_path': '/mnt/d/ML/Kaggle/CAFA6-new/Dataset/esm_t33_650M/train_embeds.npy',\n",
    "    'features_ids_path':    '/mnt/d/ML/Kaggle/CAFA6-new/Dataset/esm_t33_650M/train_ids.npy',\n",
    "    'go_embeds_paths':      '/mnt/d/ML/Kaggle/CAFA6-new/uniprot/go_embeddings.pkl'\n",
    "}\n",
    "aspect = 'F'\n",
    "data = prepare_data(data_paths, max_terms=64, aspect=aspect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03672f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C max term 42\n",
    "# F max terms 34\n",
    "# P max terms 188  128 -> 42k/53k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c83dbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52083, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['seq_2_terms'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf8d1c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "\n",
    "# num_weak = nx.number_weakly_connected_components(data['go_graph'])\n",
    "# print(num_weak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b472cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt = data['train_terms'][data['train_terms']['aspect'] == aspect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "895c85ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min terms: 64\n",
      "Max terms: 64\n",
      "Mean terms: 64.00\n",
      "Median terms: 64.00\n",
      "No. empty predicted terms: 0\n",
      "\n",
      "Sample row with padded terms:\n",
      "Number of predicted terms: 64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of terms_predicted lengths after padding\n",
    "term_lengths = data['seq_2_terms']['terms_predicted'].apply(len)\n",
    "print(f\"Min terms: {term_lengths.min()}\")\n",
    "print(f\"Max terms: {term_lengths.max()}\")\n",
    "print(f\"Mean terms: {term_lengths.mean():.2f}\")\n",
    "print(f\"Median terms: {term_lengths.median():.2f}\")\n",
    "print(f\"No. empty predicted terms: {(term_lengths == 0).sum()}\")\n",
    "print(f\"\\nSample row with padded terms:\")\n",
    "print(f\"Number of predicted terms: {len(data['seq_2_terms'].iloc[0]['terms_predicted'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "705dc874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min terms: 1\n",
      "Max terms: 34\n",
      "Mean terms: 2.30\n",
      "Median terms: 2.00\n",
      "No. empty predicted terms: 0\n",
      "\n",
      "Sample row with padded terms:\n",
      "Number of predicted terms: 64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of terms_predicted lengths after padding\n",
    "term_lengths = data['seq_2_terms']['terms_true'].apply(len)\n",
    "print(f\"Min terms: {term_lengths.min()}\")\n",
    "print(f\"Max terms: {term_lengths.max()}\")\n",
    "print(f\"Mean terms: {term_lengths.mean():.2f}\")\n",
    "print(f\"Median terms: {term_lengths.median():.2f}\")\n",
    "print(f\"No. empty predicted terms: {(term_lengths == 0).sum()}\")\n",
    "print(f\"\\nSample row with padded terms:\")\n",
    "print(f\"Number of predicted terms: {len(data['seq_2_terms'].iloc[0]['terms_predicted'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "508afac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 512)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = data['seq_2_terms'].iloc[0]\n",
    "\n",
    "feature_embed = data['features_embeds'][row['qseqid']]\n",
    "\n",
    "go_embeds = np.array([data['go_embeds'][go_id] for go_id in row['terms_predicted']])\n",
    "go_embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0588be47",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "409638ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EntryID</th>\n",
       "      <th>term</th>\n",
       "      <th>aspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q5W0B1</td>\n",
       "      <td>GO:0000785</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q5W0B1</td>\n",
       "      <td>GO:0004842</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q5W0B1</td>\n",
       "      <td>GO:0051865</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q5W0B1</td>\n",
       "      <td>GO:0006275</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q5W0B1</td>\n",
       "      <td>GO:0006513</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  EntryID        term aspect\n",
       "0  Q5W0B1  GO:0000785      C\n",
       "1  Q5W0B1  GO:0004842      F\n",
       "2  Q5W0B1  GO:0051865      P\n",
       "3  Q5W0B1  GO:0006275      P\n",
       "4  Q5W0B1  GO:0006513      P"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train_terms'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6901b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qseqid</th>\n",
       "      <th>terms_predicted</th>\n",
       "      <th>terms_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A023I7E1</td>\n",
       "      <td>[GO:0008422, GO:0030246, GO:0016787, GO:005286...</td>\n",
       "      <td>[GO:0042973]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A024RBG1</td>\n",
       "      <td>[GO:0016818, GO:0034432, GO:0016462, GO:003443...</td>\n",
       "      <td>[GO:0005515]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A024SC78</td>\n",
       "      <td>[GO:0046555, GO:0052689, GO:0016298, GO:005052...</td>\n",
       "      <td>[GO:0050525]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A026W182</td>\n",
       "      <td>[GO:0004984, GO:0015278, GO:0005516, GO:000554...</td>\n",
       "      <td>[GO:0004984]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A044RE18</td>\n",
       "      <td>[GO:0004252, GO:0005515, GO:0008201, GO:000486...</td>\n",
       "      <td>[GO:0004252]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qseqid                                    terms_predicted    terms_true\n",
       "0  A0A023I7E1  [GO:0008422, GO:0030246, GO:0016787, GO:005286...  [GO:0042973]\n",
       "1  A0A024RBG1  [GO:0016818, GO:0034432, GO:0016462, GO:003443...  [GO:0005515]\n",
       "2  A0A024SC78  [GO:0046555, GO:0052689, GO:0016298, GO:005052...  [GO:0050525]\n",
       "3  A0A026W182  [GO:0004984, GO:0015278, GO:0005516, GO:000554...  [GO:0004984]\n",
       "4  A0A044RE18  [GO:0004252, GO:0005515, GO:0008201, GO:000486...  [GO:0004252]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['seq_2_terms'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4af23152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_terms_grouped shape: (58001, 2)\n",
      "\n",
      "Sample of grouped train_terms:\n",
      "       qseqid train_terms_all\n",
      "0  A0A023FBW4    [GO:0019958]\n",
      "1  A0A023FBW7    [GO:0019957]\n",
      "2  A0A023FDY8    [GO:0019957]\n",
      "3  A0A023FF81    [GO:0019958]\n",
      "4  A0A023FFB5    [GO:0019957]\n"
     ]
    }
   ],
   "source": [
    "# Group train_terms by EntryID and concatenate all terms into a list\n",
    "if aspect is not None:\n",
    "    data['train_terms'] = data['train_terms'][data['train_terms']['aspect'] == aspect]\n",
    "    \n",
    "train_terms_grouped = data['train_terms'].groupby('EntryID')['term'].apply(list).reset_index()\n",
    "train_terms_grouped.columns = ['qseqid', 'train_terms_all']\n",
    "\n",
    "print(f\"train_terms_grouped shape: {train_terms_grouped.shape}\")\n",
    "print(f\"\\nSample of grouped train_terms:\")\n",
    "print(train_terms_grouped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b53ded0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min terms: 1\n",
      "Max terms: 34\n",
      "Mean terms: 2.21\n",
      "Median terms: 2.00\n",
      "No. empty predicted terms: 0\n",
      "\n",
      "Sample row with padded terms:\n",
      "Number of predicted terms: 64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of terms_predicted lengths after padding\n",
    "term_lengths = train_terms_grouped['train_terms_all'].apply(len)\n",
    "print(f\"Min terms: {term_lengths.min()}\")\n",
    "print(f\"Max terms: {term_lengths.max()}\")\n",
    "print(f\"Mean terms: {term_lengths.mean():.2f}\")\n",
    "print(f\"Median terms: {term_lengths.median():.2f}\")\n",
    "print(f\"No. empty predicted terms: {(term_lengths == 0).sum()}\")\n",
    "print(f\"\\nSample row with padded terms:\")\n",
    "print(f\"Number of predicted terms: {len(data['seq_2_terms'].iloc[0]['terms_predicted'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "217e3a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataframe shape: (52083, 4)\n",
      "Columns: ['qseqid', 'terms_predicted', 'terms_true', 'train_terms_all']\n",
      "\n",
      "Sample of merged data:\n"
     ]
    }
   ],
   "source": [
    "# Merge train_terms_grouped with seq_2_terms\n",
    "merged_df = data['seq_2_terms'].merge(train_terms_grouped, on='qseqid', how='left')\n",
    "\n",
    "print(f\"Merged dataframe shape: {merged_df.shape}\")\n",
    "print(f\"Columns: {merged_df.columns.tolist()}\")\n",
    "print(f\"\\nSample of merged data:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b55cb35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Coverage Analysis ===\n",
      "\n",
      "Overall Statistics:\n",
      "Average coverage: 98.23%\n",
      "Median coverage: 100.00%\n",
      "Min coverage: 0.00%\n",
      "Max coverage: 100.00%\n",
      "\n",
      "Rows with 100% coverage: 50566 / 52083\n",
      "Rows with 0% coverage: 498 / 52083\n",
      "\n",
      "Average missing terms per row: 0.04\n",
      "Total train terms checked: 119713\n",
      "Total covered terms: 117654\n",
      "Total missing terms: 2059\n"
     ]
    }
   ],
   "source": [
    "# Check if all true predictions from train_terms_all are present in terms_predicted\n",
    "def check_coverage(row):\n",
    "    \"\"\"Check what percentage of train_terms_all are in terms_predicted\"\"\"\n",
    "    # Check if train_terms_all is None or empty list\n",
    "    true_key  = 'train_terms_all'\n",
    "    if row[true_key] is None or (isinstance(row[true_key], list) and len(row[true_key]) == 0):\n",
    "        return None\n",
    "    \n",
    "    train_set = set(row[true_key])\n",
    "    pred_set = set(row['terms_predicted'])\n",
    "    \n",
    "    # Find terms in train_terms_all that are in terms_predicted\n",
    "    covered_terms = train_set.intersection(pred_set)\n",
    "    \n",
    "    # Calculate coverage percentage\n",
    "    coverage = len(covered_terms) / len(train_set) * 100 if len(train_set) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total_train_terms': len(train_set),\n",
    "        'covered_terms': len(covered_terms),\n",
    "        'missing_terms': len(train_set) - len(covered_terms),\n",
    "        'coverage_pct': coverage,\n",
    "        'missing_term_list': list(train_set - covered_terms)\n",
    "    }\n",
    "\n",
    "# Apply the check\n",
    "coverage_results = merged_df.apply(check_coverage, axis=1)\n",
    "coverage_df = pd.DataFrame(coverage_results.tolist())\n",
    "\n",
    "# Combine with original data\n",
    "analysis_df = pd.concat([merged_df, coverage_df], axis=1)\n",
    "\n",
    "print(\"=== Coverage Analysis ===\")\n",
    "print(f\"\\nOverall Statistics:\")\n",
    "print(f\"Average coverage: {coverage_df['coverage_pct'].mean():.2f}%\")\n",
    "print(f\"Median coverage: {coverage_df['coverage_pct'].median():.2f}%\")\n",
    "print(f\"Min coverage: {coverage_df['coverage_pct'].min():.2f}%\")\n",
    "print(f\"Max coverage: {coverage_df['coverage_pct'].max():.2f}%\")\n",
    "print(f\"\\nRows with 100% coverage: {(coverage_df['coverage_pct'] == 100).sum()} / {len(coverage_df)}\")\n",
    "print(f\"Rows with 0% coverage: {(coverage_df['coverage_pct'] == 0).sum()} / {len(coverage_df)}\")\n",
    "print(f\"\\nAverage missing terms per row: {coverage_df['missing_terms'].mean():.2f}\")\n",
    "print(f\"Total train terms checked: {coverage_df['total_train_terms'].sum()}\")\n",
    "print(f\"Total covered terms: {coverage_df['covered_terms'].sum()}\")\n",
    "print(f\"Total missing terms: {coverage_df['missing_terms'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e16f6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Examples of Coverage Levels ===\n",
      "\n",
      "Example with 0% coverage:\n",
      "EntryID: A0A0B5A051\n",
      "Train terms: ['GO:0005515']... (total: 1)\n",
      "Predicted terms (first 5): ['GO:0004659', 'GO:0016114', 'GO:0010355', 'GO:0009941', 'GO:0009266']...\n",
      "Missing terms: ['GO:0005515']...\n",
      "\n",
      "Example with ~50% coverage:\n",
      "EntryID: A0A0A7EPL0\n",
      "Train terms: ['GO:0016925', 'GO:0019789', 'GO:0051176', 'GO:0009651', 'GO:0009737', 'GO:0006970', 'GO:0060966', 'GO:0005515']\n",
      "Covered: 4 / 8\n",
      "Missing terms: ['GO:0009651', 'GO:0051176', 'GO:0006970', 'GO:0009737']\n",
      "\n",
      "Example with 100% coverage:\n",
      "EntryID: A0A023FBW4\n",
      "Train terms: ['GO:0019958']\n",
      "All 1 terms are present in predicted terms!\n",
      "\n",
      "\n",
      "=== Coverage Distribution ===\n",
      "coverage_pct\n",
      "(-0.001, 25.0]     1266\n",
      "(25.0, 50.0]       1181\n",
      "(50.0, 75.0]       1196\n",
      "(75.0, 100.0]     69941\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Show examples of rows with different coverage levels\n",
    "print(\"=== Examples of Coverage Levels ===\\n\")\n",
    "\n",
    "# Example with 0% coverage\n",
    "print(\"Example with 0% coverage:\")\n",
    "zero_coverage = analysis_df[analysis_df['coverage_pct'] == 0].iloc[0]\n",
    "print(f\"EntryID: {zero_coverage['qseqid']}\")\n",
    "print(f\"Train terms: {zero_coverage['train_terms_all'][:5]}... (total: {zero_coverage['total_train_terms']})\")\n",
    "print(f\"Predicted terms (first 5): {zero_coverage['terms_predicted'][:5]}...\")\n",
    "print(f\"Missing terms: {zero_coverage['missing_term_list'][:5]}...\")\n",
    "print()\n",
    "\n",
    "# Example with partial coverage\n",
    "print(\"Example with ~50% coverage:\")\n",
    "partial_coverage = analysis_df[(analysis_df['coverage_pct'] > 45) & (analysis_df['coverage_pct'] < 55)].iloc[0]\n",
    "print(f\"EntryID: {partial_coverage['qseqid']}\")\n",
    "print(f\"Train terms: {partial_coverage['train_terms_all']}\")\n",
    "print(f\"Covered: {partial_coverage['covered_terms']} / {partial_coverage['total_train_terms']}\")\n",
    "print(f\"Missing terms: {partial_coverage['missing_term_list']}\")\n",
    "print()\n",
    "\n",
    "# Example with 100% coverage\n",
    "print(\"Example with 100% coverage:\")\n",
    "full_coverage = analysis_df[analysis_df['coverage_pct'] == 100].iloc[0]\n",
    "print(f\"EntryID: {full_coverage['qseqid']}\")\n",
    "print(f\"Train terms: {full_coverage['train_terms_all']}\")\n",
    "print(f\"All {full_coverage['total_train_terms']} terms are present in predicted terms!\")\n",
    "print()\n",
    "\n",
    "# Distribution of coverage\n",
    "print(\"\\n=== Coverage Distribution ===\")\n",
    "bins = [0, 25, 50, 75, 100]\n",
    "coverage_bins = pd.cut(coverage_df['coverage_pct'], bins=bins, include_lowest=True)\n",
    "print(coverage_bins.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96747401",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f275bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsDataset(Dataset):\n",
    "    \"\"\"Dataset that yields raw embeddings; tokenization is done in collate_fn for batching.\"\"\"\n",
    "    def __init__(self, \n",
    "                 data, \n",
    "                 max_go_embeds = 256,  \n",
    "                 oversample_indices=None\n",
    "                ):\n",
    "        \n",
    "        self.data = data\n",
    "        self.max_go_embeds = max_go_embeds\n",
    "        self.oversample_indices = oversample_indices if oversample_indices is not None else list(range(len(self.data['seq_2_terms'])))\n",
    "        self.mask_embed = np.zeros(next(iter(self.data['go_embeds'].values())).shape, dtype=np.float32)\n",
    "        #ensure len of predicted go terms is less than max_go_embeds\n",
    "        #self.data['seq_2_terms'] = self.data['seq_2_terms'][self\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.oversample_indices)         \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx = self.oversample_indices[idx]\n",
    "\n",
    "        row = self.data['seq_2_terms'].iloc[sample_idx]\n",
    "        qseqid = row['qseqid']\n",
    "\n",
    "        feature_embed = self.data['features_embeds'][qseqid]\n",
    "\n",
    "        true_terms_set = set(row['terms_true'])\n",
    "        predicted_terms = row['terms_predicted']\n",
    "        \n",
    "        # Filter terms that have embeddings (should be all of them after padding)\n",
    "        # valid_terms = [term for term in predicted_terms if term in self.data['go_embeds']]\n",
    "        valid_terms = predicted_terms\n",
    "        # Vectorized operations using list comprehensions\n",
    "        go_embeds = np.array([self.data['go_embeds'].get(term, self.mask_embed) for term in valid_terms])\n",
    "        label = np.array([term in true_terms_set for term in valid_terms], dtype=np.float32)\n",
    "        \n",
    "        return {\n",
    "            'entryID'   : qseqid,\n",
    "            'feature'   : feature_embed,\n",
    "            'go_embed'  : go_embeds,\n",
    "            'label'     : label,\n",
    "            'predicted_terms': valid_terms,\n",
    "            'true_terms': row['terms_true']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3592df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EmbeddingsDataset(data)\n",
    "\n",
    "aa = dataset[1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bef64d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4681f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_orthogonal_vectors_qr(dimension, num_vectors=None):\n",
    "    \"\"\"\n",
    "    Generates a set of orthogonal vectors using QR factorization.\n",
    "    \n",
    "    Args:\n",
    "        dimension (int): The dimensionality of the vectors.\n",
    "        num_vectors (int, optional): The number of vectors to generate. \n",
    "                                     Defaults to 'dimension' for a full basis.\n",
    "    Returns:\n",
    "        numpy.ndarray: A matrix where each column is an orthogonal vector.\n",
    "    \"\"\"\n",
    "    if num_vectors is None:\n",
    "        num_vectors = dimension\n",
    "        \n",
    "    # Generate a random matrix\n",
    "    A = np.random.rand(dimension, num_vectors)\n",
    "    \n",
    "    # Perform QR factorization\n",
    "    # The 'Q' matrix contains the orthogonal columns\n",
    "    Q, R = np.linalg.qr(A)\n",
    "    \n",
    "    # Return the first 'num_vectors' columns of Q\n",
    "    return Q[:, :num_vectors]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "028807d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dot product of the first two vectors: 2.7755575615628914e-17\n",
      "(1280, 1280)\n"
     ]
    }
   ],
   "source": [
    "vectors = generate_orthogonal_vectors_qr(1280 ) #, num_vectors=512)\n",
    "\n",
    "# Verification (dot product of any two distinct columns should be near zero):\n",
    "print(\"\\nDot product of the first two vectors:\", np.dot(vectors[:, 0], vectors[:, 1]))\n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c761773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 1280)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "307e06bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmbedTokenizer(nn.Module):\n",
    "    def __init__(self, D, d, N, rng=None):  \n",
    "        super(EmbedTokenizer, self).__init__()\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        D : int\n",
    "            Dimension of the embedding space.\n",
    "        d : int\n",
    "            Dimension of the token space.\n",
    "        N : int\n",
    "            Number of tokens.\n",
    "        rng : np.random.Generator or None\n",
    "            Random generator. If None, use default.\n",
    "        \"\"\"\n",
    "        if rng is None:\n",
    "            rng = np.random.default_rng()\n",
    "\n",
    "        V = generate_orthogonal_vectors_qr(D)\n",
    "        K = D\n",
    "        # Build P as a single tensor of shape (N, d, D) and register as buffer so it's moved with .to()\n",
    "        P_list = []\n",
    "        for i in range(N):\n",
    "            indices = np.arange(D)\n",
    "            sampled_idx = np.random.choice(indices, size=d, replace=False)\n",
    "            p = V[:, sampled_idx].T\n",
    "            P_list.append(p)\n",
    "\n",
    "        P_np = np.stack(P_list, axis=0).astype(np.float32)  # (N, d, D)\n",
    "        P_tensor = torch.from_numpy(P_np)\n",
    "        # register as buffer so it's not a parameter but moves with the module\n",
    "        self.register_buffer('P_buffer', P_tensor)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Tensor, shape (D)\n",
    "            Input embeddings.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tokens : Tensor, shape (batch_size, N, d)\n",
    "            Token representations.\n",
    "        \"\"\"\n",
    "        # x: (batch_size, D) or (D,) -> ensure batch\n",
    "        squeeze_output = False\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "            squeeze_output = True\n",
    "\n",
    "        # Get P and move to correct device/dtype\n",
    "        P = self.P_buffer.to(dtype=x.dtype, device=x.device)  # (N, d, D)\n",
    "\n",
    "        # Vectorized matmul: (batch_size, D) @ (D, N*d) -> (batch_size, N*d) -> reshape\n",
    "        D = x.shape[1]\n",
    "        P_2d = P.permute(2, 0, 1).reshape(D, -1)  # (D, N*d)\n",
    "        tokens = torch.matmul(x, P_2d)  # (batch_size, N*d)\n",
    "        tokens = tokens.reshape(x.shape[0], P.shape[0], P.shape[1])  # (batch_size, N, d)\n",
    "\n",
    "        if squeeze_output:\n",
    "            tokens = tokens.squeeze(0)\n",
    "\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a882a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = EmbedTokenizer(D=1280, d=512, N=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3099468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2364,  0.2243, -0.0518,  ...,  0.2313, -0.4923, -0.4198],\n",
       "          [-0.1851,  0.1678,  0.0376,  ..., -0.4407, -0.2901, -0.1140],\n",
       "          [-0.3032, -0.0757, -0.2510,  ..., -0.3959, -0.0704,  0.0801],\n",
       "          ...,\n",
       "          [-0.6705, -0.0571,  0.1888,  ...,  0.1879, -0.2961, -0.3327],\n",
       "          [ 0.1733,  0.0521, -0.3762,  ..., -0.1296, -0.1628, -0.1476],\n",
       "          [ 0.0667,  0.3748, -0.2992,  ...,  0.2143,  0.1407,  0.1951]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(torch.tensor(aa['feature'], dtype=torch.float32).to(device).unsqueeze(0)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10afa4c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfeatures_embeds\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "data['features_embeds'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9beff280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_tokenize(batch, tokenizer, device=None, dtype=torch.float32):\n",
    "    \"\"\"Custom collate function to handle variable-length data.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of samples from the dataset\n",
    "        tokenizer: Tokenizer to apply to features\n",
    "        device: Device to move tensors to (cuda or cpu)\n",
    "        dtype: Target dtype for tensors (torch.float32, torch.float16, or torch.bfloat16)\n",
    "    \"\"\"\n",
    "    features = torch.stack([torch.from_numpy(item['feature']) for item in batch])\n",
    "    features = features.to(dtype=dtype, device=device) if device else features.to(dtype=dtype)\n",
    "    features = tokenizer(features)\n",
    "    \n",
    "    go_embed = torch.stack([torch.from_numpy(item['go_embed']) for item in batch])\n",
    "    go_embed = go_embed.to(dtype=dtype, device=device) if device else go_embed.to(dtype=dtype)\n",
    "    \n",
    "    label = torch.stack([torch.from_numpy(item['label']) for item in batch])\n",
    "    label = label.to(dtype=dtype, device=device) if device else label.to(dtype=dtype)\n",
    "    \n",
    "    return {\n",
    "        'entryID': [item['entryID'] for item in batch],\n",
    "        'feature': features,\n",
    "        'go_embed': go_embed,\n",
    "        'label': label,\n",
    "        'predicted_terms': [item['predicted_terms'] for item in batch],\n",
    "        'true_terms': [item['true_terms'] for item in batch]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4952f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options: torch.float32 (default), torch.float16, torch.bfloat16\n",
    "target_dtype = torch.float32  # Change this to torch.float16 or torch.bfloat16 if needed\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=lambda b: collate_tokenize(b, tokenizer, device, dtype=target_dtype)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7c23812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefetchLoader:\n",
    "    \"\"\"\n",
    "    Prefetch loader that loads batches asynchronously to GPU for faster training.\n",
    "    Overlaps data transfer with computation by loading the next batch while \n",
    "    the model processes the current batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        self.stream = torch.cuda.Stream() if device.type == 'cuda' else None\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.stream is not None:\n",
    "            # CUDA prefetching\n",
    "            return self._cuda_iter()\n",
    "        else:\n",
    "            # CPU fallback - no prefetching needed\n",
    "            return iter(self.dataloader)\n",
    "    \n",
    "    def _cuda_iter(self):\n",
    "        \"\"\"Iterator with CUDA stream prefetching.\"\"\"\n",
    "        loader_iter = iter(self.dataloader)\n",
    "        \n",
    "        # Preload first batch\n",
    "        try:\n",
    "            with torch.cuda.stream(self.stream):\n",
    "                next_batch = next(loader_iter)\n",
    "                next_batch = self._to_device(next_batch)\n",
    "        except StopIteration:\n",
    "            return\n",
    "        \n",
    "        while True:\n",
    "            # Wait for the prefetch stream to finish\n",
    "            torch.cuda.current_stream().wait_stream(self.stream)\n",
    "            batch = next_batch\n",
    "            \n",
    "            # Make sure tensors are ready before yielding\n",
    "            if isinstance(batch, dict):\n",
    "                for k, v in batch.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        v.record_stream(torch.cuda.current_stream())\n",
    "            \n",
    "            # Start loading next batch in background\n",
    "            try:\n",
    "                with torch.cuda.stream(self.stream):\n",
    "                    next_batch = next(loader_iter)\n",
    "                    next_batch = self._to_device(next_batch)\n",
    "            except StopIteration:\n",
    "                yield batch\n",
    "                break\n",
    "                    \n",
    "            yield batch\n",
    "    \n",
    "    def _to_device(self, batch):\n",
    "        \"\"\"Move batch to device (already moved in collate_fn, but ensure it's there).\"\"\"\n",
    "        if isinstance(batch, dict):\n",
    "            # Batch is already on device from collate_fn, just return it\n",
    "            return batch\n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "773c16a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length: 2300\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Create prefetch loader\n",
    "prefetch_loader = PrefetchLoader(dataloader, device)\n",
    "\n",
    "print(f\"DataLoader length: {len(prefetch_loader)}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be48d34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing regular dataloader:\n",
      "Regular loader time (5 batches): 0.9673s\n",
      "\n",
      "Testing prefetch loader:\n",
      "Prefetch loader time (5 batches): 0.6510s\n",
      "Speedup: 1.49x\n"
     ]
    }
   ],
   "source": [
    "# Test prefetch loader - compare timing\n",
    "import time\n",
    "\n",
    "# Test without prefetch\n",
    "print(\"Testing regular dataloader:\")\n",
    "start = time.time()\n",
    "for i, batch in enumerate(dataloader):\n",
    "    if i >= 5:  # Just test 5 batches\n",
    "        break\n",
    "    # Simulate some processing\n",
    "    _ = batch['feature'].sum()\n",
    "regular_time = time.time() - start\n",
    "print(f\"Regular loader time (5 batches): {regular_time:.4f}s\")\n",
    "\n",
    "# Test with prefetch\n",
    "print(\"\\nTesting prefetch loader:\")\n",
    "start = time.time()\n",
    "for i, batch in enumerate(prefetch_loader):\n",
    "    if i >= 5:  # Just test 5 batches\n",
    "        break\n",
    "    # Simulate some processing\n",
    "    _ = batch['feature'].sum()\n",
    "prefetch_time = time.time() - start\n",
    "print(f\"Prefetch loader time (5 batches): {prefetch_time:.4f}s\")\n",
    "print(f\"Speedup: {regular_time/prefetch_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f3f66",
   "metadata": {},
   "source": [
    "## Check Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e29010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c8e5068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32, Num terms per sample: 256, Embedding dim: 512\n",
      "\n",
      "======================================================================\n",
      "PER-INSTANCE ANALYSIS\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26270/4193956610.py:83: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)\n",
      "  instance_stats['positive_sim_std'].append(pos_sim.std().item())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance   Positives    Random Sim      Pos Sim         Pos-Neg Sim    \n",
      "----------------------------------------------------------------------\n",
      "0          6            0.0143          0.0272          0.0255         \n",
      "1          5            0.0182          0.0240          0.0190         \n",
      "2          2            0.0112          0.0540          0.0261         \n",
      "3          7            0.0163          0.0677          0.0238         \n",
      "4          3            0.0039          -0.0007         0.0201         \n",
      "5          23           0.0156          0.0115          0.0163         \n",
      "6          5            0.0073          -0.0186         0.0129         \n",
      "7          4            0.0151          -0.0136         0.0104         \n",
      "8          14           0.0146          0.0282          0.0223         \n",
      "9          1            0.0182          N/A             -0.0413        \n",
      "... (22 more instances)\n",
      "\n",
      "======================================================================\n",
      "AGGREGATE STATISTICS ACROSS ALL INSTANCES\n",
      "======================================================================\n",
      "\n",
      "Average number of positives per instance: 6.84\n",
      "Average number of negatives per instance: 249.16\n",
      "\n",
      "=== Random Pairs (within each instance) ===\n",
      "Mean across instances: 0.0132\n",
      "Std across instances: 0.0049\n",
      "Min: 0.0039, Max: 0.0246\n",
      "\n",
      "=== Positive Pairs (within each instance) ===\n",
      "Mean across instances: 0.0290\n",
      "Std across instances: 0.0436\n",
      "Min: -0.0527, Max: 0.1539\n",
      "Instances with positive pairs: 26 / 32\n",
      "\n",
      "=== Positive vs Negative Pairs (within each instance) ===\n",
      "Mean across instances: 0.0205\n",
      "Std across instances: 0.0184\n",
      "Min: -0.0413, Max: 0.0534\n",
      "Instances with both pos and neg: 31 / 32\n"
     ]
    }
   ],
   "source": [
    "# Analyze cosine similarity per instance (within each sample's 256 token embeddings)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "go_embeds = batch['go_embed']  # Shape: (batch_size, 256, 512)\n",
    "labels = batch['label']  # Shape: (batch_size, 256)\n",
    "\n",
    "batch_size, num_terms, embed_dim = go_embeds.shape\n",
    "print(f\"Batch size: {batch_size}, Num terms per sample: {num_terms}, Embedding dim: {embed_dim}\")\n",
    "\n",
    "# Store results for each instance\n",
    "instance_stats = {\n",
    "    'random_sim_mean': [],\n",
    "    'random_sim_std': [],\n",
    "    'positive_sim_mean': [],\n",
    "    'positive_sim_std': [],\n",
    "    'pos_neg_sim_mean': [],\n",
    "    'pos_neg_sim_std': [],\n",
    "    'num_positives': [],\n",
    "    'num_negatives': []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PER-INSTANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    # Get embeddings and labels for this instance\n",
    "    instance_embeds = go_embeds[i]  # (256, 512)\n",
    "    instance_labels = labels[i]  # (256,)\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    normalized = F.normalize(instance_embeds, p=2, dim=1)\n",
    "    \n",
    "    # Count positive and negative labels\n",
    "    num_pos = (instance_labels == 1).sum().item()\n",
    "    num_neg = (instance_labels == 0).sum().item()\n",
    "    instance_stats['num_positives'].append(num_pos)\n",
    "    instance_stats['num_negatives'].append(num_neg)\n",
    "    \n",
    "    # 1. Random pairs within this instance\n",
    "    num_samples = min(1000, num_terms * (num_terms - 1) // 2)\n",
    "    idx1 = torch.randint(0, num_terms, (num_samples,))\n",
    "    idx2 = torch.randint(0, num_terms, (num_samples,))\n",
    "    # Filter out same indices\n",
    "    mask = idx1 != idx2\n",
    "    idx1 = idx1[mask]\n",
    "    idx2 = idx2[mask]\n",
    "    \n",
    "    if len(idx1) > 0:\n",
    "        random_sim = F.cosine_similarity(normalized[idx1], normalized[idx2], dim=1)\n",
    "        instance_stats['random_sim_mean'].append(random_sim.mean().item())\n",
    "        instance_stats['random_sim_std'].append(random_sim.std().item())\n",
    "    else:\n",
    "        instance_stats['random_sim_mean'].append(0.0)\n",
    "        instance_stats['random_sim_std'].append(0.0)\n",
    "    \n",
    "    # 2. Positive pairs\n",
    "    pos_indices = torch.where(instance_labels == 1)[0]\n",
    "    if len(pos_indices) > 1:\n",
    "        # Compute all pairs or sample\n",
    "        if len(pos_indices) > 50:\n",
    "            num_pos_samples = min(1000, len(pos_indices) * (len(pos_indices) - 1) // 2)\n",
    "            pos_idx1 = pos_indices[torch.randint(0, len(pos_indices), (num_pos_samples,))]\n",
    "            pos_idx2 = pos_indices[torch.randint(0, len(pos_indices), (num_pos_samples,))]\n",
    "            mask = pos_idx1 != pos_idx2\n",
    "            pos_idx1 = pos_idx1[mask]\n",
    "            pos_idx2 = pos_idx2[mask]\n",
    "        else:\n",
    "            # All pairs\n",
    "            pos_idx1 = []\n",
    "            pos_idx2 = []\n",
    "            for pi in range(len(pos_indices)):\n",
    "                for pj in range(pi+1, len(pos_indices)):\n",
    "                    pos_idx1.append(pos_indices[pi])\n",
    "                    pos_idx2.append(pos_indices[pj])\n",
    "            if len(pos_idx1) > 0:\n",
    "                pos_idx1 = torch.stack(pos_idx1)\n",
    "                pos_idx2 = torch.stack(pos_idx2)\n",
    "        \n",
    "        if len(pos_idx1) > 0:\n",
    "            pos_sim = F.cosine_similarity(normalized[pos_idx1], normalized[pos_idx2], dim=1)\n",
    "            instance_stats['positive_sim_mean'].append(pos_sim.mean().item())\n",
    "            instance_stats['positive_sim_std'].append(pos_sim.std().item())\n",
    "        else:\n",
    "            instance_stats['positive_sim_mean'].append(float('nan'))\n",
    "            instance_stats['positive_sim_std'].append(float('nan'))\n",
    "    else:\n",
    "        instance_stats['positive_sim_mean'].append(float('nan'))\n",
    "        instance_stats['positive_sim_std'].append(float('nan'))\n",
    "    \n",
    "    # 3. Positive vs Negative pairs\n",
    "    neg_indices = torch.where(instance_labels == 0)[0]\n",
    "    if len(pos_indices) > 0 and len(neg_indices) > 0:\n",
    "        num_cross = min(1000, len(pos_indices) * len(neg_indices))\n",
    "        cross_pos_idx = pos_indices[torch.randint(0, len(pos_indices), (num_cross,))]\n",
    "        cross_neg_idx = neg_indices[torch.randint(0, len(neg_indices), (num_cross,))]\n",
    "        \n",
    "        cross_sim = F.cosine_similarity(normalized[cross_pos_idx], normalized[cross_neg_idx], dim=1)\n",
    "        instance_stats['pos_neg_sim_mean'].append(cross_sim.mean().item())\n",
    "        instance_stats['pos_neg_sim_std'].append(cross_sim.std().item())\n",
    "    else:\n",
    "        instance_stats['pos_neg_sim_mean'].append(float('nan'))\n",
    "        instance_stats['pos_neg_sim_std'].append(float('nan'))\n",
    "\n",
    "# Convert to tensors for easy computation (ignoring NaN values)\n",
    "import numpy as np\n",
    "\n",
    "print(f\"\\n{'Instance':<10} {'Positives':<12} {'Random Sim':<15} {'Pos Sim':<15} {'Pos-Neg Sim':<15}\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(min(10, batch_size)):  # Show first 10 instances\n",
    "    random_sim = instance_stats['random_sim_mean'][i]\n",
    "    pos_sim = instance_stats['positive_sim_mean'][i]\n",
    "    pos_neg_sim = instance_stats['pos_neg_sim_mean'][i]\n",
    "    num_pos = instance_stats['num_positives'][i]\n",
    "    \n",
    "    pos_sim_str = f\"{pos_sim:.4f}\" if not np.isnan(pos_sim) else \"N/A\"\n",
    "    pos_neg_str = f\"{pos_neg_sim:.4f}\" if not np.isnan(pos_neg_sim) else \"N/A\"\n",
    "    \n",
    "    print(f\"{i:<10} {num_pos:<12} {random_sim:<15.4f} {pos_sim_str:<15} {pos_neg_str:<15}\")\n",
    "\n",
    "if batch_size > 10:\n",
    "    print(f\"... ({batch_size - 10} more instances)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AGGREGATE STATISTICS ACROSS ALL INSTANCES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute means across all instances (excluding NaN)\n",
    "random_sim_array = np.array(instance_stats['random_sim_mean'])\n",
    "pos_sim_array = np.array(instance_stats['positive_sim_mean'])\n",
    "pos_neg_sim_array = np.array(instance_stats['pos_neg_sim_mean'])\n",
    "\n",
    "print(f\"\\nAverage number of positives per instance: {np.mean(instance_stats['num_positives']):.2f}\")\n",
    "print(f\"Average number of negatives per instance: {np.mean(instance_stats['num_negatives']):.2f}\")\n",
    "\n",
    "print(f\"\\n=== Random Pairs (within each instance) ===\")\n",
    "print(f\"Mean across instances: {np.mean(random_sim_array):.4f}\")\n",
    "print(f\"Std across instances: {np.std(random_sim_array):.4f}\")\n",
    "print(f\"Min: {np.min(random_sim_array):.4f}, Max: {np.max(random_sim_array):.4f}\")\n",
    "\n",
    "valid_pos_sim = pos_sim_array[~np.isnan(pos_sim_array)]\n",
    "if len(valid_pos_sim) > 0:\n",
    "    print(f\"\\n=== Positive Pairs (within each instance) ===\")\n",
    "    print(f\"Mean across instances: {np.mean(valid_pos_sim):.4f}\")\n",
    "    print(f\"Std across instances: {np.std(valid_pos_sim):.4f}\")\n",
    "    print(f\"Min: {np.min(valid_pos_sim):.4f}, Max: {np.max(valid_pos_sim):.4f}\")\n",
    "    print(f\"Instances with positive pairs: {len(valid_pos_sim)} / {batch_size}\")\n",
    "else:\n",
    "    print(f\"\\n=== Positive Pairs ===\")\n",
    "    print(\"No instances with multiple positive labels\")\n",
    "\n",
    "valid_pos_neg_sim = pos_neg_sim_array[~np.isnan(pos_neg_sim_array)]\n",
    "if len(valid_pos_neg_sim) > 0:\n",
    "    print(f\"\\n=== Positive vs Negative Pairs (within each instance) ===\")\n",
    "    print(f\"Mean across instances: {np.mean(valid_pos_neg_sim):.4f}\")\n",
    "    print(f\"Std across instances: {np.std(valid_pos_neg_sim):.4f}\")\n",
    "    print(f\"Min: {np.min(valid_pos_neg_sim):.4f}, Max: {np.max(valid_pos_neg_sim):.4f}\")\n",
    "    print(f\"Instances with both pos and neg: {len(valid_pos_neg_sim)} / {batch_size}\")\n",
    "else:\n",
    "    print(f\"\\n=== Positive vs Negative Pairs ===\")\n",
    "    print(\"No instances with both positive and negative labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06234e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing entire dataset for instances with no positive labels...\n",
      "======================================================================\n",
      "Processing batches...\n",
      "Processed 100 batches (12800 instances)...\n",
      "Processed 200 batches (25600 instances)...\n",
      "Processed 300 batches (38400 instances)...\n",
      "Processed 400 batches (51200 instances)...\n",
      "Processed 500 batches (64000 instances)...\n",
      "\n",
      "Total instances in dataset: 73584\n",
      "Instances with NO positive labels: 999 (1.36%)\n",
      "Instances with at least 1 positive label: 72585 (98.64%)\n",
      "\n",
      "=== Positive Label Distribution ===\n",
      "Mean positive labels per instance: 6.67\n",
      "Median positive labels per instance: 4\n",
      "Std deviation: 8.07\n",
      "Min positive labels: 0\n",
      "Max positive labels: 192\n",
      "\n",
      "=== Distribution of Positive Label Counts ===\n",
      "Num Positives   Count      Percentage\n",
      "----------------------------------------\n",
      "0               999        1.36%\n",
      "1               12549      17.05%\n",
      "2               9485       12.89%\n",
      "3               8270       11.24%\n",
      "4               6950       9.44%\n",
      "5               5838       7.93%\n",
      "6               4733       6.43%\n",
      "7               3896       5.29%\n",
      "8               3197       4.34%\n",
      "9               2707       3.68%\n",
      "10              2193       2.98%\n",
      "11              1746       2.37%\n",
      "12              1522       2.07%\n",
      "13              1148       1.56%\n",
      "14              1093       1.49%\n",
      "15              858        1.17%\n",
      "16              729        0.99%\n",
      "17              674        0.92%\n",
      "18              498        0.68%\n",
      "19              468        0.64%\n",
      "... (97 more categories)\n",
      "\n",
      "=== Sample Instances with 0 Positive Labels ===\n",
      "\n",
      "Instance 0:\n",
      "  EntryID: A0A0B5A051\n",
      "  True terms: ['GO:0005515']\n",
      "  Predicted terms (first 5): ['GO:0004659', 'GO:0016114', 'GO:0010355', 'GO:0009941', 'GO:0009266']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 1:\n",
      "  EntryID: A0A0F6MY85\n",
      "  True terms: ['GO:0046661', 'GO:0030238', 'GO:0048024']\n",
      "  Predicted terms (first 5): ['GO:0003730', 'GO:0003723', 'GO:0048255', 'GO:0000398', 'GO:0030027']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 2:\n",
      "  EntryID: A0A0H2URK1\n",
      "  True terms: ['GO:0005515', 'GO:0044010', 'GO:0052031', 'GO:0009275']\n",
      "  Predicted terms (first 5): ['GO:0007155', 'GO:0005576', 'GO:0032311', 'GO:0007162', 'GO:0031395']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 3:\n",
      "  EntryID: A0A1D5NS60\n",
      "  True terms: ['GO:0031643', 'GO:0022010', 'GO:0048714']\n",
      "  Predicted terms (first 5): ['GO:0140944', 'GO:0030511', 'GO:1904520', 'GO:0006355', 'GO:0007411']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 4:\n",
      "  EntryID: A0A1D8PNR5\n",
      "  True terms: ['GO:0036170', 'GO:0044182', 'GO:0036180', 'GO:0009267', 'GO:0051454', 'GO:0015606', 'GO:0015848', 'GO:0071248', 'GO:0030447']\n",
      "  Predicted terms (first 5): ['GO:0005886', 'GO:0031402', 'GO:0015804', 'GO:1905039', 'GO:0015807']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 5:\n",
      "  EntryID: A0A2I1BT15\n",
      "  True terms: ['GO:0016854', 'GO:0140782']\n",
      "  Predicted terms (first 5): ['GO:0016787', 'GO:0005576', 'GO:0019786', 'GO:0032311', 'GO:0016825']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 6:\n",
      "  EntryID: A0A482N8M8\n",
      "  True terms: ['GO:0140781', 'GO:0016854', 'GO:0016218']\n",
      "  Predicted terms (first 5): ['GO:0016020', 'GO:0046872', 'GO:0051539', 'GO:0016491', 'GO:0005737']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 7:\n",
      "  EntryID: A0A7J6KD88\n",
      "  True terms: ['GO:0005615', 'GO:0097570']\n",
      "  Predicted terms (first 5): ['GO:0016301', 'GO:0009608', 'GO:0005634', 'GO:0005524', 'GO:0005516']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 8:\n",
      "  EntryID: A0SVK0\n",
      "  True terms: ['GO:0042802', 'GO:0010162', 'GO:2000033', 'GO:0009738', 'GO:0005515', 'GO:0010182']\n",
      "  Predicted terms (first 5): ['GO:0005634', 'GO:0005737', 'GO:0003674', 'GO:0043565', 'GO:0006351']\n",
      "  True terms in predicted: None\n",
      "\n",
      "Instance 9:\n",
      "  EntryID: A1A4F0\n",
      "  True terms: ['GO:0005515']\n",
      "  Predicted terms (first 5): ['GO:0061459', 'GO:1903401', 'GO:0005765', 'GO:0015819', 'GO:1903826']\n",
      "  True terms in predicted: None\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Check the entire dataset for instances with no positive labels using prefetch_loader\n",
    "print(\"Analyzing entire dataset for instances with no positive labels...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "instances_with_no_positives = 0\n",
    "instances_with_positives = 0\n",
    "positive_label_counts = []\n",
    "zero_pos_samples = []  # Store samples with 0 positives for later inspection\n",
    "\n",
    "# Use dataloader for faster iteration\n",
    "full_dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=128,  # Larger batch size for faster processing\n",
    "    shuffle=False,  # Don't shuffle to maintain order\n",
    "    num_workers=0, \n",
    "    collate_fn=lambda b: collate_tokenize(b, tokenizer, device, dtype=target_dtype)\n",
    ")\n",
    "\n",
    "print(\"Processing batches...\")\n",
    "batch_count = 0\n",
    "for batch_data in full_dataloader:\n",
    "    batch_count += 1\n",
    "    if batch_count % 100 == 0:\n",
    "        print(f\"Processed {batch_count} batches ({batch_count * 128} instances)...\")\n",
    "    \n",
    "    labels_batch = batch_data['label']  # Shape: (batch_size, 256)\n",
    "    \n",
    "    # Count positives for each instance in the batch\n",
    "    num_positives_per_instance = (labels_batch == 1).sum(dim=1).cpu().numpy()\n",
    "    \n",
    "    for i, num_pos in enumerate(num_positives_per_instance):\n",
    "        positive_label_counts.append(num_pos)\n",
    "        \n",
    "        if num_pos == 0:\n",
    "            instances_with_no_positives += 1\n",
    "            # Store sample info for later (only store first few to save memory)\n",
    "            if len(zero_pos_samples) < 10:\n",
    "                zero_pos_samples.append({\n",
    "                    'entryID': batch_data['entryID'][i],\n",
    "                    'true_terms': batch_data['true_terms'][i],\n",
    "                    'predicted_terms': batch_data['predicted_terms'][i]\n",
    "                })\n",
    "        else:\n",
    "            instances_with_positives += 1\n",
    "\n",
    "total_instances = len(dataset)\n",
    "\n",
    "print(f\"\\nTotal instances in dataset: {total_instances}\")\n",
    "print(f\"Instances with NO positive labels: {instances_with_no_positives} ({instances_with_no_positives/total_instances*100:.2f}%)\")\n",
    "print(f\"Instances with at least 1 positive label: {instances_with_positives} ({instances_with_positives/total_instances*100:.2f}%)\")\n",
    "\n",
    "# Statistics on positive label distribution\n",
    "positive_label_counts = np.array(positive_label_counts)\n",
    "print(f\"\\n=== Positive Label Distribution ===\")\n",
    "print(f\"Mean positive labels per instance: {positive_label_counts.mean():.2f}\")\n",
    "print(f\"Median positive labels per instance: {np.median(positive_label_counts):.0f}\")\n",
    "print(f\"Std deviation: {positive_label_counts.std():.2f}\")\n",
    "print(f\"Min positive labels: {positive_label_counts.min()}\")\n",
    "print(f\"Max positive labels: {positive_label_counts.max()}\")\n",
    "\n",
    "# Show histogram of positive label counts\n",
    "print(f\"\\n=== Distribution of Positive Label Counts ===\")\n",
    "unique, counts = np.unique(positive_label_counts, return_counts=True)\n",
    "print(f\"{'Num Positives':<15} {'Count':<10} {'Percentage':<10}\")\n",
    "print(\"-\" * 40)\n",
    "for num_pos, count in zip(unique[:20], counts[:20]):  # Show first 20\n",
    "    print(f\"{int(num_pos):<15} {count:<10} {count/total_instances*100:.2f}%\")\n",
    "if len(unique) > 20:\n",
    "    print(f\"... ({len(unique) - 20} more categories)\")\n",
    "\n",
    "# Show sample instances with 0 positives\n",
    "if instances_with_no_positives > 0 and len(zero_pos_samples) > 0:\n",
    "    print(f\"\\n=== Sample Instances with 0 Positive Labels ===\")\n",
    "    for i, sample in enumerate(zero_pos_samples):\n",
    "        print(f\"\\nInstance {i}:\")\n",
    "        print(f\"  EntryID: {sample['entryID']}\")\n",
    "        print(f\"  True terms: {sample['true_terms']}\")\n",
    "        print(f\"  Predicted terms (first 5): {sample['predicted_terms'][:5]}\")\n",
    "        # Check if true terms are in predicted terms\n",
    "        true_in_pred = set(sample['true_terms']).intersection(set(sample['predicted_terms']))\n",
    "        print(f\"  True terms in predicted: {true_in_pred if true_in_pred else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80755c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "885c5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model.Query2Label import Query2Label\n",
    "\n",
    "\n",
    "model = Query2Label(num_classes = 256,\n",
    "                    in_dim = 512,\n",
    "                    nheads = 8,\n",
    "                    num_encoder_layers = 1,\n",
    "                    num_decoder_layers = 2,\n",
    "                    dim_feedforward = 2048,\n",
    "                    dropout = 0.1,\n",
    "                    use_positional_encoding = True,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70ab20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from Utils.AsymetricLoss import AsymmetricLossOptimized, AsymmetricLoss\n",
    "from Utils.RankLoss import RankLossPair\n",
    "from Model.Query2Label import Query2Label\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class Query2Label_pl(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        in_dim: int,\n",
    "        nheads: int = 8,\n",
    "        num_encoder_layers: int = 1,\n",
    "        num_decoder_layers: int = 2,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        use_positional_encoding: bool = True,\n",
    "        lr: float = 1e-4,\n",
    "        weight_decay: float = 1e-5,\n",
    "        # Loss selection: 'ASL' or 'BCE'\n",
    "        loss_function: str = 'ASL',\n",
    "        # Asymmetric loss parameters\n",
    "        gamma_neg: float = 4.0,\n",
    "        gamma_pos: float = 0.0,\n",
    "        clip: float = 0.05,\n",
    "        loss_eps: float = 1e-8,\n",
    "        disable_torch_grad_focal_loss: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = Query2Label(\n",
    "            num_classes=num_classes,\n",
    "            in_dim=in_dim,\n",
    "            nheads=nheads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            use_positional_encoding=use_positional_encoding,\n",
    "        )\n",
    "\n",
    "    \n",
    "        # Instantiate chosen loss function\n",
    "        lf = (loss_function or 'ASL').upper()\n",
    "        if lf in ('ASL', 'ASYMMETRIC', 'ASYMMETRIC_LOSS'):\n",
    "            # prefer optimized implementation\n",
    "            try:\n",
    "                self.criterion = AsymmetricLossOptimized(\n",
    "                    gamma_neg=gamma_neg,\n",
    "                    gamma_pos=gamma_pos,\n",
    "                    clip=clip,\n",
    "                    eps=loss_eps,\n",
    "                    disable_torch_grad_focal_loss=disable_torch_grad_focal_loss,\n",
    "                )\n",
    "            except Exception:\n",
    "                self.criterion = AsymmetricLoss(\n",
    "                    gamma_neg=gamma_neg,\n",
    "                    gamma_pos=gamma_pos,\n",
    "                    clip=clip,\n",
    "                    eps=loss_eps,\n",
    "                    disable_torch_grad_focal_loss=disable_torch_grad_focal_loss,\n",
    "                )\n",
    "        elif lf in ('BCE', 'BCEWITHLOGITS', 'BCEWITHLOGITSLOSS'):\n",
    "            # Use BCEWithLogitsLoss; set reduction='sum' to match ASL sum-based scale\n",
    "            self.criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "        elif lf in ('RANK', 'RANKLOSS', 'RANK_LOSS'):\n",
    "            # RankLoss uses RankLossPair class\n",
    "            self.criterion = RankLossPair(reduction='mean')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss_function: {loss_function}. Supported: 'ASL', 'BCE', 'RankLoss'.\")\n",
    "        \n",
    "        # Store loss function name for training step\n",
    "        self.loss_function = lf\n",
    "\n",
    "        # Store validation step outputs for on_validation_epoch_end\n",
    "        self.validation_step_outputs = []\n",
    "        # Test step outputs\n",
    "        self.test_step_outputs = []\n",
    "        # For GO term-based F1 computation (store predictions per GO term)\n",
    "        self._val_go_predictions = {}  # {go_term_id: list of probabilities}\n",
    "        self._val_go_targets = {}      # {go_term_id: list of binary labels}\n",
    "        self._rank_score_list = []\n",
    "        self._cutoff_score_list = []\n",
    "\n",
    "    def forward(self, x: torch.Tensor, f: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x, f) \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Standard training step.\n",
    "\n",
    "        Expects `batch` to be a dict with keys `'tokens'` and `'label'` where\n",
    "        - `tokens` is a float Tensor of shape (B, L, C_in)\n",
    "        - `label` is a binary Tensor of shape (B, num_classes)\n",
    "        \"\"\"\n",
    "        x = batch['go_embed']   # (B, L, C_in)\n",
    "        f = batch['feature']   \n",
    "        y = batch['label']\n",
    "\n",
    "        logits = self.forward(x, f)\n",
    "        \n",
    "        # Calculate loss based on loss function type\n",
    "        if self.loss_function in ('RANK', 'RANKLOSS', 'RANK_LOSS'):\n",
    "            # RankLoss: compute loss for each class separately and sum\n",
    "            losses = []\n",
    "            for class_idx in range(logits.shape[1]):\n",
    "                scores = logits[:, class_idx]  # (B,)\n",
    "                labels = y[:, class_idx]  # (B,)\n",
    "                \n",
    "                # Only compute loss if we have both positive and negative samples\n",
    "                if labels.sum() > 0 and labels.sum() < len(labels):\n",
    "                    class_loss = self.criterion(scores, labels)\n",
    "                    losses.append(class_loss)\n",
    "            \n",
    "            if losses:\n",
    "                loss = torch.stack(losses).mean()\n",
    "        else:\n",
    "            # Standard loss functions (ASL, BCE)\n",
    "            loss = self.criterion(logits, y)\n",
    "\n",
    "        # Log loss\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Validation step computes loss and stores for F-max computation.\"\"\"\n",
    "        x = batch['go_embed']   # (B, L, C_in)\n",
    "        f = batch['feature']   \n",
    "        y = batch['label']\n",
    "\n",
    "        logits = self.forward(x, f)\n",
    "        \n",
    "        # Calculate loss based on loss function type\n",
    "        if self.loss_function in ('RANK', 'RANKLOSS', 'RANK_LOSS'):\n",
    "            # RankLoss: compute loss for each class separately and sum\n",
    "            losses = []\n",
    "            for class_idx in range(logits.shape[1]):\n",
    "                scores = logits[:, class_idx]  # (B,)\n",
    "                labels = y[:, class_idx]  # (B,)\n",
    "                \n",
    "                # Only compute loss if we have both positive and negative samples\n",
    "                if labels.sum() > 0 and labels.sum() < len(labels):\n",
    "                    class_loss = self.criterion(scores, labels)\n",
    "                    losses.append(class_loss)\n",
    "            \n",
    "            if losses:\n",
    "                loss = torch.stack(losses).mean()\n",
    "            else:\n",
    "                # Fallback to BCE if no valid pairs\n",
    "                loss = nn.functional.binary_cross_entropy_with_logits(logits, y, reduction='mean')\n",
    "        else:\n",
    "            # Standard loss functions (ASL, BCE)\n",
    "            loss = self.criterion(logits, y)\n",
    "\n",
    "        # Log per-step validation loss (will be reduced automatically)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        # Store for epoch end aggregation\n",
    "        self.validation_step_outputs.append({'val_loss': loss.detach()})\n",
    "        \n",
    "        # Store GO term-specific predictions for macro/micro F1\n",
    "        probs = torch.sigmoid(logits)\n",
    "        targets = (y > 0.5).int()\n",
    "        try:\n",
    "            probs_np = probs.detach().cpu().numpy()\n",
    "            targets_np = targets.detach().cpu().numpy()\n",
    "            predicted_terms = batch['predicted_terms']  # List of lists of GO term IDs\n",
    "            \n",
    "            #Calculate rankscore and cutoff score\n",
    "            rank_score = self._compute_rank_score(probs_np, targets_np, predicted_terms)\n",
    "            cutoff_score = self._compute_cutoff_score(probs_np, targets_np, predicted_terms)\n",
    "\n",
    "            self._rank_score_list.append(rank_score)\n",
    "            self._cutoff_score_list.append(cutoff_score)\n",
    "\n",
    "            # For each sample in the batch\n",
    "            for sample_idx in range(len(predicted_terms)):\n",
    "                sample_terms = predicted_terms[sample_idx]\n",
    "                sample_probs = probs_np[sample_idx]  # Shape: (num_terms_for_sample,)\n",
    "                sample_targets = targets_np[sample_idx]  # Shape: (num_terms_for_sample,)\n",
    "                \n",
    "                # For each GO term in this sample\n",
    "                for term_idx, go_term in enumerate(sample_terms):\n",
    "                    if go_term not in self._val_go_predictions:\n",
    "                        self._val_go_predictions[go_term] = []\n",
    "                        self._val_go_targets[go_term] = []\n",
    "                    \n",
    "                    self._val_go_predictions[go_term].append(float(sample_probs[term_idx]))\n",
    "                    self._val_go_targets[go_term].append(int(sample_targets[term_idx]))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not store GO term predictions: {e}\")\n",
    "            pass\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        \"\"\"Called at the end of validation epoch. Compute F-max only.\"\"\"\n",
    "        # compute mean validation loss for the epoch\n",
    "        if not self.validation_step_outputs:\n",
    "            return\n",
    "        losses = torch.stack([o['val_loss'] for o in self.validation_step_outputs])\n",
    "        mean_loss = losses.mean()\n",
    "\n",
    "        \n",
    "        self.log('val_loss_epoch', mean_loss, prog_bar=True, logger=True)\n",
    "        \n",
    "        mean_rank_score = float(np.mean(self._rank_score_list)) if self._rank_score_list else 0.0\n",
    "        mean_cutoff_score = float(np.mean(self._cutoff_score_list)) if self._cutoff_score_list else 0.0\n",
    "        self.log('val_rank_score', mean_rank_score, prog_bar=True, logger=True)\n",
    "        self.log('val_cutoff_score', mean_cutoff_score, prog_bar=True, logger=True)\n",
    "\n",
    "\n",
    "        # Compute GO term-based macro and micro F1 scores\n",
    "        try:\n",
    "            if len(self._val_go_predictions) > 0:\n",
    "                macro_f1, micro_f1 = self._compute_go_f1_scores()\n",
    "                self.log('val_f1_macro_go', macro_f1, prog_bar=True, logger=True)\n",
    "                self.log('val_f1_micro_go', micro_f1, prog_bar=True, logger=True)\n",
    "                \n",
    "                try:\n",
    "                    rank = getattr(self, 'global_rank', 0)\n",
    "                except Exception:\n",
    "                    rank = 0\n",
    "                if rank == 0:\n",
    "                    print(f\"Validation GO-based Macro F1: {macro_f1:.4f}, Micro F1: {micro_f1:.4f}\")\n",
    "                \n",
    "                # Clear GO term buffers\n",
    "                self._val_go_predictions.clear()\n",
    "                self._val_go_targets.clear()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not compute GO-based F1 scores: {e}\")\n",
    "            pass\n",
    "        \n",
    "        # Clear stored outputs for next epoch\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"Test step computes loss and logs it.\"\"\"\n",
    "        x = batch['tokens']\n",
    "        y = batch['label'].float()\n",
    "\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "\n",
    "        # Log per-step test loss\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        self.test_step_outputs.append({'test_loss': loss.detach()})\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        \"\"\"Aggregate test metrics at epoch end and log them.\"\"\"\n",
    "        if not self.test_step_outputs:\n",
    "            return\n",
    "        losses = torch.stack([o['test_loss'] for o in self.test_step_outputs])\n",
    "        mean_loss = losses.mean()\n",
    "\n",
    "        self.log('test_loss_epoch', mean_loss, prog_bar=True, logger=True)\n",
    "        print(f\"Test Loss: {mean_loss:.4f}\")\n",
    "\n",
    "        self.test_step_outputs.clear()\n",
    "\n",
    "    def _compute_rank_score(self, probs_np, targets_np, predicted_terms):\n",
    "        \"\"\"Compute rank score based on GO term predictions.\n",
    "        \n",
    "        For each instance:\n",
    "        1. Threshold = number of positive labels\n",
    "        2. Sort predictions in descending order\n",
    "        3. Count how many true labels are in top `threshold` positions\n",
    "        4. Score = (true labels in top k) / (total true labels)\n",
    "        \"\"\"\n",
    "        batch_scores = []\n",
    "        \n",
    "        for sample_idx in range(len(probs_np)):\n",
    "            sample_probs = probs_np[sample_idx]\n",
    "            sample_targets = targets_np[sample_idx]\n",
    "            \n",
    "            # Threshold is the number of positive labels\n",
    "            num_positives = int(sample_targets.sum())\n",
    "            \n",
    "            # Skip if no positives\n",
    "            if num_positives == 0:\n",
    "                continue\n",
    "            \n",
    "            # Get indices of predictions sorted in descending order\n",
    "            sorted_indices = np.argsort(sample_probs)[::-1]\n",
    "            \n",
    "            # Get top k indices where k = number of positive labels\n",
    "            top_k_indices = sorted_indices[:num_positives]\n",
    "            \n",
    "            # Check how many of the top k predictions are true positives\n",
    "            true_positives_in_top_k = sample_targets[top_k_indices].sum()\n",
    "            \n",
    "            # Calculate score for this sample\n",
    "            score = true_positives_in_top_k / num_positives\n",
    "            batch_scores.append(score)\n",
    "        \n",
    "        # Return average score across all samples in batch\n",
    "        return float(np.mean(batch_scores)) if batch_scores else 0.0\n",
    "        \n",
    "\n",
    "    def _compute_cutoff_score(self, probs_np, targets_np, predicted_terms):\n",
    "        \"\"\"Compute cutoff score based on GO term predictions.\n",
    "        \n",
    "        For each instance:\n",
    "        1. Sort predictions in descending order\n",
    "        2. Find the index of the last (lowest ranked) true label\n",
    "        3. That index is the cutoff score\n",
    "        \"\"\"\n",
    "        batch_scores = []\n",
    "        \n",
    "        for sample_idx in range(len(probs_np)):\n",
    "            sample_probs = probs_np[sample_idx]\n",
    "            sample_targets = targets_np[sample_idx]\n",
    "            \n",
    "            # Skip if no positives\n",
    "            num_positives = int(sample_targets.sum())\n",
    "            if num_positives == 0:\n",
    "                continue\n",
    "            \n",
    "            # Get indices of predictions sorted in descending order\n",
    "            sorted_indices = np.argsort(sample_probs)[::-1]\n",
    "            \n",
    "            # Find positions of true labels in the sorted order\n",
    "            true_label_positions = []\n",
    "            for idx, sorted_idx in enumerate(sorted_indices):\n",
    "                if sample_targets[sorted_idx] == 1:\n",
    "                    true_label_positions.append(idx)\n",
    "            \n",
    "            # The cutoff score is the index of the last true label\n",
    "            if true_label_positions:\n",
    "                cutoff_score = max(true_label_positions)\n",
    "                batch_scores.append(cutoff_score)\n",
    "        \n",
    "        # Return average score across all samples in batch\n",
    "        return float(np.mean(batch_scores)) if batch_scores else 0.0\n",
    "\n",
    "\n",
    "    def _compute_go_f1_scores(self, threshold=0.5):\n",
    "        \"\"\"Compute macro and micro F1 scores based on GO term predictions.\n",
    "        \n",
    "        For each GO term, we have multiple predictions across different samples.\n",
    "        Macro F1: Average F1 score across all GO terms\n",
    "        Micro F1: F1 score computed from global TP, FP, FN counts\n",
    "        \"\"\"\n",
    "        eps = 1e-8\n",
    "        per_term_f1_scores = []\n",
    "        \n",
    "        # Global counts for micro F1\n",
    "        global_tp = 0\n",
    "        global_fp = 0\n",
    "        global_fn = 0\n",
    "        \n",
    "        # Compute F1 for each GO term\n",
    "        for go_term, predictions in self._val_go_predictions.items():\n",
    "            targets = self._val_go_targets[go_term]\n",
    "            \n",
    "            # Convert to numpy for easier computation\n",
    "            preds_array = np.array(predictions)\n",
    "            targets_array = np.array(targets)\n",
    "            \n",
    "            # Binarize predictions\n",
    "            preds_binary = (preds_array >= threshold).astype(int)\n",
    "            \n",
    "            # Compute TP, FP, FN for this GO term\n",
    "            tp = np.sum((preds_binary == 1) & (targets_array == 1))\n",
    "            fp = np.sum((preds_binary == 1) & (targets_array == 0))\n",
    "            fn = np.sum((preds_binary == 0) & (targets_array == 1))\n",
    "            \n",
    "            # Accumulate for micro F1\n",
    "            global_tp += tp\n",
    "            global_fp += fp\n",
    "            global_fn += fn\n",
    "            \n",
    "            # Compute F1 for this term\n",
    "            precision = tp / (tp + fp + eps)\n",
    "            recall = tp / (tp + fn + eps)\n",
    "            f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "            \n",
    "            per_term_f1_scores.append(f1)\n",
    "        \n",
    "        # Compute macro F1 (average across all GO terms)\n",
    "        macro_f1 = float(np.mean(per_term_f1_scores)) if per_term_f1_scores else 0.0\n",
    "        \n",
    "        # Compute micro F1 (global counts)\n",
    "        micro_precision = global_tp / (global_tp + global_fp + eps)\n",
    "        micro_recall = global_tp / (global_tp + global_fn + eps)\n",
    "        micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall + eps)\n",
    "        micro_f1 = float(micro_f1)\n",
    "        \n",
    "        return macro_f1, micro_f1\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Use AdamW; hyperparameters were saved in self.hparams by save_hyperparameters()\n",
    "        lr = self.hparams.get('lr', 1e-4) if isinstance(self.hparams, dict) else getattr(self.hparams, 'lr', 1e-4)\n",
    "        weight_decay = self.hparams.get('weight_decay', 1e-5) if isinstance(self.hparams, dict) else getattr(self.hparams, 'weight_decay', 1e-5)\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # Use ReduceLROnPlateau scheduler (monitoring validation metric 'val_fmax_macro').\n",
    "        # Parameters can be provided via hparams: 'plateau_factor', 'plateau_patience', 'min_lr', 'plateau_threshold'.\n",
    "        if isinstance(self.hparams, dict):\n",
    "            plateau_factor = self.hparams.get('plateau_factor', 0.5)\n",
    "            plateau_patience = int(self.hparams.get('plateau_patience', 3))\n",
    "            plateau_min_lr = self.hparams.get('min_lr', 1e-6)\n",
    "            plateau_threshold = self.hparams.get('plateau_threshold', 1e-4)\n",
    "        else:\n",
    "            plateau_factor = getattr(self.hparams, 'plateau_factor', 0.5)\n",
    "            plateau_patience = int(getattr(self.hparams, 'plateau_patience', 3))\n",
    "            plateau_min_lr = getattr(self.hparams, 'min_lr', 1e-6)\n",
    "            plateau_threshold = getattr(self.hparams, 'plateau_threshold', 1e-4)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='max',\n",
    "            factor=float(plateau_factor),\n",
    "            patience=int(plateau_patience),\n",
    "            min_lr=float(plateau_min_lr),\n",
    "            threshold=float(plateau_threshold)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_f1_macro_go',\n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1,\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1ddf3",
   "metadata": {},
   "source": [
    "## Test Rank Score and Cutoff Score Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94fbce18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST: Rank Score and Cutoff Score Functions\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "SAMPLE DATA\n",
      "======================================================================\n",
      "\n",
      "Instance 1:\n",
      "  Predictions: [0.9  0.85 0.7  0.65 0.5  0.45 0.3  0.2 ]\n",
      "  True labels: [1 1 0 1 0 0 1 0]\n",
      "  Num positives: 4\n",
      "  Sorted indices: [0 1 2 3 4 5 6 7]\n",
      "  Sorted predictions: [0.9  0.85 0.7  0.65 0.5  0.45 0.3  0.2 ]\n",
      "  Sorted labels: [1 1 0 1 0 0 1 0]\n",
      "  True label positions in sorted order: [0, 1, 3, 6]\n",
      "  --> Rank Score: 3/4 = 0.75\n",
      "  --> Cutoff Score: 6 (index of last true label)\n",
      "\n",
      "Instance 2:\n",
      "  Predictions: [0.95 0.8  0.75 0.6  0.4  0.3  0.25 0.15]\n",
      "  True labels: [1 0 1 0 1 0 0 0]\n",
      "  Num positives: 3\n",
      "  Sorted indices: [0 1 2 3 4 5 6 7]\n",
      "  Sorted predictions: [0.95 0.8  0.75 0.6  0.4  0.3  0.25 0.15]\n",
      "  Sorted labels: [1 0 1 0 1 0 0 0]\n",
      "  True label positions in sorted order: [0, 2, 4]\n",
      "  --> Rank Score: 2/3 = 0.67\n",
      "  --> Cutoff Score: 4 (index of last true label)\n",
      "\n",
      "Instance 3:\n",
      "  Predictions: [0.7  0.6  0.5  0.4  0.3  0.2  0.1  0.05]\n",
      "  True labels: [0 0 0 1 1 0 0 0]\n",
      "  Num positives: 2\n",
      "  Sorted indices: [0 1 2 3 4 5 6 7]\n",
      "  Sorted predictions: [0.7  0.6  0.5  0.4  0.3  0.2  0.1  0.05]\n",
      "  Sorted labels: [0 0 0 1 1 0 0 0]\n",
      "  True label positions in sorted order: [3, 4]\n",
      "  --> Rank Score: 0/2 = 0.00\n",
      "  --> Cutoff Score: 4 (index of last true label)\n",
      "\n",
      "======================================================================\n",
      "FUNCTION RESULTS\n",
      "======================================================================\n",
      "\n",
      "Average Rank Score across batch: 0.4722\n",
      "Average Cutoff Score across batch: 4.67\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION\n",
      "======================================================================\n",
      "\n",
      "Rank Score:\n",
      "  - Measures how many true labels are in the top-k predictions\n",
      "  - k = number of true labels for each instance\n",
      "  - Higher is better (1.0 = perfect ranking)\n",
      "\n",
      "Cutoff Score:\n",
      "  - Index position of the last (lowest-ranked) true label\n",
      "  - Lower is better (all true labels ranked high)\n",
      "  - Useful for determining prediction threshold\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create test data for rank score and cutoff score functions\n",
    "print(\"=\"*70)\n",
    "print(\"TEST: Rank Score and Cutoff Score Functions\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create sample data with same length (8 elements each) to allow stacking\n",
    "# Instance 1: 4 true labels, predictions with varying scores\n",
    "sample1_probs = np.array([0.9, 0.85, 0.7, 0.65, 0.5, 0.45, 0.3, 0.2])\n",
    "sample1_targets = np.array([1, 1, 0, 1, 0, 0, 1, 0])  # 4 positives at indices 0, 1, 3, 6\n",
    "\n",
    "# Instance 2: 3 true labels (padded to 8)\n",
    "sample2_probs = np.array([0.95, 0.8, 0.75, 0.6, 0.4, 0.3, 0.25, 0.15])\n",
    "sample2_targets = np.array([1, 0, 1, 0, 1, 0, 0, 0])  # 3 positives at indices 0, 2, 4\n",
    "\n",
    "# Instance 3: 2 true labels with low predictions (padded to 8)\n",
    "sample3_probs = np.array([0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.05])\n",
    "sample3_targets = np.array([0, 0, 0, 1, 1, 0, 0, 0])  # 2 positives at indices 3, 4 (lowest scores)\n",
    "\n",
    "# Stack into batch format (all same length now)\n",
    "probs_np = np.stack([sample1_probs, sample2_probs, sample3_probs])\n",
    "targets_np = np.stack([sample1_targets, sample2_targets, sample3_targets])\n",
    "\n",
    "# Create dummy predicted_terms (not used in computation but required by function signature)\n",
    "predicted_terms = [\n",
    "    [f\"GO:000000{i}\" for i in range(len(sample1_probs))],\n",
    "    [f\"GO:000000{i}\" for i in range(len(sample2_probs))],\n",
    "    [f\"GO:000000{i}\" for i in range(len(sample3_probs))]\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analyze each instance\n",
    "for idx, (probs, targets) in enumerate(zip(probs_np, targets_np)):\n",
    "    print(f\"\\nInstance {idx + 1}:\")\n",
    "    print(f\"  Predictions: {probs}\")\n",
    "    print(f\"  True labels: {targets}\")\n",
    "    print(f\"  Num positives: {int(targets.sum())}\")\n",
    "    \n",
    "    # Show sorted order\n",
    "    sorted_indices = np.argsort(probs)[::-1]\n",
    "    print(f\"  Sorted indices: {sorted_indices}\")\n",
    "    print(f\"  Sorted predictions: {probs[sorted_indices]}\")\n",
    "    print(f\"  Sorted labels: {targets[sorted_indices]}\")\n",
    "    \n",
    "    # Find positions of true labels in sorted order\n",
    "    true_positions = [i for i, idx in enumerate(sorted_indices) if targets[idx] == 1]\n",
    "    print(f\"  True label positions in sorted order: {true_positions}\")\n",
    "    \n",
    "    # Calculate metrics for this instance\n",
    "    num_pos = int(targets.sum())\n",
    "    if num_pos > 0:\n",
    "        top_k_indices = sorted_indices[:num_pos]\n",
    "        true_in_top_k = targets[top_k_indices].sum()\n",
    "        rank_score = true_in_top_k / num_pos\n",
    "        cutoff_score = max(true_positions) if true_positions else 0\n",
    "        \n",
    "        print(f\"  --> Rank Score: {true_in_top_k}/{num_pos} = {rank_score:.2f}\")\n",
    "        print(f\"  --> Cutoff Score: {cutoff_score} (index of last true label)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FUNCTION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create a model instance to access the methods\n",
    "test_model = Query2Label_pl(num_classes=8, in_dim=512)\n",
    "\n",
    "# Test the functions\n",
    "rank_score = test_model._compute_rank_score(probs_np, targets_np, predicted_terms)\n",
    "cutoff_score = test_model._compute_cutoff_score(probs_np, targets_np, predicted_terms)\n",
    "\n",
    "print(f\"\\nAverage Rank Score across batch: {rank_score:.4f}\")\n",
    "print(f\"Average Cutoff Score across batch: {cutoff_score:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nRank Score:\")\n",
    "print(\"  - Measures how many true labels are in the top-k predictions\")\n",
    "print(\"  - k = number of true labels for each instance\")\n",
    "print(\"  - Higher is better (1.0 = perfect ranking)\")\n",
    "print(\"\\nCutoff Score:\")\n",
    "print(\"  - Index position of the last (lowest-ranked) true label\")\n",
    "print(\"  - Lower is better (all true labels ranked high)\")\n",
    "print(\"  - Useful for determining prediction threshold\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ea9631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
       "       64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_np = np.ones((128, 64))\n",
    "true_np.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bf2a336",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot specify order when the array has no fields.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m pred_np = np.random.rand(\u001b[32m128\u001b[39m, \u001b[32m64\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mpred_np\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquicksort\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mascending\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m pred_np\n",
      "\u001b[31mValueError\u001b[39m: Cannot specify order when the array has no fields."
     ]
    }
   ],
   "source": [
    "# pred_np = np.random.rand(128, 64)\n",
    "\n",
    "# pred_np.sort(axis=1, kind='quicksort', order='ascending')\n",
    "# pred_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7cabb1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7fc4cd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['go_embed'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ac9fad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['feature'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c8dcebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(batch['go_embed'], batch['feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53018c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56a712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cafa6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
